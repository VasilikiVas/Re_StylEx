{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StylEx.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/StyleEX/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU75gboUp8T8",
        "outputId": "81bea2e3-6517-4a53-dda1-600b59c70a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/StyleEX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls data/stylegan2/Dogs-Cats/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtRwCmEknfdm",
        "outputId": "d608987a-4793-4aaf-9992-d6f1e995c74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mtrain\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install labml\n",
        "%pip install labml_helpers\n",
        "%pip install labml_nn\n",
        "%pip install lpips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JMI3N_Qr_Rh",
        "outputId": "591608cf-c89e-4801-fb64-3d94794fa660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: labml in /usr/local/lib/python3.7/dist-packages (0.4.144)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml) (3.1.26)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.0)\n",
            "Requirement already satisfied: labml_helpers in /usr/local/lib/python3.7/dist-packages (0.4.84)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from labml_helpers) (1.10.0+cu111)\n",
            "Requirement already satisfied: labml>=0.4.133 in /usr/local/lib/python3.7/dist-packages (from labml_helpers) (0.4.144)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.133->labml_helpers) (3.1.26)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.133->labml_helpers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.133->labml_helpers) (3.13)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.133->labml_helpers) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.133->labml_helpers) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.133->labml_helpers) (5.0.0)\n",
            "Requirement already satisfied: labml_nn in /usr/local/lib/python3.7/dist-packages (0.4.118)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from labml_nn) (1.10.0+cu111)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.4.0)\n",
            "Requirement already satisfied: labml-helpers>=0.4.84 in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.4.84)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.11.0)\n",
            "Requirement already satisfied: labml>=0.4.135 in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.4.144)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml_nn) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.11.1+cu111)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.135->labml_nn) (3.13)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.135->labml_nn) (3.1.26)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.135->labml_nn) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.135->labml_nn) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.135->labml_nn) (5.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext->labml_nn) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext->labml_nn) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (2.10)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->labml_nn) (7.1.2)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.19.5)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.11.1+cu111)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->lpips) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W3G2kn5j_Dw"
      },
      "outputs": [],
      "source": [
        "#@title Code\n",
        "import math\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class MappingNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"mapping_network\"></a>\n",
        "    ## Mapping Network\n",
        "    ![Mapping Network](mapping_network.svg)\n",
        "    This is an MLP with 8 linear layers.\n",
        "    The mapping network maps the latent vector $z \\in \\mathcal{W}$\n",
        "    to an intermediate latent space $w \\in \\mathcal{W}$.\n",
        "    $\\mathcal{W}$ space will be disentangled from the image space\n",
        "    where the factors of variation become more linear.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features: int, n_layers: int, outputs: int = 0):\n",
        "        \"\"\"\n",
        "        * `features` is the number of features in $z$ and $w$\n",
        "        * `n_layers` is the number of layers in the mapping network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if outputs == 0:\n",
        "          outputs = features\n",
        "\n",
        "        # Create the MLP\n",
        "        layers = []\n",
        "        for i in range(n_layers-1):\n",
        "            # [Equalized learning-rate linear layers](#equalized_linear)\n",
        "            layers.append(EqualizedLinear(features, features))\n",
        "            # Leaky Relu\n",
        "            layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        # Change sizes in last layer\n",
        "        layers.append(EqualizedLinear(features, outputs))\n",
        "        layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, z: torch.Tensor):\n",
        "        # Normalize $z$\n",
        "        z = F.normalize(z, dim=1)\n",
        "        # Map $z$ to $w$\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"generator\"></a>\n",
        "    ## StyleGAN2 Generator\n",
        "    ![Generator](style_gan2.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n",
        "    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n",
        "    The generator starts with a learned constant.\n",
        "    Then it has a series of blocks. The feature map resolution is doubled at each block\n",
        "    Each block outputs an RGB image and they are scaled up and summed to get the final RGB image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, d_latent: int, n_features: int = 32, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (final block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Calculate the number of features for each block\n",
        "        #\n",
        "        # Something like `[512, 512, 256, 128, 64, 32]`\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\n",
        "        # Number of generator blocks\n",
        "        self.n_blocks = len(features)\n",
        "\n",
        "        # Trainable $4 \\times 4$ constant\n",
        "        self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\n",
        "\n",
        "        # First style block for $4 \\times 4$ resolution and layer to get RGB\n",
        "        self.style_block = StyleBlock(d_latent, features[0], features[0])\n",
        "        self.to_rgb = ToRGB(d_latent, features[0])\n",
        "\n",
        "        # Generator blocks\n",
        "        blocks = [GeneratorBlock(d_latent, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        # $2 \\times$ up sampling layer. The feature space is up sampled\n",
        "        # at each block\n",
        "        self.up_sample = UpSample()\n",
        "\n",
        "    def forward(self, w: torch.Tensor, input_noise: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]]):\n",
        "        \"\"\"\n",
        "        * `w` is $w$. In order to mix-styles (use different $w$ for different layers), we provide a separate\n",
        "        $w$ for each [generator block](#generator_block). It has shape `[n_blocks, batch_size, d_latent]`.\n",
        "        * `input_noise` is the noise for each block.\n",
        "        It's a list of pairs of noise sensors because each block (except the initial) has two noise inputs\n",
        "        after each convolution layer (see the diagram).\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size\n",
        "        batch_size = w.shape[1]\n",
        "\n",
        "        # Expand the learned constant to match batch size\n",
        "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "        # The first style block\n",
        "        x = self.style_block(x, w[0], input_noise[0][1])\n",
        "        # Get first rgb image\n",
        "        rgb = self.to_rgb(x, w[0])\n",
        "\n",
        "        # Evaluate rest of the blocks\n",
        "        for i in range(1, self.n_blocks):\n",
        "            # Up sample the feature map\n",
        "            x = self.up_sample(x)\n",
        "            # Run it through the [generator block](#generator_block)\n",
        "            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\n",
        "            # Up sample the RGB image and add to the rgb from the block\n",
        "            rgb = self.up_sample(rgb) + rgb_new\n",
        "\n",
        "        # Return the final RGB image\n",
        "        return rgb\n",
        "\n",
        "\n",
        "class GeneratorBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"generator_block\"></a>\n",
        "    ### Generator Block\n",
        "    ![Generator block](generator_block.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n",
        "    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n",
        "    The generator block consists of two [style blocks](#style_block) ($3 \\times 3$ convolutions with style modulation)\n",
        "    and an RGB output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First [style block](#style_block) changes the feature map size to `out_features`\n",
        "        self.style_block1 = StyleBlock(d_latent, in_features, out_features)\n",
        "        # Second [style block](#style_block)\n",
        "        self.style_block2 = StyleBlock(d_latent, out_features, out_features)\n",
        "\n",
        "        # *toRGB* layer\n",
        "        self.to_rgb = ToRGB(d_latent, out_features)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tuple of two noise tensors of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # First style block with first noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        x = self.style_block1(x, w, noise[0])\n",
        "        # Second style block with second noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        x = self.style_block2(x, w, noise[1])\n",
        "\n",
        "        # Get RGB image\n",
        "        rgb = self.to_rgb(x, w)\n",
        "\n",
        "        # Return feature map and rgb image\n",
        "        return x, rgb\n",
        "\n",
        "\n",
        "class StyleBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"style_block\"></a>\n",
        "    ### Style Block\n",
        "    ![Style block](style_block.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is single channel).*---\n",
        "    Style block has a weight modulation convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n",
        "        # an [equalized learning-rate linear layer](#equalized_linear)\n",
        "        self.to_style = EqualizedLinear(d_latent, in_features, bias=1.0)\n",
        "        # Weight modulated convolution layer\n",
        "        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n",
        "        # Noise scale\n",
        "        self.scale_noise = nn.Parameter(torch.zeros(1))\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "        # Activation function\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Optional[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tensor of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$\n",
        "        s = self.to_style(w)\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, s)\n",
        "        # Scale and add noise\n",
        "        if noise is not None:\n",
        "            x = x + self.scale_noise[None, :, None, None] * noise\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "\n",
        "class ToRGB(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"to_rgb\"></a>\n",
        "    ### To RGB\n",
        "    ![To RGB](to_rgb.svg)\n",
        "    ---*$A$ denotes a linear layer.*---\n",
        "    Generates an RGB image from a feature map using $1 \\times 1$ convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `features` is the number of features in the feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n",
        "        # an [equalized learning-rate linear layer](#equalized_linear)\n",
        "        self.to_style = EqualizedLinear(d_latent, features, bias=1.0)\n",
        "\n",
        "        # Weight modulated convolution layer without demodulation\n",
        "        self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.zeros(3))\n",
        "        # Activation function\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$\n",
        "        style = self.to_style(w)\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, style)\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "\n",
        "class Conv2dWeightModulate(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Convolution with Weight Modulation and Demodulation\n",
        "    This layer scales the convolution weights by the style vector and demodulates by normalizing it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, kernel_size: int,\n",
        "                 demodulate: float = True, eps: float = 1e-8):\n",
        "        \"\"\"\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        * `kernel_size` is the size of the convolution kernel\n",
        "        * `demodulate` is flag whether to normalize weights by its standard deviation\n",
        "        * `eps` is the $\\epsilon$ for normalizing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Number of output features\n",
        "        self.out_features = out_features\n",
        "        # Whether to normalize weights\n",
        "        self.demodulate = demodulate\n",
        "        # Padding size\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "\n",
        "        # [Weights parameter with equalized learning rate](#equalized_weight)\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        # $\\epsilon$\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor, s: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `s` is style based scaling tensor of shape `[batch_size, in_features]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size, height and width\n",
        "        b, _, h, w = x.shape\n",
        "\n",
        "        # Reshape the scales\n",
        "        s = s[:, None, :, None, None]\n",
        "        # Get [learning rate equalized weights](#equalized_weight)\n",
        "        weights = self.weight()[None, :, :, :, :]\n",
        "        # $$w`_{i,j,k} = s_i * w_{i,j,k}$$\n",
        "        # where $i$ is the input channel, $j$ is the output channel, and $k$ is the kernel index.\n",
        "        #\n",
        "        # The result has shape `[batch_size, out_features, in_features, kernel_size, kernel_size]`\n",
        "        weights = weights * s\n",
        "\n",
        "        # Demodulate\n",
        "        if self.demodulate:\n",
        "            # $$\\sigma_j = \\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}$$\n",
        "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
        "            # $$w''_{i,j,k} = \\frac{w'_{i,j,k}}{\\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}}$$\n",
        "            weights = weights * sigma_inv\n",
        "\n",
        "        # Reshape `x`\n",
        "        x = x.reshape(1, -1, h, w)\n",
        "\n",
        "        # Reshape weights\n",
        "        _, _, *ws = weights.shape\n",
        "        weights = weights.reshape(b * self.out_features, *ws)\n",
        "\n",
        "        # Use grouped convolution to efficiently calculate the convolution with sample wise kernel.\n",
        "        # i.e. we have a different kernel (weights) for each sample in the batch\n",
        "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
        "\n",
        "        # Reshape `x` to `[batch_size, out_features, height, width]` and return\n",
        "        return x.reshape(-1, self.out_features, h, w)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, PATH):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = load_classifier(PATH)\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.model(x)\n",
        "        \n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"discriminator\"></a>\n",
        "    ## StyleGAN 2 Discriminator\n",
        "    ![Discriminator](style_gan2_disc.svg)\n",
        "    Discriminator first transforms the image to a feature map of the same resolution and then\n",
        "    runs it through a series of blocks with residual connections.\n",
        "    The resolution is down-sampled by $2 \\times$ at each block while doubling the\n",
        "    number of features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, n_features: int = 64, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (first block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer to convert RGB image to a feature map with `n_features` number of features.\n",
        "        self.from_rgb = nn.Sequential(\n",
        "            EqualizedConv2d(3, n_features, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
        "        n_blocks = len(features) - 1\n",
        "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.std_dev = MiniBatchStdDev()\n",
        "        final_features = features[-1] + 1\n",
        "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
        "        self.final = EqualizedLinear(2 * 2 * final_features, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        x = x - 0.5\n",
        "        x = self.from_rgb(x)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        x = self.std_dev(x)\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.final(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"discriminator\"></a>\n",
        "    ## StyleGAN 2 Discriminator\n",
        "    ![Discriminator](style_gan2_disc.svg)\n",
        "    Discriminator first transforms the image to a feature map of the same resolution and then\n",
        "    runs it through a series of blocks with residual connections.\n",
        "    The resolution is down-sampled by $2 \\times$ at each block while doubling the\n",
        "    number of features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, d_latent: int, n_features: int = 64, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (first block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer to convert RGB image to a feature map with `n_features` number of features.\n",
        "        self.from_rgb = nn.Sequential(\n",
        "            EqualizedConv2d(3, n_features, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
        "        n_blocks = len(features) - 1\n",
        "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.std_dev = MiniBatchStdDev()\n",
        "        final_features = features[-1] + 1\n",
        "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
        "        self.final = EqualizedLinear(2 * 2 * final_features, d_latent)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        x = x - 0.5\n",
        "        x = self.from_rgb(x)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        x = self.std_dev(x)\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(DownSample(),\n",
        "                                      EqualizedConv2d(in_features, out_features, kernel_size=1))\n",
        "        self.block = nn.Sequential(\n",
        "            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "        self.down_sample = DownSample()\n",
        "        self.scale = 1 / math.sqrt(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        x = self.block(x)\n",
        "        x = self.down_sample(x)\n",
        "        return (x + residual) * self.scale\n",
        "\n",
        "\n",
        "class MiniBatchStdDev(nn.Module):\n",
        "\n",
        "    def __init__(self, group_size: int = 4):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.shape[0] % self.group_size == 0\n",
        "        grouped = x.view(self.group_size, -1)\n",
        "        std = torch.sqrt(grouped.var(dim=0) + 1e-8)\n",
        "        std = std.mean().view(1, 1, 1, 1)\n",
        "        b, _, h, w = x.shape\n",
        "        std = std.expand(b, -1, h, w)\n",
        "        return torch.cat([x, std], dim=1)\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Smoothing layer\n",
        "        self.smooth = Smooth()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.smooth(x)\n",
        "        return F.interpolate(x, (x.shape[2] // 2, x.shape[3] // 2), mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.smooth = Smooth()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.smooth(self.up_sample(x))\n",
        "\n",
        "\n",
        "class Smooth(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kernel = [[1, 2, 1],\n",
        "                  [2, 4, 2],\n",
        "                  [1, 2, 1]]\n",
        "        kernel = torch.tensor([[kernel]], dtype=torch.float)\n",
        "        kernel /= kernel.sum()\n",
        "        self.kernel = nn.Parameter(kernel, requires_grad=False)\n",
        "        self.pad = nn.ReplicationPad2d(1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.view(-1, 1, h, w)\n",
        "        x = self.pad(x)\n",
        "        x = F.conv2d(x, self.kernel)\n",
        "        return x.view(b, c, h, w)\n",
        "\n",
        "\n",
        "class EqualizedLinear(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: float = 0.):\n",
        "        super().__init__()\n",
        "        self.weight = EqualizedWeight([out_features, in_features])\n",
        "        self.bias = nn.Parameter(torch.ones(out_features) * bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.linear(x, self.weight(), bias=self.bias)\n",
        "\n",
        "\n",
        "class EqualizedConv2d(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int,\n",
        "                 kernel_size: int, padding: int = 0):\n",
        "        super().__init__()\n",
        "        self.padding = padding\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        self.bias = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.conv2d(x, self.weight(), bias=self.bias, padding=self.padding)\n",
        "\n",
        "\n",
        "class EqualizedWeight(nn.Module):\n",
        "\n",
        "    def __init__(self, shape: List[int]):\n",
        "        super().__init__()\n",
        "        self.c = 1 / math.sqrt(np.prod(shape[1:]))\n",
        "        self.weight = nn.Parameter(torch.randn(shape))\n",
        "    def forward(self):\n",
        "        return self.weight * self.c\n",
        "\n",
        "\n",
        "class GradientPenalty(nn.Module):\n",
        "    def forward(self, x: torch.Tensor, d: torch.Tensor):\n",
        "        batch_size = x.shape[0]\n",
        "        gradients, *_ = torch.autograd.grad(outputs=d,\n",
        "                                            inputs=x,\n",
        "                                            grad_outputs=d.new_ones(d.shape),\n",
        "                                            create_graph=True)\n",
        "        \n",
        "        gradients = gradients.reshape(batch_size, -1)\n",
        "        norm = gradients.norm(2, dim=-1)\n",
        "        return torch.mean(norm ** 2)\n",
        "\n",
        "\n",
        "class PathLengthPenalty(nn.Module):\n",
        "    def __init__(self, beta: float):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "\n",
        "    def forward(self, w: torch.Tensor, x: torch.Tensor):\n",
        "        device = x.device\n",
        "        image_size = x.shape[2] * x.shape[3]\n",
        "        y = torch.randn(x.shape, device=device)\n",
        "        output = (x * y).sum() / math.sqrt(image_size)\n",
        "\n",
        "        gradients, *_ = torch.autograd.grad(outputs=output,\n",
        "                                            inputs=w,\n",
        "                                            grad_outputs=torch.ones(output.shape, device=device),\n",
        "                                            create_graph=True)\n",
        "        \n",
        "        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\n",
        "\n",
        "        if self.steps > 0:\n",
        "            a = self.exp_sum_a / (1 - self.beta ** self.steps)\n",
        "            loss = torch.mean((norm - a) ** 2)\n",
        "        else:\n",
        "            loss = norm.new_tensor(0)\n",
        "\n",
        "        mean = norm.mean().detach()\n",
        "        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n",
        "        self.steps.add_(1.)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title StylEx\n",
        "\n",
        "\n",
        "class StylEx(nn.Module):\n",
        "  def __init__(self,\n",
        "               log_resolution = 32, # log image resolution\n",
        "               n_z = 512, # dimensionality of the latent encoding\n",
        "               n_c = 2, # dimensionality of conditional encoding (number of classes)\n",
        "               n_s = 512, # dimensionality of the latent attribute layer\n",
        "               color_channels = 3, # number of input channels\n",
        "               classifier_path = './classifier.pt',\n",
        "               device = None\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.latent_dim = n_z\n",
        "    self.conditional_dim = n_c\n",
        "\n",
        "    self.generator  = Generator(log_resolution, n_s, n_features=32)\n",
        "    self.encoder = Encoder(log_resolution, d_latent=n_z)\n",
        "    self.discriminator = Discriminator(log_resolution, n_features=32)\n",
        "\n",
        "    self.classifier = load_classifier(classifier_path)\n",
        "    self.random_mapping = MappingNetwork(features=n_s, n_layers=8)\n",
        "    # self.mapping = MappingNetwork(features=n_z + n_c, n_layers=8, outputs=n_s)\n",
        "    # self.affine = torch.nn.Linear(n_z+n_c, n_s)\n",
        "\n",
        "    self.n_gen_blocks = self.generator.n_blocks\n",
        "    self.device = device\n",
        "\n",
        "  # Generate noise for random generation\n",
        "  def __noise__(self, batch_size: int):\n",
        "    noise = []\n",
        "    resolution = 4\n",
        "    for i in range(self.n_gen_blocks):\n",
        "      if i == 0:\n",
        "        n1 = None\n",
        "      else:\n",
        "        n1 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "      n2 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "      noise.append((n1, n2))\n",
        "      resolution *= 2\n",
        "    return noise\n",
        "\n",
        "  # Sample random styles for training\n",
        "  def __get_random__(self, batch_size: int):\n",
        "    # Mix styles\n",
        "    if torch.rand(()).item() < self.style_mixing_prob:\n",
        "      # Random cross-over point\n",
        "      cross_over_point = int(torch.rand(()).item() * self.n_gen_blocks)\n",
        "      # Sample $z_1$ and $z_2$\n",
        "      z2 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "      z1 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "      # Get $w_1$ and $w_2$\n",
        "      w1 = self.model.random_mapping(z1)\n",
        "      w2 = self.model.random_mapping(z2)\n",
        "      # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n",
        "      w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n",
        "      w2 = w2[None, :, :].expand(self.n_gen_blocks - cross_over_point, -1, -1)\n",
        "      return torch.cat((w1, w2), dim=0)\n",
        "\n",
        "    # Without mixing\n",
        "    else:\n",
        "      # Sample $z$\n",
        "      z = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "      # Use the mapping network trained for random sampling\n",
        "      w = self.model.random_mapping(z)\n",
        "      # Expand $w$ for the generator blocks\n",
        "      return w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n",
        "\n",
        "  # Find the latent encoding vector appended with the classification vector\n",
        "  def get_latent(self, x):\n",
        "    z = self.encoder(x)\n",
        "    c = self.classifier(x)\n",
        "    w = torch.concat((z, c), dim=1)\n",
        "    return w\n",
        "  # Classify the input image\n",
        "  def classify(self, x):\n",
        "    return self.classifier(x)\n",
        "\n",
        "  # Return latent encoding of an image\n",
        "  def encode(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  # Transform latent, w, vector into the StyleSpace\n",
        "  def get_attribute(self, w):\n",
        "    return self.generator.StyleBlock(w)\n",
        "  \n",
        "  # Returns the reconstructed image and latent vector\n",
        "  def reconstruct(self, x):\n",
        "    w = self.get_latent(x)\n",
        "    s = self.get_attribute(w)\n",
        "\n",
        "    # Reconstructed\n",
        "    if self.training:\n",
        "      noise = self.__noise__(x.shape[0])\n",
        "    else:\n",
        "      noise = 0\n",
        "    x_ = self.generator(s, noise)\n",
        "\n",
        "    # Reconstructed latent space\n",
        "    w_ = self.encoder(x_)\n",
        "    return x_, w_\n",
        "\n",
        "  def generate_images(self, batch_size: int):\n",
        "        # Get latent\n",
        "        w = self.get_w(batch_size)\n",
        "        # Get noise\n",
        "        noise = self.__noise__(batch_size)\n",
        "        images = self.model.generator(w, input_noise=noise)\n",
        "\n",
        "        # Return images and w\n",
        "        return images\n",
        "\n",
        "\n",
        "  def generate_style(self, s):\n",
        "    return generator(s)\n"
      ],
      "metadata": {
        "id": "d7GuJmCKHSVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Config\n",
        "\"\"\"\n",
        "---\n",
        "title: StyleGAN 2 Model Training\n",
        "summary: >\n",
        " An annotated PyTorch implementation of StyleGAN2 model training code.\n",
        "---\n",
        "# [StyleGAN 2](index.html) Model Training\n",
        "This is the training code for [StyleGAN 2](index.html) model.\n",
        "![Generated Images](generated_64.png)\n",
        "---*These are $64 \\times 64$ images generated after training for about 80K steps.*---\n",
        "*Our implementation is a minimalistic StyleGAN 2 model training code.\n",
        "Only single GPU training is supported to keep the implementation simple.\n",
        "We managed to shrink it to keep it at less than 500 lines of code, including the training loop.*\n",
        "*Without DDP (distributed data parallel) and multi-gpu training it will not be possible to train the model\n",
        "for large resolutions (128+).\n",
        "If you want training code with fp16 and DDP take a look at\n",
        "[lucidrains/stylegan2-pytorch](https://github.com/lucidrains/stylegan2-pytorch).*\n",
        "We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n",
        "You can find the download instruction in this\n",
        "[discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n",
        "Save the images inside [`data/stylegan` folder](#dataset_path).\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Iterator, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import lpips\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from labml import tracker, lab, monit, experiment\n",
        "from labml.configs import BaseConfigs\n",
        "from labml_helpers.device import DeviceConfigs\n",
        "from labml_helpers.train_valid import ModeState, hook_model_outputs\n",
        "#from labml_nn.gan.stylegan import Discriminator, Generator, MappingNetwork, GradientPenalty, PathLengthPenalty\n",
        "from labml_nn.gan.wasserstein import DiscriminatorLoss, GeneratorLoss\n",
        "from labml_nn.utils import cycle_dataloader\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    ## Dataset\n",
        "    This loads the training dataset and resize it to the give image size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str, image_size: int):\n",
        "        \"\"\"\n",
        "        * `path` path to the folder containing the images\n",
        "        * `image_size` size of the image\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Get the paths of all `jpg` files\n",
        "        self.paths = [p for p in Path(path).glob(f'**/*.jpg')]\n",
        "\n",
        "        # Transformation\n",
        "        self.transform = torchvision.transforms.Compose([\n",
        "            # Resize the image\n",
        "            torchvision.transforms.Resize(image_size),\n",
        "            # Convert to PyTorch tensor\n",
        "            torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of images\"\"\"\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get the the `index`-th image\"\"\"\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path)\n",
        "        return self.transform(img)\n",
        "def get_mnist_data(train_dir, image_size, digit=8):\n",
        "\n",
        "  data_transform = transforms.Compose([\n",
        "    #transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((image_size,image_size)),\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "\n",
        "  ])\n",
        "  train_files = os.listdir(train_dir)\n",
        "  dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                        train=True, \n",
        "                        download=True, \n",
        "                        transform=data_transform)\n",
        "  idx = dataset.targets==digit\n",
        "  dataset.targets = dataset.targets[idx]\n",
        "  dataset.data = dataset.data[idx]\n",
        "  return dataset\n",
        "def get_mnist_2_digit_data(train_dir, image_size, digit_0=9, digit_1=8):\n",
        "\n",
        "  data_transform = transforms.Compose([\n",
        "    #transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((image_size,image_size)),\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "  ])\n",
        "  train_files = os.listdir(train_dir)\n",
        "  dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                        train=True, \n",
        "                        download=True, \n",
        "                        transform=data_transform)\n",
        "  idx_0 = dataset.targets==digit_0\n",
        "  idx_1 = dataset.targets==digit_1\n",
        "  \n",
        "  data_0 = dataset.data[idx_0]\n",
        "  targets_0 = torch.zeros(len(data_0))\n",
        "  data_1 = dataset.data[idx_1]\n",
        "  targets_1 = torch.zeros(len(data_1))\n",
        "  dataset.data = torch.concat((data_0,data_1), dim = 0)\n",
        "  dataset.targets = torch.concat((targets_0,targets_1), dim = 0)\n",
        "  return dataset\n",
        "\n",
        "def get_catdog_data(train_dir, image_size):\n",
        "\n",
        "  class CatDogDataset(Dataset):\n",
        "    def __init__(self, file_list, dir, mode='train', transform = None):\n",
        "        self.file_list = file_list\n",
        "        self.dir = dir\n",
        "        self.mode= mode\n",
        "        self.transform = transform\n",
        "        if self.mode == 'train':\n",
        "            if 'dog' in self.file_list[0]:\n",
        "                self.label = 1\n",
        "            else:\n",
        "                self.label = 0\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(os.path.join(self.dir, self.file_list[idx]))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.mode == 'train':\n",
        "            img = img.numpy()\n",
        "            return img.astype('float32'), self.label\n",
        "        else:\n",
        "            img = img.numpy()\n",
        "            return img.astype('float32'), self.file_list[idx]\n",
        "        \n",
        "  data_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((image_size,image_size)),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "  train_files = os.listdir(train_dir)\n",
        "  cat_files = [tf for tf in train_files if 'cat' in tf]\n",
        "  dog_files = [tf for tf in train_files if 'dog' in tf]\n",
        "\n",
        "  cats = CatDogDataset(cat_files, train_dir, transform = data_transform)\n",
        "  dogs = CatDogDataset(dog_files, train_dir, transform = data_transform)\n",
        "\n",
        "  catdogs = ConcatDataset([cats, dogs])\n",
        "  return catdogs\n",
        "class Configs(BaseConfigs):\n",
        "    \"\"\"\n",
        "    ## Configurations\n",
        "    \"\"\"\n",
        "\n",
        "    # Device to train the model on.\n",
        "    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n",
        "    #  picks up an available CUDA device or defaults to CPU.\n",
        "    device: torch.device = DeviceConfigs()\n",
        "\n",
        "    # [StyleGAN2 Discriminator](index.html#discriminator)\n",
        "    discriminator: Discriminator\n",
        "    # [StyleGAN2 Generator](index.html#generator)\n",
        "    generator: Generator\n",
        "    # [StylEx Encoder](index.html#encoder)\n",
        "    encoder: Encoder\n",
        "    # [StylEx Classifier](index.html#classifier)\n",
        "    classifier: Classifier\n",
        "\n",
        "    # [StylEx Model]\n",
        "    model: StylEx\n",
        "    \n",
        "    # [Mapping network](index.html#mapping_network)\n",
        "    mapping_network: MappingNetwork\n",
        "\n",
        "    # Discriminator and generator loss functions.\n",
        "    # We use [Wasserstein loss](../wasserstein/index.html)\n",
        "    discriminator_loss: DiscriminatorLoss\n",
        "    generator_loss: GeneratorLoss\n",
        "\n",
        "    # Optimizers\n",
        "    generator_optimizer: torch.optim.Adam\n",
        "    discriminator_optimizer: torch.optim.Adam\n",
        "    mapping_network_optimizer: torch.optim.Adam\n",
        "    encoder_optimizer: torch.optim.Adam\n",
        "    affine_optimizer: torch.optim.Adam\n",
        "\n",
        "    # [Gradient Penalty Regularization Loss](index.html#gradient_penalty)\n",
        "    gradient_penalty = GradientPenalty()\n",
        "    # Gradient penalty coefficient $\\gamma$\n",
        "    gradient_penalty_coefficient: float = 10.\n",
        "\n",
        "    # [Path length penalty](index.html#path_length_penalty)\n",
        "    path_length_penalty: PathLengthPenalty\n",
        "\n",
        "    # Data loader\n",
        "    loader: Iterator\n",
        "\n",
        "    # Batch size\n",
        "    batch_size: int = 32\n",
        "    # Dimensionality of $z$ and $w$\n",
        "    d_latent: int = 512\n",
        "    # Height/width of the image\n",
        "    image_size: int = 32\n",
        "    # Number of layers in the mapping network\n",
        "    mapping_network_layers: int = 8\n",
        "    # Generator & Discriminator learning rate\n",
        "    learning_rate: float = 1e-3\n",
        "    # Mapping network learning rate ($100 \\times$ lower than the others)\n",
        "    mapping_network_learning_rate: float = 1e-5\n",
        "    # Number of steps to accumulate gradients on. Use this to increase the effective batch size.\n",
        "    gradient_accumulate_steps: int = 1\n",
        "    # $\\beta_1$ and $\\beta_2$ for Adam optimizer\n",
        "    adam_betas: Tuple[float, float] = (0.0, 0.99)\n",
        "    # Probability of mixing styles\n",
        "    style_mixing_prob: float = 0.9\n",
        "    # Number of classes\n",
        "    num_classes: int = 2\n",
        "\n",
        "    # Total number of training steps\n",
        "    training_steps: int = 150_000\n",
        "\n",
        "    # Number of blocks in the generator (calculated based on image resolution)\n",
        "    n_gen_blocks: int\n",
        "\n",
        "    # ### Lazy regularization\n",
        "    # Instead of calculating the regularization losses, the paper proposes lazy regularization\n",
        "    # where the regularization terms are calculated once in a while.\n",
        "    # This improves the training efficiency a lot.\n",
        "\n",
        "    # The interval at which to compute gradient penalty\n",
        "    lazy_gradient_penalty_interval: int = 4\n",
        "    # Path length penalty calculation interval\n",
        "    lazy_path_penalty_interval: int = 32\n",
        "    # Skip calculating path length penalty during the initial phase of training\n",
        "    lazy_path_penalty_after: int = 5_000\n",
        "\n",
        "    # How often to log generated images\n",
        "    log_generated_interval: int = 500\n",
        "    # How often to save model checkpoints\n",
        "    save_checkpoint_interval: int = 2_000\n",
        "\n",
        "    # Training mode state for logging activations\n",
        "    mode: ModeState\n",
        "    # Whether to log model layer outputs\n",
        "    log_layer_outputs: bool = False\n",
        "\n",
        "    KL_loss: torch.nn = torch.nn.KLDivLoss()\n",
        "    lpips_loss: torch.nn = lpips.LPIPS(net='alex')\n",
        "    L1_loss: torch.nn = torch.nn.L1Loss()\n",
        "    # <a id=\"dataset_path\"></a>\n",
        "    # We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n",
        "    # You can find the download instruction in this\n",
        "    # [discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n",
        "    # Save the images inside `data/stylegan` folder.\n",
        "\n",
        "    dataset_path: str = str('data/stylegan2/Dogs-Cats/train/')\n",
        "    classifier_path: str = str('./mnist_classifier.pt')\n",
        "    load_model: bool = False\n",
        "    model_path: str = './DogCat.pt'\n",
        "\n",
        "    def init(self):\n",
        "        \"\"\"\n",
        "        ### Initialize\n",
        "        \"\"\"\n",
        "        # Create dataset\n",
        "        # dataset = get_catdog_data(self.dataset_path, self.image_size)\n",
        "        dataset = get_mnist_2_digit_data(self.dataset_path, self.image_size)\n",
        "        #dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "        #                                download=True, transform=transform)\n",
        "        \n",
        "\n",
        "        # Create data loader\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, num_workers=2,\n",
        "                                                 shuffle=True, drop_last=True, pin_memory=True)\n",
        "        # Continuous [cyclic loader](../../utils.html#cycle_dataloader)\n",
        "        self.loader = cycle_dataloader(dataloader)\n",
        "        \n",
        "        # $\\log_2$ of image resolution\n",
        "        log_resolution = int(math.log2(self.image_size))\n",
        "\n",
        "        # Create discriminator and generator\n",
        "        #self.discriminator = Discriminator(log_resolution).to(self.device)\n",
        "        #self.generator = Generator(log_resolution, self.d_latent).to(self.device)\n",
        "        #self.encoder = Encoder(log_resolution, self.d_latent).to(self.device)\n",
        "        \n",
        "        self.classifier = Classifier(self.classifier_path).to(self.device)\n",
        "\n",
        "        self.model = StylEx(log_resolution,\n",
        "                            n_z = self.d_latent, \n",
        "                            n_s = self.d_latent,\n",
        "                            n_c = self.num_classes,\n",
        "                            classifier_path = self.classifier_path,\n",
        "                            device = self.device\n",
        "                            ).to(self.device)\n",
        "        if self.load_model:\n",
        "          self.model.load_state_dict(torch.load(self.model_path))\n",
        "        # Get number of generator blocks for creating style and noise inputs\n",
        "        self.n_gen_blocks = self.model.generator.n_blocks\n",
        "        \n",
        "        # Create mapping network\n",
        "        #self.mapping_network = MappingNetwork(self.d_latent, self.mapping_network_layers).to(self.device)\n",
        "        # Create path length penalty loss\n",
        "        self.path_length_penalty = PathLengthPenalty(0.99).to(self.device)\n",
        "\n",
        "        # Add model hooks to monitor layer outputs\n",
        "        if self.log_layer_outputs:\n",
        "            hook_model_outputs(self.mode, self.model.discriminator, 'discriminator')\n",
        "            hook_model_outputs(self.mode, self.model.generator, 'generator')\n",
        "            hook_model_outputs(self.mode, self.model.random_mapping, 'mapping_network')\n",
        "            hook_model_outputs(self.mode, self.model.encoder_mapping, 'encoder->generator mapping')\n",
        "            hook_model_outputs(self.mode, self.model.encoder, 'encoder')\n",
        "\n",
        "        # Discriminator and generator losses\n",
        "        self.discriminator_loss = DiscriminatorLoss().to(self.device)\n",
        "        self.generator_loss = GeneratorLoss().to(self.device)\n",
        "\n",
        "        # Create optimizers\n",
        "        self.discriminator_optimizer = torch.optim.Adam(\n",
        "            self.model.discriminator.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.generator_optimizer = torch.optim.Adam(\n",
        "            self.model.generator.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.mapping_network_optimizer = torch.optim.Adam(\n",
        "            self.model.random_mapping.parameters(),\n",
        "            lr=self.mapping_network_learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.encoder_optimizer = torch.optim.Adam(\n",
        "            self.model.random_mapping.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.affine_optimizer = torch.optim.Adam(\n",
        "            self.model.random_mapping.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "\n",
        "        # Set tracker configurations\n",
        "        tracker.set_image(\"generated\", True)\n",
        "    def reconstruct_loss(self, x, x_, w = None):\n",
        "\n",
        "        # Latent reconstruction loss\n",
        "        if w == None:\n",
        "          w = self.model.encode(x)\n",
        "        w_ = self.model.encode(x_)\n",
        "        L_w = self.L1loss(w, w_)\n",
        "\n",
        "        # Feature reconstruction\n",
        "        L_x = self.L1loss(x, x_)\n",
        "\n",
        "        # Feature LPIPS loss\n",
        "        L_LPIPS = self.LPIPSloss(x, x_)\n",
        "\n",
        "        L = L_w + L_x + L_LPIPS\n",
        "\n",
        "        return L\n",
        "    '''\n",
        "      def adversarial_loss(x, model):\n",
        "        valid = Variable(FloatTensor(x.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "\n",
        "        # Create a random latent vector/classification\n",
        "        w = Variable(FloatTensor(np.random.normal(0, 1, (x.shape[0], model.latent_dim))))\n",
        "        c = Variable(FloatTensor(np.random.uniform(0, 1, (x.shape[0], model.conditional_dim))))\n",
        "\n",
        "        # Decode/generate from latent vector\n",
        "        gen_imgs = model.generator(w, c)\n",
        "\n",
        "        # Try to trick discriminator\n",
        "        validity = model.discriminator(gen_imgs, c)\n",
        "        L = wals_loss_g(validity)\n",
        "        return L\n",
        "    '''\n",
        "\n",
        "    def classifier_loss(self, x, x_):\n",
        "        L = self.KLLoss(self.model.classify(x), self.model.classify(x_))\n",
        "        return L\n",
        "\n",
        "    def get_w(self, batch_size: int):\n",
        "          \"\"\"\n",
        "          ### Sample $w$\n",
        "          This samples $z$ randomly and get $w$ from the mapping network.\n",
        "          We also apply style mixing sometimes where we generate two latent variables\n",
        "          $z_1$ and $z_2$ and get corresponding $w_1$ and $w_2$.\n",
        "          Then we randomly sample a cross-over point and apply $w_1$ to\n",
        "          the generator blocks before the cross-over point and\n",
        "          $w_2$ to the blocks after.\n",
        "          \"\"\"\n",
        "\n",
        "          # Mix styles\n",
        "          if torch.rand(()).item() < self.style_mixing_prob:\n",
        "              # Random cross-over point\n",
        "              cross_over_point = int(torch.rand(()).item() * self.n_gen_blocks)\n",
        "              # Sample $z_1$ and $z_2$\n",
        "              z2 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "              z1 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "              # Get $w_1$ and $w_2$\n",
        "              w1 = self.model.random_mapping(z1)\n",
        "              w2 = self.model.random_mapping(z2)\n",
        "              # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n",
        "              w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n",
        "              w2 = w2[None, :, :].expand(self.n_gen_blocks - cross_over_point, -1, -1)\n",
        "              return torch.cat((w1, w2), dim=0)\n",
        "          # Without mixing\n",
        "          else:\n",
        "              # Sample $z$ and $z$\n",
        "              z = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "              # Get $w$ and $w$\n",
        "              w = self.model.random_mapping(z)\n",
        "              # Expand $w$ for the generator blocks\n",
        "              return w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n",
        "\n",
        "    def get_noise(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        ### Generate noise\n",
        "        This generates noise for each [generator block](index.html#generator_block)\n",
        "        \"\"\"\n",
        "        # List to store noise\n",
        "        noise = []\n",
        "        # Noise resolution starts from $4$\n",
        "        resolution = 4\n",
        "\n",
        "        # Generate noise for each generator block\n",
        "        for i in range(self.n_gen_blocks):\n",
        "            # The first block has only one $3 \\times 3$ convolution\n",
        "            if i == 0:\n",
        "                n1 = None\n",
        "            # Generate noise to add after the first convolution layer\n",
        "            else:\n",
        "                n1 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "            # Generate noise to add after the second convolution layer\n",
        "            n2 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "\n",
        "            # Add noise tensors to the list\n",
        "            noise.append((n1, n2))\n",
        "\n",
        "            # Next block has $2 \\times$ resolution\n",
        "            resolution *= 2\n",
        "\n",
        "        # Return noise tensors\n",
        "        return noise\n",
        "\n",
        "    def generate_images(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        ### Generate images\n",
        "        This generate images using the generator\n",
        "        \"\"\"\n",
        "\n",
        "        # Get $w$\n",
        "        w = self.get_w(batch_size)\n",
        "        # Get noise\n",
        "        noise = self.get_noise(batch_size)\n",
        "\n",
        "        # Generate images\n",
        "        images = self.model.generator(w, noise)\n",
        "\n",
        "        # Return images and $w$\n",
        "        return images, w\n",
        "    \n",
        "    def step(self, idx: int):\n",
        "        \"\"\"\n",
        "        ### Training Step\n",
        "        \"\"\"\n",
        "\n",
        "        # Train the discriminator\n",
        "\n",
        "        #print(idx)\n",
        "\n",
        "        with monit.section('Discriminator'):\n",
        "            # Reset gradients\n",
        "            self.discriminator_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Update `mode`. Set whether to log activation\n",
        "                with self.mode.update(is_log_activations=(idx + 1) % self.log_generated_interval == 0):\n",
        "                    # Sample images from generator\n",
        "                    generated_images, _ = self.generate_images(self.batch_size)\n",
        "                    # Discriminator classification for generated images\n",
        "                    fake_output = self.model.discriminator(generated_images.detach())\n",
        "\n",
        "                    # Get real images from the data loader\n",
        "                    r_img, lbl = next(self.loader)\n",
        "                    real_images = r_img.to(self.device)\n",
        "                    # We need to calculate gradients w.r.t. real images for gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        real_images.requires_grad_()\n",
        "                    # Discriminator classification for real images\n",
        "                    real_output = self.model.discriminator(real_images)\n",
        "\n",
        "                    # Get discriminator loss\n",
        "                    real_loss, fake_loss = self.discriminator_loss(real_output, fake_output)\n",
        "                    disc_loss = real_loss + fake_loss\n",
        "\n",
        "                    # Add gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        # Calculate and log gradient penalty\n",
        "                        gp = self.gradient_penalty(real_images, real_output)\n",
        "                        tracker.add('loss.gp', gp)\n",
        "                        # Multiply by coefficient and add gradient penalty\n",
        "                        disc_loss = disc_loss + 0.5 * self.gradient_penalty_coefficient * gp * self.lazy_gradient_penalty_interval\n",
        "\n",
        "                    # Compute gradients\n",
        "                    disc_loss.backward()\n",
        "\n",
        "                    # Log discriminator loss\n",
        "                    tracker.add('loss.discriminator_real', real_loss)\n",
        "                    tracker.add('loss.discriminator_fake', fake_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('discriminator', self.model.discriminator)\n",
        "            \n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.discriminator.parameters(), max_norm=1.0)\n",
        "            # Take optimizer step\n",
        "            self.discriminator_optimizer.step()\n",
        "\n",
        "        # Train the generator\n",
        "        with monit.section('Generator'):\n",
        "            # Reset gradients\n",
        "            self.generator_optimizer.zero_grad()\n",
        "            self.mapping_network_optimizer.zero_grad()\n",
        "            self.encoder_optimizer.zero_grad()\n",
        "            self.affine_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Sample images from generator\n",
        "                generated_images, w = self.generate_images(self.batch_size)\n",
        "                # Discriminator classification for generated images\n",
        "                fake_output = self.model.discriminator(generated_images)\n",
        "                # Reconstructed images\n",
        "                recon_images = self.model.reconstruct(real_images)\n",
        "\n",
        "                # Get generator loss\n",
        "                gen_loss_rec = self.reconstruct_loss(real_images, \n",
        "                                                     recons_images)\n",
        "                # Adversarial Loss\n",
        "                gen_loss_adv = self.generator_loss(fake_output)\n",
        "                # Classifier Loss\n",
        "                gen_loss_class = self.KL_loss(\n",
        "                    self.model.classify(real_images),\n",
        "                    self.model.classify(recon_images)\n",
        "                    )\n",
        "                # Combine losses using config weight\n",
        "                gen_loss =  self.loss_weights[0]*gen_loss_rec + \\\n",
        "                            self.loss_weights[1]*gen_loss_adv + \\\n",
        "                            self.loss_weights[2]*gen_loss_class\n",
        "\n",
        "                # Add path length penalty\n",
        "                if idx > self.lazy_path_penalty_after and \\\n",
        "                  (idx + 1) % self.lazy_path_penalty_interval == 0:\n",
        "                    # Calculate path length penalty\n",
        "                    plp = self.path_length_penalty(w, generated_images)\n",
        "                    # Ignore if `nan`\n",
        "                    if not torch.isnan(plp):\n",
        "                        tracker.add('loss.plp', plp)\n",
        "                        gen_loss = gen_loss + self.loss_weights[3] * plp\n",
        "\n",
        "                # Calculate gradients\n",
        "                gen_loss.backward()\n",
        "\n",
        "                # Log generator loss\n",
        "                tracker.add('loss.generator', gen_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('generator', self.model.generator)\n",
        "                tracker.add('mapping_network', self.model.random_mapping)\n",
        "\n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.generator.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.random_mapping.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Take optimizer step\n",
        "            self.generator_optimizer.step()\n",
        "            self.mapping_network_optimizer.step()\n",
        "            self.encoder_optimizer.step()\n",
        "            self.affine_optimizer.step()\n",
        "\n",
        "        # Log generated images\n",
        "        if (idx + 1) % self.log_generated_interval == 0:\n",
        "            tracker.add('generated', torch.cat([generated_images[:6], real_images[:3]], dim=0))\n",
        "        # Save model checkpoints\n",
        "        if (idx + 1) % self.save_checkpoint_interval == 0:\n",
        "            experiment.save_checkpoint()\n",
        "\n",
        "        # Flush tracker\n",
        "        tracker.save()\n",
        "        #print(disc_loss + gen_loss)\n",
        "\n",
        "        # Train the generator\n",
        "        with monit.section('Generator'):\n",
        "            # Reset gradients\n",
        "            self.generator_optimizer.zero_grad()\n",
        "            self.mapping_network_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Sample images from generator\n",
        "                generated_images, w = self.generate_images(self.batch_size)\n",
        "                # Discriminator classification for generated images\n",
        "                fake_output = self.model.discriminator(generated_images)\n",
        "\n",
        "                # Get generator loss\n",
        "                gen_loss = self.generator_loss(fake_output)\n",
        "\n",
        "                # Add path length penalty\n",
        "                if idx > self.lazy_path_penalty_after and (idx + 1) % self.lazy_path_penalty_interval == 0:\n",
        "                    # Calculate path length penalty\n",
        "                    plp = self.path_length_penalty(w, generated_images)\n",
        "                    # Ignore if `nan`\n",
        "                    if not torch.isnan(plp):\n",
        "                        tracker.add('loss.plp', plp)\n",
        "                        gen_loss = gen_loss + plp\n",
        "\n",
        "                # Calculate gradients\n",
        "                gen_loss.backward()\n",
        "\n",
        "                # Log generator loss\n",
        "                tracker.add('loss.generator', gen_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('generator', self.model.generator)\n",
        "                tracker.add('mapping_network', self.model.random_mapping)\n",
        "\n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.generator.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.random_mapping.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Take optimizer step\n",
        "            self.generator_optimizer.step()\n",
        "            self.mapping_network_optimizer.step()\n",
        "\n",
        "        # Log generated images\n",
        "        if (idx + 1) % self.log_generated_interval == 0:\n",
        "            tracker.add('generated', torch.cat([generated_images[:6], real_images[:3]], dim=0))\n",
        "        # Save model checkpoints\n",
        "        if (idx + 1) % self.save_checkpoint_interval == 0:\n",
        "            experiment.save_checkpoint()\n",
        "\n",
        "        # Flush tracker\n",
        "        tracker.save()\n",
        "        #print(disc_loss + gen_loss)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        ## Train model\n",
        "        \"\"\"\n",
        "\n",
        "        # Loop for `training_steps`\n",
        "        for i in monit.loop(self.training_steps):\n",
        "            # Take a training step\n",
        "            self.step(i)\n",
        "            #\n",
        "            if (i + 1) % self.log_generated_interval == 0:\n",
        "                tracker.new_line()\n",
        "            if i % 1000 == 0:\n",
        "              torch.save(self.model.state_dict(), './StylEx.pt')\n",
        "def load_classifier(path):\n",
        "  device = 'cuda'\n",
        "  model = torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "  num_ftrs = model.classifier.in_features\n",
        "  model.classifier = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 500),\n",
        "    nn.Linear(500, 2)\n",
        "    )\n",
        "  model = model.to(device)\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  return model\n",
        "\n",
        "def main(load_model=False):\n",
        "    \"\"\"\n",
        "    ### Train StyleGAN2\n",
        "    \"\"\"\n",
        "\n",
        "    # Create an experiment\n",
        "    experiment.create(name='stylegan2')\n",
        "    # Create configurations object\n",
        "    configs = Configs()\n",
        "\n",
        "    # Set configurations and override some\n",
        "    experiment.configs(configs, {\n",
        "        'device.cuda_device': 0,\n",
        "        'image_size': 32,\n",
        "        'log_generated_interval': 200,\n",
        "        'load_model' : load_model  \n",
        "        })\n",
        "\n",
        "    # Initialize\n",
        "    configs.init()\n",
        "    # Set models for saving and loading\n",
        "    experiment.add_pytorch_models(mapping_network=configs.model.random_mapping,\n",
        "                                  generator=configs.model.generator,\n",
        "                                  discriminator=configs.model.discriminator)\n",
        "\n",
        "    # Start the experiment\n",
        "    with experiment.start():\n",
        "        # Run the training loop\n",
        "        configs.train()"
      ],
      "metadata": {
        "id": "9wCuQtrlkIQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "a1ea2d40-100b-41ba-ebd3-dcf0ee012e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main(load_model=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RkEOBbv2yFcP",
        "outputId": "93c6aa3e-cf50-40dc-f9b8-dafe55cb29b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device...\n",
              "  Prepare device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t8.04ms</span>\n",
              "Prepare device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t13.30ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">62ad2c7e7c5311ec8f380242ac1c0002</span>\n",
              "\t[dirty]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong><span style=\"color: #DDB62B\">Still updating app.labml.ai, please wait for it to complete...</span></strong></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-414e3ec9e8eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-957e2494bd0e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(load_model)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;31m# Run the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-957e2494bd0e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Take a training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_generated_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-957e2494bd0e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0;31m# Reconstructed images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mrecon_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0;31m# Get generator loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b8f862348971>\u001b[0m in \u001b[0;36mreconstruct\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Reconstructed latent space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d6b62df82836>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, w, input_noise)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The first style block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_noise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Get first rgb image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d6b62df82836>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, w, noise)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Weight modulated convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Scale and add noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d6b62df82836>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, s)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Reshape the scales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Get [learning rate equalized weights](#equalized_weight)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VF3kdKWb6WxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = config.generate_images(32)[0].cpu().detach()[0]\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "QoPAuNJOO1PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "\n",
        "def step(self, idx: int):\n",
        "        \"\"\"\n",
        "        ### Training Step\n",
        "        \"\"\"\n",
        "\n",
        "        # Train the discriminator\n",
        "\n",
        "        #print(idx)\n",
        "\n",
        "        with monit.section('Discriminator'):\n",
        "            # Reset gradients\n",
        "            self.discriminator_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Update `mode`. Set whether to log activation\n",
        "                with self.mode.update(is_log_activations=(idx + 1) % self.log_generated_interval == 0):\n",
        "                    # Sample images from generator\n",
        "                    generated_images, _ = self.generate_images(self.batch_size)\n",
        "                    # Discriminator classification for generated images\n",
        "                    fake_output = self.model.discriminator(generated_images.detach())\n",
        "\n",
        "                    # Get real images from the data loader\n",
        "                    r_img, lbl = next(self.loader)\n",
        "                    real_images = r_img.to(self.device)\n",
        "                    # We need to calculate gradients w.r.t. real images for gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        real_images.requires_grad_()\n",
        "                    # Discriminator classification for real images\n",
        "                    real_output = self.model.discriminator(real_images)\n",
        "\n",
        "                    # Get discriminator loss\n",
        "                    real_loss, fake_loss = self.discriminator_loss(real_output, fake_output)\n",
        "                    disc_loss = real_loss + fake_loss\n",
        "\n",
        "                    # Add gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        # Calculate and log gradient penalty\n",
        "                        gp = self.gradient_penalty(real_images, real_output)\n",
        "                        tracker.add('loss.gp', gp)\n",
        "                        # Multiply by coefficient and add gradient penalty\n",
        "                        disc_loss = disc_loss + 0.5 * self.gradient_penalty_coefficient * gp * self.lazy_gradient_penalty_interval\n",
        "\n",
        "                    # Compute gradients\n",
        "                    disc_loss.backward()\n",
        "\n",
        "                    # Log discriminator loss\n",
        "                    tracker.add('loss.discriminator_real', real_loss)\n",
        "                    tracker.add('loss.discriminator_fake', fake_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('discriminator', self.model.discriminator)\n",
        "            \n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.discriminator.parameters(), max_norm=1.0)\n",
        "            # Take optimizer step\n",
        "            self.discriminator_optimizer.step()\n",
        "\n",
        "        # Train the generator\n",
        "        with monit.section('Generator'):\n",
        "            # Reset gradients\n",
        "            self.generator_optimizer.zero_grad()\n",
        "            self.mapping_network_optimizer.zero_grad()\n",
        "            self.encoder_optimizer.zero_grad()\n",
        "            self.affine_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Sample images from generator\n",
        "                generated_images, w = self.generate_images(self.batch_size)\n",
        "                # Discriminator classification for generated images\n",
        "                fake_output = self.model.discriminator(generated_images)\n",
        "                # Reconstructed images\n",
        "                recon_images = self.model.reconstruct(real_images)\n",
        "\n",
        "                # Get generator loss\n",
        "                gen_loss_rec = self.reconstruct_loss(real_images, \n",
        "                                                     recons_images)\n",
        "                # Adversarial Loss\n",
        "                gen_loss_adv = self.generator_loss(fake_output)\n",
        "                # Classifier Loss\n",
        "                gen_loss_class = self.KL_loss(\n",
        "                    self.model.classify(real_images),\n",
        "                    self.model.classify(recon_images)\n",
        "                    )\n",
        "                # Combine losses using config weight\n",
        "                gen_loss =  self.loss_weights[0]*gen_loss_rec + \\\n",
        "                            self.loss_weights[1]*gen_loss_adv + \\\n",
        "                            self.loss_weights[2]*gen_loss_class\n",
        "\n",
        "                # Add path length penalty\n",
        "                if idx > self.lazy_path_penalty_after and \\\n",
        "                  (idx + 1) % self.lazy_path_penalty_interval == 0:\n",
        "                    # Calculate path length penalty\n",
        "                    plp = self.path_length_penalty(w, generated_images)\n",
        "                    # Ignore if `nan`\n",
        "                    if not torch.isnan(plp):\n",
        "                        tracker.add('loss.plp', plp)\n",
        "                        gen_loss = gen_loss + self.loss_weights[3] * plp\n",
        "\n",
        "                # Calculate gradients\n",
        "                gen_loss.backward()\n",
        "\n",
        "                # Log generator loss\n",
        "                tracker.add('loss.generator', gen_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('generator', self.model.generator)\n",
        "                tracker.add('mapping_network', self.model.random_mapping)\n",
        "\n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.generator.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.random_mapping.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Take optimizer step\n",
        "            self.generator_optimizer.step()\n",
        "            self.mapping_network_optimizer.step()\n",
        "            self.encoder_optimizer.step()\n",
        "            self.affine_optimizer.step()\n",
        "\n",
        "        # Log generated images\n",
        "        if (idx + 1) % self.log_generated_interval == 0:\n",
        "            tracker.add('generated', torch.cat([generated_images[:6], real_images[:3]], dim=0))\n",
        "        # Save model checkpoints\n",
        "        if (idx + 1) % self.save_checkpoint_interval == 0:\n",
        "            experiment.save_checkpoint()\n",
        "\n",
        "        # Flush tracker\n",
        "        tracker.save()\n",
        "        #print(disc_loss + gen_loss)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KyjXRbj-9ZiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}