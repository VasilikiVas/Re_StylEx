{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OHVmbIrPJV2G",
        "outputId": "4b53c33e-b755-4d10-da3f-6c6764ef3694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU75gboUp8T8",
        "outputId": "ee8a251d-ab4a-4e50-f1c7-f134ca2ce541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/StyleEX/'\n",
            "/content/drive/MyDrive/StyleEX\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/StyleEX/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtRwCmEknfdm",
        "outputId": "22612c6c-2a6d-4031-a870-7d5fca7d4886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mtrain\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls data/stylegan2/Dogs-Cats/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JMI3N_Qr_Rh",
        "outputId": "99b59fab-aa1f-4a7f-c262-557964a9be21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: labml in /usr/local/lib/python3.7/dist-packages (0.4.144)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml) (3.13)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml) (3.1.26)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.0)\n",
            "Requirement already satisfied: labml_helpers in /usr/local/lib/python3.7/dist-packages (0.4.84)\n",
            "Requirement already satisfied: labml>=0.4.133 in /usr/local/lib/python3.7/dist-packages (from labml_helpers) (0.4.144)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from labml_helpers) (1.10.0+cu111)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.133->labml_helpers) (3.13)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.133->labml_helpers) (3.1.26)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.133->labml_helpers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.133->labml_helpers) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.133->labml_helpers) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.133->labml_helpers) (5.0.0)\n",
            "Requirement already satisfied: labml_nn in /usr/local/lib/python3.7/dist-packages (0.4.118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml_nn) (1.19.5)\n",
            "Requirement already satisfied: labml>=0.4.135 in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.4.144)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.11.1+cu111)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.4.0)\n",
            "Requirement already satisfied: labml-helpers>=0.4.84 in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.4.84)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from labml_nn) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from labml_nn) (0.11.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.135->labml_nn) (3.1.26)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.135->labml_nn) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.135->labml_nn) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.135->labml_nn) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.135->labml_nn) (5.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext->labml_nn) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext->labml_nn) (4.62.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml_nn) (2.10)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->labml_nn) (7.1.2)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.4.1)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.62.3)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->lpips) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (7.1.2)\n",
            "Requirement already satisfied: tfrecord in /usr/local/lib/python3.7/dist-packages (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfrecord) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from tfrecord) (3.17.3)\n",
            "Requirement already satisfied: crc32c in /usr/local/lib/python3.7/dist-packages (from tfrecord) (2.2.post0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->tfrecord) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install labml\n",
        "%pip install labml_helpers\n",
        "%pip install labml_nn\n",
        "%pip install lpips\n",
        "%pip install tfrecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2W3G2kn5j_Dw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Architecture Utils\n",
        "import math\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class MappingNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"mapping_network\"></a>\n",
        "    ## Mapping Network\n",
        "    ![Mapping Network](mapping_network.svg)\n",
        "    This is an MLP with 8 linear layers.\n",
        "    The mapping network maps the latent vector $z \\in \\mathcal{W}$\n",
        "    to an intermediate latent space $w \\in \\mathcal{W}$.\n",
        "    $\\mathcal{W}$ space will be disentangled from the image space\n",
        "    where the factors of variation become more linear.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features: int, n_layers: int, outputs: int = 0):\n",
        "        \"\"\"\n",
        "        * `features` is the number of features in $z$ and $w$\n",
        "        * `n_layers` is the number of layers in the mapping network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if outputs == 0:\n",
        "          outputs = features\n",
        "\n",
        "        # Create the MLP\n",
        "        layers = []\n",
        "        for i in range(n_layers-1):\n",
        "            # [Equalized learning-rate linear layers](#equalized_linear)\n",
        "            layers.append(EqualizedLinear(features, features))\n",
        "            # Leaky Relu\n",
        "            layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        # Change sizes in last layer\n",
        "        layers.append(EqualizedLinear(features, outputs))\n",
        "        layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, z: torch.Tensor):\n",
        "        # Normalize $z$\n",
        "        z = F.normalize(z, dim=1)\n",
        "        # Map $z$ to $w$\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"generator\"></a>\n",
        "    ## StyleGAN2 Generator\n",
        "    ![Generator](style_gan2.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n",
        "    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n",
        "    The generator starts with a learned constant.\n",
        "    Then it has a series of blocks. The feature map resolution is doubled at each block\n",
        "    Each block outputs an RGB image and they are scaled up and summed to get the final RGB image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, d_latent: int, n_features: int = 32, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (final block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Calculate the number of features for each block\n",
        "        #\n",
        "        # Something like `[512, 512, 256, 128, 64, 32]`\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\n",
        "        # Number of generator blocks\n",
        "        self.n_blocks = len(features)\n",
        "\n",
        "        # Trainable $4 \\times 4$ constant\n",
        "        self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\n",
        "\n",
        "        # First style block for $4 \\times 4$ resolution and layer to get RGB\n",
        "        self.style_block = StyleBlock(d_latent, features[0], features[0])\n",
        "        self.to_rgb = ToRGB(d_latent, features[0])\n",
        "\n",
        "        # Generator blocks\n",
        "        blocks = [GeneratorBlock(d_latent, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        # $2 \\times$ up sampling layer. The feature space is up sampled\n",
        "        # at each block\n",
        "        self.up_sample = UpSample()\n",
        "\n",
        "    def forward(self, w: torch.Tensor, input_noise: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]]):\n",
        "        \"\"\"\n",
        "        * `w` is $w$. In order to mix-styles (use different $w$ for different layers), we provide a separate\n",
        "        $w$ for each [generator block](#generator_block). It has shape `[n_blocks, batch_size, d_latent]`.\n",
        "        * `input_noise` is the noise for each block.\n",
        "        It's a list of pairs of noise sensors because each block (except the initial) has two noise inputs\n",
        "        after each convolution layer (see the diagram).\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size\n",
        "        batch_size = w.shape[1]\n",
        "\n",
        "        # Expand the learned constant to match batch size\n",
        "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "        # The first style block\n",
        "        x = self.style_block(x, w[0], input_noise[0][0])\n",
        "        # Get first rgb image\n",
        "        rgb = self.to_rgb(x, w[0])\n",
        "\n",
        "        # Evaluate rest of the blocks\n",
        "        for i in range(1, self.n_blocks):\n",
        "            # Up sample the feature map\n",
        "            x = self.up_sample(x)\n",
        "            # Run it through the [generator block](#generator_block)\n",
        "            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\n",
        "            # Up sample the RGB image and add to the rgb from the block\n",
        "            rgb = self.up_sample(rgb) + rgb_new\n",
        "\n",
        "        # Return the final RGB image\n",
        "        return rgb\n",
        "\n",
        "    def synthesis(self, w: torch.Tensor, s: torch.Tensor, input_noise: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]], s_rgb: torch.Tensor = None,):\n",
        "        \"\"\"\n",
        "        * `w` is $w$. In order to mix-styles (use different $w$ for different layers), we provide a separate\n",
        "        $w$ for each [generator block](#generator_block). It has shape `[n_blocks, batch_size, d_latent]`.\n",
        "        * `input_noise` is the noise for each block.\n",
        "        It's a list of pairs of noise sensors because each block (except the initial) has two noise inputs\n",
        "        after each convolution layer (see the diagram).\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size\n",
        "        batch_size = w.shape[1]\n",
        "\n",
        "        # Expand the learned constant to match batch size\n",
        "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "        # The first style block\n",
        "        x = self.style_block.synthesis(x, s[0].unsqueeze(0), input_noise[0][1])\n",
        "        # Get first rgb image\n",
        "        if s_rgb != None:\n",
        "          rgb = self.to_rgb.synthesis(x, s_rgb[0])\n",
        "        else:\n",
        "          rgb = self.to_rgb(x, w[0])\n",
        "\n",
        "        # Evaluate rest of the blocks\n",
        "        for i in range(1, self.n_blocks):\n",
        "            # Up sample the feature map\n",
        "            x = self.up_sample(x)\n",
        "            # Run it through the [generator block](#generator_block)\n",
        "            x, rgb_new = self.blocks[i - 1].given_styles(x, [s[2*i-1],s[2*i]], input_noise[i], s_rgb = s_rgb[i])\n",
        "            # Up sample the RGB image and add to the rgb from the block\n",
        "            rgb = self.up_sample(rgb) + rgb_new\n",
        "\n",
        "        # Return the final RGB image\n",
        "        return rgb\n",
        "  def __noise__(self, batch_size: int, zeros:bool = False):\n",
        "    noise = []\n",
        "    resolution = 4\n",
        "    if zeros:\n",
        "      for i in range(self.n_gen_blocks):\n",
        "        if i == 0:\n",
        "          n1 = None\n",
        "        else:\n",
        "          n1 = torch.zeros(batch_size, 1, resolution, resolution)\n",
        "        n2 = torch.zeros(batch_size, 1, resolution, resolution)\n",
        "        noise.append((n1, n2))\n",
        "        resolution *= 2\n",
        "    else:\n",
        "      for i in range(self.n_gen_blocks):\n",
        "        if i == 0:\n",
        "          n1 = None\n",
        "        else:\n",
        "          n1 = torch.randn(batch_size, 1, resolution, resolution)\n",
        "        n2 = torch.randn(batch_size, 1, resolution, resolution)\n",
        "        noise.append((n1, n2))\n",
        "        resolution *= 2\n",
        "    return noise\n",
        "\n",
        "    def image_given_dlatent(self, w: torch.Tensor, s: torch.Tensor, input_noise: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]], s_rgb: torch.Tensor = None,):\n",
        "        return self.synthesis(w, s, input_noise, s_rgb)\n",
        "\n",
        "    def style_vector_calculator(self, w: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `w` is $w$. In order to mix-styles (use different $w$ for different layers), we provide a separate\n",
        "        $w$ for each [generator block](#generator_block). It has shape `[n_blocks, batch_size, d_latent]`.\n",
        "        * `input_noise` is the noise for each block.\n",
        "        It's a list of pairs of noise sensors because each block (except the initial) has two noise inputs\n",
        "        after each convolution layer (see the diagram).\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size\n",
        "        batch_size = w.shape[1]\n",
        "\n",
        "        style_vector_block = []\n",
        "        style_vector_torgb = []\n",
        "      \n",
        "        # The first style block\n",
        "        s = self.style_block.get_styles(w[0])\n",
        "        style_vector_block.append(s)\n",
        "\n",
        "        # Get first rgb image\n",
        "        rgb = self.to_rgb.get_styles(w[0])\n",
        "        style_vector_torgb.append(rgb)\n",
        "\n",
        "        # Evaluate rest of the blocks\n",
        "        for i in range(1, self.n_blocks):\n",
        "            # Run it through the [generator block](#generator_block)\n",
        "            s, rgb = self.blocks[i - 1].get_styles_from_block(w[i])\n",
        "            style_vector_block += s\n",
        "            style_vector_torgb += rgb\n",
        "\n",
        "        # Return the final RGB image\n",
        "        return style_vector_torgb, style_vector_block\n",
        "\n",
        "class GeneratorBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"generator_block\"></a>\n",
        "    ### Generator Block\n",
        "    ![Generator block](generator_block.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n",
        "    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n",
        "    The generator block consists of two [style blocks](#style_block) ($3 \\times 3$ convolutions with style modulation)\n",
        "    and an RGB output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First [style block](#style_block) changes the feature map size to `out_features`\n",
        "        self.style_block1 = StyleBlock(d_latent, in_features, out_features)\n",
        "        # Second [style block](#style_block)\n",
        "        self.style_block2 = StyleBlock(d_latent, out_features, out_features)\n",
        "        self.s1_length = in_features\n",
        "        self.s2_length = out_features\n",
        "        # *toRGB* layer\n",
        "        self.to_rgb = ToRGB(d_latent, out_features)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tuple of two noise tensors of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # First style block with first noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        x = self.style_block1(x, w, noise[0])\n",
        "        # Second style block with second noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        x = self.style_block2(x, w, noise[1])\n",
        "\n",
        "        # Get RGB image\n",
        "        rgb = self.to_rgb(x, w)\n",
        "\n",
        "        # Return feature map and rgb image\n",
        "        return x, rgb\n",
        "    \n",
        "    def given_styles(self, x: torch.Tensor, s: torch.Tensor, noise: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]], s_rgb: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tuple of two noise tensors of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # First style block with first noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        if len(s[0].shape) == 1:\n",
        "          s[0].unsqueeze_(0)\n",
        "        if len(s[1].shape) == 1:\n",
        "          s[1].unsqueeze_(0)\n",
        "        x = self.style_block1.synthesis(x, s[0], noise[0])\n",
        "        # Second style block with second noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        x = self.style_block2.synthesis(x, s[1], noise[1])\n",
        "\n",
        "        # Get RGB image\n",
        "        rgb = self.to_rgb.synthesis(x, s_rgb.unsqueeze(0))\n",
        "\n",
        "        # Return feature map and rgb image\n",
        "        return x, rgb\n",
        "\n",
        "    def get_styles_from_block(self, w: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tuple of two noise tensors of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # First style block with first noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        s1 = self.style_block1.get_styles(w)\n",
        "        # Second style block with second noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        s2 = self.style_block2.get_styles(w)\n",
        "\n",
        "        # Get RGB image\n",
        "        rgb = self.to_rgb.get_styles(w)\n",
        "\n",
        "        # Return feature map and rgb image\n",
        "        print(s1.shape)\n",
        "        print(s2.shape)\n",
        "        print()\n",
        "        return [s1, s2], rgb\n",
        "\n",
        "class StyleBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"style_block\"></a>\n",
        "    ### Style Block\n",
        "    ![Style block](style_block.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is single channel).*---\n",
        "    Style block has a weight modulation convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n",
        "        # an [equalized learning-rate linear layer](#equalized_linear)\n",
        "        self.to_style = EqualizedLinear(d_latent, in_features, bias=1.0)\n",
        "        # Weight modulated convolution layer\n",
        "        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n",
        "        # Noise scale\n",
        "        self.scale_noise = nn.Parameter(torch.zeros(1))\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        self.style_length = in_features\n",
        "\n",
        "        # Activation function\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Optional[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tensor of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$\n",
        "        s = self.to_style(w)\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, s)\n",
        "        # Scale and add noise\n",
        "        if noise is not None:\n",
        "            x = x + self.scale_noise[None, :, None, None] * noise\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "    def synthesis(self, x: torch.Tensor, s: torch.Tensor, noise: Optional[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tensor of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, s)\n",
        "        # Scale and add noise\n",
        "        if noise is not None:\n",
        "            x = x + self.scale_noise[None, :, None, None] * noise\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "    def get_styles(self, w: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tensor of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$\n",
        "        return self.to_style(w)\n",
        "\n",
        "class ToRGB(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"to_rgb\"></a>\n",
        "    ### To RGB\n",
        "    ![To RGB](to_rgb.svg)\n",
        "    ---*$A$ denotes a linear layer.*---\n",
        "    Generates an RGB image from a feature map using $1 \\times 1$ convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `features` is the number of features in the feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n",
        "        # an [equalized learning-rate linear layer](#equalized_linear)\n",
        "        self.to_style = EqualizedLinear(d_latent, features, bias=1.0)\n",
        "\n",
        "        # Weight modulated convolution layer without demodulation\n",
        "        self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.zeros(3))\n",
        "        # Activation function\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$\n",
        "        style = self.to_style(w)\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, style)\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "    def synthesis(self, x: torch.Tensor, style: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        \"\"\"\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, style)\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "    def get_styles(self, w: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$       \n",
        "        return self.to_style(w)\n",
        "\n",
        "\n",
        "class Conv2dWeightModulate(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Convolution with Weight Modulation and Demodulation\n",
        "    This layer scales the convolution weights by the style vector and demodulates by normalizing it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, kernel_size: int,\n",
        "                 demodulate: float = True, eps: float = 1e-8):\n",
        "        \"\"\"\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        * `kernel_size` is the size of the convolution kernel\n",
        "        * `demodulate` is flag whether to normalize weights by its standard deviation\n",
        "        * `eps` is the $\\epsilon$ for normalizing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Number of output features\n",
        "        self.out_features = out_features\n",
        "        # Whether to normalize weights\n",
        "        self.demodulate = demodulate\n",
        "        # Padding size\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "\n",
        "        # [Weights parameter with equalized learning rate](#equalized_weight)\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        # $\\epsilon$\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor, s: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `s` is style based scaling tensor of shape `[batch_size, in_features]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size, height and width\n",
        "        b, _, h, w = x.shape\n",
        "\n",
        "        # Reshape the scales\n",
        "        s = s[:, None, :, None, None]\n",
        "        # Get [learning rate equalized weights](#equalized_weight)\n",
        "        weights = self.weight()[None, :, :, :, :]\n",
        "        # $$w`_{i,j,k} = s_i * w_{i,j,k}$$\n",
        "        # where $i$ is the input channel, $j$ is the output channel, and $k$ is the kernel index.\n",
        "        #\n",
        "        # The result has shape `[batch_size, out_features, in_features, kernel_size, kernel_size]`\n",
        "        weights = weights * s\n",
        "\n",
        "        # Demodulate\n",
        "        if self.demodulate:\n",
        "            # $$\\sigma_j = \\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}$$\n",
        "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
        "            # $$w''_{i,j,k} = \\frac{w'_{i,j,k}}{\\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}}$$\n",
        "            weights = weights * sigma_inv\n",
        "\n",
        "        # Reshape `x`\n",
        "        x = x.reshape(1, -1, h, w)\n",
        "\n",
        "        # Reshape weights\n",
        "        _, _, *ws = weights.shape\n",
        "        weights = weights.reshape(b * self.out_features, *ws)\n",
        "\n",
        "        # Use grouped convolution to efficiently calculate the convolution with sample wise kernel.\n",
        "        # i.e. we have a different kernel (weights) for each sample in the batch\n",
        "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
        "\n",
        "        # Reshape `x` to `[batch_size, out_features, height, width]` and return\n",
        "        return x.reshape(-1, self.out_features, h, w)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, PATH):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = load_classifier(PATH)\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.model(x)\n",
        "        \n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"discriminator\"></a>\n",
        "    ## StyleGAN 2 Discriminator\n",
        "    ![Discriminator](style_gan2_disc.svg)\n",
        "    Discriminator first transforms the image to a feature map of the same resolution and then\n",
        "    runs it through a series of blocks with residual connections.\n",
        "    The resolution is down-sampled by $2 \\times$ at each block while doubling the\n",
        "    number of features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, n_features: int = 64, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (first block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer to convert RGB image to a feature map with `n_features` number of features.\n",
        "        self.from_rgb = nn.Sequential(\n",
        "            EqualizedConv2d(3, n_features, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
        "        n_blocks = len(features) - 1\n",
        "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.std_dev = MiniBatchStdDev()\n",
        "        final_features = features[-1] + 1\n",
        "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
        "        self.final = EqualizedLinear(2 * 2 * final_features, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        x = x - 0.5\n",
        "        x = self.from_rgb(x)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        x = self.std_dev(x)\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.final(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"discriminator\"></a>\n",
        "    ## StyleGAN 2 Discriminator\n",
        "    ![Discriminator](style_gan2_disc.svg)\n",
        "    Discriminator first transforms the image to a feature map of the same resolution and then\n",
        "    runs it through a series of blocks with residual connections.\n",
        "    The resolution is down-sampled by $2 \\times$ at each block while doubling the\n",
        "    number of features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, d_latent: int, n_features: int = 64, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (first block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer to convert RGB image to a feature map with `n_features` number of features.\n",
        "        self.from_rgb = nn.Sequential(\n",
        "            EqualizedConv2d(3, n_features, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
        "        n_blocks = len(features) - 1\n",
        "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.std_dev = MiniBatchStdDev()\n",
        "        final_features = features[-1] + 1\n",
        "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
        "        self.final = EqualizedLinear(2 * 2 * final_features, d_latent)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        #print(x)\n",
        "        x = x - 0.5\n",
        "        x = self.from_rgb(x)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        x = self.std_dev(x)\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(DownSample(),\n",
        "                                      EqualizedConv2d(in_features, out_features, kernel_size=1))\n",
        "        self.block = nn.Sequential(\n",
        "            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "        self.down_sample = DownSample()\n",
        "        self.scale = 1 / math.sqrt(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        x = self.block(x)\n",
        "        x = self.down_sample(x)\n",
        "        return (x + residual) * self.scale\n",
        "\n",
        "\n",
        "class MiniBatchStdDev(nn.Module):\n",
        "\n",
        "    def __init__(self, group_size: int = 4):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.shape[0] % self.group_size == 0\n",
        "        grouped = x.view(self.group_size, -1)\n",
        "        std = torch.sqrt(grouped.var(dim=0) + 1e-8)\n",
        "        std = std.mean().view(1, 1, 1, 1)\n",
        "        b, _, h, w = x.shape\n",
        "        std = std.expand(b, -1, h, w)\n",
        "        return torch.cat([x, std], dim=1)\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Smoothing layer\n",
        "        self.smooth = Smooth()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.smooth(x)\n",
        "        return F.interpolate(x, (x.shape[2] // 2, x.shape[3] // 2), mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.smooth = Smooth()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.smooth(self.up_sample(x))\n",
        "\n",
        "\n",
        "class Smooth(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kernel = [[1, 2, 1],\n",
        "                  [2, 4, 2],\n",
        "                  [1, 2, 1]]\n",
        "        kernel = torch.tensor([[kernel]], dtype=torch.float)\n",
        "        kernel /= kernel.sum()\n",
        "        self.kernel = nn.Parameter(kernel, requires_grad=False)\n",
        "        self.pad = nn.ReplicationPad2d(1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.view(-1, 1, h, w)\n",
        "        x = self.pad(x)\n",
        "        x = F.conv2d(x, self.kernel)\n",
        "        return x.view(b, c, h, w)\n",
        "\n",
        "\n",
        "class EqualizedLinear(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: float = 0.):\n",
        "        super().__init__()\n",
        "        self.weight = EqualizedWeight([out_features, in_features])\n",
        "        self.bias = nn.Parameter(torch.ones(out_features) * bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.linear(x, self.weight(), bias=self.bias)\n",
        "\n",
        "\n",
        "class EqualizedConv2d(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int,\n",
        "                 kernel_size: int, padding: int = 0):\n",
        "        super().__init__()\n",
        "        self.padding = padding\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        self.bias = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.conv2d(x, self.weight(), bias=self.bias, padding=self.padding)\n",
        "\n",
        "\n",
        "class EqualizedWeight(nn.Module):\n",
        "\n",
        "    def __init__(self, shape: List[int]):\n",
        "        super().__init__()\n",
        "        self.c = 1 / math.sqrt(np.prod(shape[1:]))\n",
        "        self.weight = nn.Parameter(torch.randn(shape))\n",
        "    def forward(self):\n",
        "        return self.weight * self.c\n",
        "\n",
        "\n",
        "class GradientPenalty(nn.Module):\n",
        "    def forward(self, x: torch.Tensor, d: torch.Tensor):\n",
        "        batch_size = x.shape[0]\n",
        "        gradients, *_ = torch.autograd.grad(outputs=d,\n",
        "                                            inputs=x,\n",
        "                                            grad_outputs=d.new_ones(d.shape),\n",
        "                                            create_graph=True)\n",
        "        \n",
        "        gradients = gradients.reshape(batch_size, -1)\n",
        "        norm = gradients.norm(2, dim=-1)\n",
        "        return torch.mean(norm ** 2)\n",
        "\n",
        "\n",
        "class PathLengthPenalty(nn.Module):\n",
        "    def __init__(self, beta: float):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "\n",
        "    def forward(self, w: torch.Tensor, x: torch.Tensor):\n",
        "        device = x.device\n",
        "        image_size = x.shape[2] * x.shape[3]\n",
        "        y = torch.randn(x.shape, device=device)\n",
        "        output = (x * y).sum() / math.sqrt(image_size)\n",
        "\n",
        "        gradients, *_ = torch.autograd.grad(outputs=output,\n",
        "                                            inputs=w,\n",
        "                                            grad_outputs=torch.ones(output.shape, device=device),\n",
        "                                            create_graph=True)\n",
        "        \n",
        "        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\n",
        "\n",
        "        if self.steps > 0:\n",
        "            a = self.exp_sum_a / (1 - self.beta ** self.steps)\n",
        "            loss = torch.mean((norm - a) ** 2)\n",
        "        else:\n",
        "            loss = norm.new_tensor(0)\n",
        "\n",
        "        mean = norm.mean().detach()\n",
        "        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n",
        "        self.steps.add_(1.)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Utils \n",
        "from typing import Optional, Tuple, List\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "import pickle\n",
        "\n",
        "def make_animation(image: np.ndarray,\n",
        "                   resolution: int,\n",
        "                   figsize: Tuple[int, int] = (20, 8)):\n",
        "  fig = plt.figure(1, figsize=figsize)\n",
        "  _ = plt.gca()\n",
        "\n",
        "  def transpose_image(image):\n",
        "    image_reshape = image.reshape([-1, resolution, resolution, 3])\n",
        "    return image_reshape.transpose([1, 0, 2, 3]).reshape([resolution, -1, 3])\n",
        "  im = plt.imshow(transpose_image(image[:, :resolution, :]),\n",
        "                  interpolation='none')\n",
        "  def animate_func(i):\n",
        "    im.set_array(transpose_image(image[:, resolution*i:resolution*(i+1), :]))\n",
        "    return [im]\n",
        "\n",
        "  animation = matplotlib.animation.FuncAnimation(\n",
        "      fig, animate_func, frames=image.shape[1] // resolution, interval=600)\n",
        "\n",
        "  plt.close(1)\n",
        "  return animation\n",
        "\n",
        "\n",
        "def show_image(image, fmt='png'):\n",
        "  if image.dtype == np.float32:\n",
        "    image = np.uint8(image * 127.5 + 127.5)\n",
        "  if image.shape[0] == 3:\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "  bytes_io = BytesIO()\n",
        "  Image.fromarray(image).save(bytes_io, fmt)\n",
        "\n",
        "  IPython.display.display(IPython.display.Image(data=bytes_io.getvalue()))\n",
        "\n",
        "\n",
        "def filter_unstable_images(style_change_effect: np.ndarray,\n",
        "                           effect_threshold: float = 0.3,\n",
        "                           num_indices_threshold: int = 750) -> np.ndarray:\n",
        "  \"\"\"Filters out images which are affected by too many S values.\"\"\"\n",
        "  unstable_images = (\n",
        "      torch.sum(torch.abs(style_change_effect) > effect_threshold, dim=(1, 2, 3)) >\n",
        "      num_indices_threshold)\n",
        "  style_change_effect[unstable_images] = 0\n",
        "  return style_change_effect\n",
        "\n",
        "\n",
        "# @tf.function\n",
        "# def call_synthesis(generator: networks.Generator,\n",
        "#                    dlatents_in: tf.Tensor,\n",
        "#                    conditioning_in: Optional[tf.Tensor] = None,\n",
        "#                    labels_in: Optional[tf.Tensor] = None,\n",
        "#                    training: bool = False,\n",
        "#                    num_layers: int = 14,\n",
        "#                    dlatent_size: int = 512) -> tf.Tensor:\n",
        "#   \"\"\"Calls the synthesis.\n",
        "\n",
        "#   Args:\n",
        "#     dlatents_in: the intermediate latent representation of shape [batch size,\n",
        "#       num_layers, dlatent_size].\n",
        "#     conditioning_in: Conditioning input to the synthesis network (can be an\n",
        "#       image or output from an encoder) of shape [minibatch, channels,\n",
        "#       resolution, resolution]. Set to None if unused.\n",
        "#     labels_in: of shape [batch_size, label_size]. Set to None if unused.\n",
        "#     training: Whether this is a training call.\n",
        "\n",
        "#   Returns:\n",
        "#     The output images and optional latent vector.\n",
        "\n",
        "#   \"\"\"\n",
        "#   if labels_in is not None:\n",
        "#     zero_labels = tf.zeros_like(labels_in)\n",
        "#     dlatents_labels = tf.tile(tf.expand_dims(zero_labels, 1), [1, num_layers, 1])\n",
        "#     if dlatent_size > 0:\n",
        "#       dlatents_expanded = tf.concat([dlatents_in, dlatents_labels], axis=2)\n",
        "#     else:\n",
        "#       dlatents_expanded = dlatents_labels\n",
        "#   else:\n",
        "#     if dlatent_size == 0:\n",
        "#       raise ValueError('Dlatents are empty and no labels were provided.')\n",
        "#     dlatents_expanded = dlatents_in\n",
        "#   # Evaluate synthesis network.\n",
        "#   style_vector_blocks, style_vector_torgb = generator.style_vector_calculator(\n",
        "#       dlatents_expanded[:, 0], training=training)\n",
        "#   if conditioning_in is not None:\n",
        "#     network_inputs = (style_vector_blocks, style_vector_torgb,\n",
        "#                       conditioning_in)\n",
        "#   else:\n",
        "#     network_inputs = (style_vector_blocks, style_vector_torgb)\n",
        "#   synthesis_results = generator.g_synthesis(network_inputs, training=training)\n",
        "\n",
        "#   # Return requested outputs.\n",
        "#   return tf.maximum(tf.minimum(synthesis_results, 1), -1)\n",
        "\n",
        "\n",
        "def discriminator_filter(style_change_effect: np.ndarray,\n",
        "                         all_dlatents: np.ndarray,\n",
        "                         generator: Generator,\n",
        "                         discriminator: None, # set to None until we ask Tim\n",
        "                         classifier: Classifier,\n",
        "                         sindex: int,\n",
        "                         style_min: float,\n",
        "                         style_max: float,\n",
        "                         class_index: int,\n",
        "                         num_images: int = 10,\n",
        "                         label_size: int = 2,\n",
        "                         change_threshold: float = 0.5,\n",
        "                         shift_size: float = 2,\n",
        "                         effect_threshold: float = 0.2,\n",
        "                         sindex_offset: int = 0) -> bool:\n",
        "  \"\"\"Returns false if changing the style index adds artifacts to the images.\n",
        "  Args:\n",
        "    style_change_effect: A shape of [num_images, 2, style_size, num_classes].\n",
        "      The effect of each change of style on specific direction on each image.\n",
        "    all_dlatents: The dlatents of each image, shape of [num_images,\n",
        "      dlatent_size].\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    discriminator: The discriminator model.\n",
        "    sindex: The style index.\n",
        "    style_min: The style min value in all images.\n",
        "    style_max: The style max value in all images.\n",
        "    class_index: The index of the class to check.\n",
        "    num_images: The number of images to do the disciminator_filter test.\n",
        "    label_size: The label size.\n",
        "    change_threshold: The maximal change allowed in the discriminator\n",
        "      prediction.\n",
        "    shift_size: The size to shift the style index.\n",
        "    effect_threshold: Used for choosing images that the classification was\n",
        "      changed enough.\n",
        "    sindex_offset: The offset of the style index if style_change_effect contains\n",
        "      some of the layers and not all styles.\n",
        "  \"\"\"\n",
        "  for style_sign_index in range(2):\n",
        "    images_idx = ((style_change_effect[:, style_sign_index, sindex,\n",
        "                                       class_index]) >\n",
        "                  effect_threshold).nonzero()[0]\n",
        "\n",
        "    images_idx = images_idx[:num_images]\n",
        "    dlatents = all_dlatents[images_idx]\n",
        "\n",
        "    for i in range(len(images_idx)):\n",
        "      cur_dlatent = dlatents[i:i + 1]\n",
        "      (discriminator_orig, \n",
        "       discriminator_change) = get_discriminator_results_given_dlatent(\n",
        "           dlatent=cur_dlatent,\n",
        "           generator=generator,\n",
        "           discriminator=discriminator,\n",
        "           classifier=classifier,\n",
        "           class_index=class_index,\n",
        "           sindex=sindex + sindex_offset,\n",
        "           s_style_min=style_min,\n",
        "           s_style_max=style_max,\n",
        "           style_direction_index=style_sign_index,\n",
        "           shift_size=shift_size,\n",
        "           label_size=label_size)\n",
        "\n",
        "      if np.abs(discriminator_orig - discriminator_change) > change_threshold:\n",
        "        return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def find_significant_styles_image_fraction(\n",
        "    style_change_effect: np.ndarray,\n",
        "    num_indices: int,\n",
        "    class_index: int,\n",
        "    generator: Generator,\n",
        "    classifier: Classifier,\n",
        "    all_dlatents: np.ndarray,\n",
        "    style_min: np.ndarray,\n",
        "    style_max: np.ndarray,\n",
        "    effect_threshold: float = 0.2,\n",
        "    min_changed_images_fraction: float = 0.03,\n",
        "    label_size: int = 2,\n",
        "    sindex_offset: int = 0,\n",
        "    discriminator: Optional[None] = None, #set to None until we ask Tim\n",
        "    discriminator_threshold: float = 0.2) -> List[Tuple[int, int]]:\n",
        "  \"\"\"Returns indices in the style vector which affect the classifier.\n",
        "  Args:\n",
        "    style_change_effect: A shape of [num_images, 2, style_size, num_classes].\n",
        "      The effect of each change of style on specific direction on each image.\n",
        "    num_indices: Number of styles in the result.\n",
        "    class_index: The index of the class to visualize.\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    all_dlatents: The dlatents of each image, shape of [num_images,\n",
        "      dlatent_size].\n",
        "    style_min: An array with the min value for each style index.\n",
        "    style_max: An array with the max value for each style index.\n",
        "    effect_threshold: Minimal change of classifier output to be considered.\n",
        "    min_changed_images_fraction: Minimal fraction of images which are changed.\n",
        "    label_size: The label size.\n",
        "    sindex_offset: The offset of the style index if style_change_effect contains\n",
        "      some of the layers and not all styles.\n",
        "    discriminator: The discriminator model. If None, don't filter style indices.\n",
        "    discriminator_threshold: Used in discriminator_filter to define the maximal\n",
        "      change allowed in the discriminator prediction.\n",
        "    \n",
        "  \"\"\"\n",
        "  effect_positive = np.sum(\n",
        "      style_change_effect[:, :, :, class_index] > effect_threshold, axis=0)\n",
        "  effect_positive = effect_positive.flatten()\n",
        "  all_sindices = []\n",
        "  sindices = np.argsort(effect_positive)[::-1]\n",
        "  if discriminator is not None:\n",
        "    print('Using discriminator...')\n",
        "  for sindex in sindices[:num_indices*2]:\n",
        "    if (effect_positive[sindex] <\n",
        "        min_changed_images_fraction * style_change_effect.shape[0]):\n",
        "      break\n",
        "    if discriminator is not None:\n",
        "      s_index = sindex % style_change_effect.shape[2]\n",
        "      if not discriminator_filter(\n",
        "          style_change_effect,\n",
        "          all_dlatents,\n",
        "          generator,\n",
        "          discriminator,\n",
        "          classifier,\n",
        "          s_index,\n",
        "          style_min[s_index + sindex_offset],\n",
        "          style_max[s_index + sindex_offset],\n",
        "          class_index,\n",
        "          label_size=label_size,\n",
        "          change_threshold=discriminator_threshold,\n",
        "          sindex_offset=sindex_offset):\n",
        "        continue\n",
        "    all_sindices.append(sindex)\n",
        "    if len(all_sindices) == num_indices:\n",
        "      break\n",
        "\n",
        "  return [(x // style_change_effect.shape[2],\n",
        "           (x % style_change_effect.shape[2]) + sindex_offset)\n",
        "          for x in all_sindices]\n",
        "\n",
        "\n",
        "def find_significant_styles(\n",
        "    style_change_effect: np.ndarray,\n",
        "    num_indices: int,\n",
        "    class_index: int,\n",
        "    discriminator: Optional[None], #set to None until we ask Tim\n",
        "    generator: Generator,\n",
        "    classifier: Classifier,\n",
        "    all_dlatents: np.ndarray,\n",
        "    style_min: np.ndarray,\n",
        "    style_max: np.ndarray,\n",
        "    max_image_effect: float = 0.2,\n",
        "    label_size: int = 2,\n",
        "    discriminator_threshold: float = 0.2,\n",
        "    sindex_offset: int = 0) -> List[Tuple[int, int]]:\n",
        "  \"\"\"Returns indices in the style vector which affect the classifier.\n",
        "  Args:\n",
        "    style_change_effect: A shape of [num_images, 2, style_size, num_classes].\n",
        "      The effect of each change of style on specific direction on each image.\n",
        "    num_indices: Number of styles in the result.\n",
        "    class_index: The index of the class to visualize.\n",
        "    discriminator: The discriminator model. If None, don't filter style indices.\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    all_dlatents: The dlatents of each image, shape of [num_images,\n",
        "      dlatent_size].\n",
        "    style_min: An array with the min value for each style index.\n",
        "    style_max: An array with the max value for each style index.\n",
        "    max_image_effect: Ignore contributions of styles if the previously found\n",
        "      styles changed the probability of the image by more than this threshold.\n",
        "    label_size: The label size.\n",
        "    discriminator_threshold: Used in discriminator_filter to define the maximal\n",
        "      change allowed in the discriminator prediction.\n",
        "    sindex_offset: The offset of the style index if style_change_effect contains\n",
        "      some of the layers and not all styles.\n",
        "  \"\"\"\n",
        "  print(style_change_effect.shape)\n",
        "  num_images = style_change_effect.shape[0]\n",
        "  style_effect_direction = np.maximum(\n",
        "      0, style_change_effect[:, :, :, class_index].reshape((num_images, -1)))\n",
        "\n",
        "  images_effect = np.zeros(num_images)\n",
        "  all_sindices = []\n",
        "  discriminator_removed = []\n",
        "  while len(all_sindices) < num_indices:\n",
        "    next_s = np.argmax(\n",
        "        np.mean(\n",
        "            style_effect_direction[images_effect < max_image_effect], axis=0))\n",
        "    # if discriminator is not None:\n",
        "    #   sindex = next_s % style_change_effect.shape[2]\n",
        "    #   if sindex == 0:\n",
        "    #     break\n",
        "    #   if not discriminator_filter(\n",
        "    #       style_change_effect=style_change_effect,\n",
        "    #       all_dlatents=all_dlatents,\n",
        "    #       generator=generator,\n",
        "    #       discriminator=discriminator,\n",
        "    #       classifier=classifier,\n",
        "    #       sindex=sindex,\n",
        "    #       style_min=style_min[sindex + sindex_offset],\n",
        "    #       style_max=style_max[sindex + sindex_offset],\n",
        "    #       class_index=class_index,\n",
        "    #       label_size=label_size,\n",
        "    #       change_threshold=discriminator_threshold,\n",
        "    #       sindex_offset=sindex_offset):\n",
        "    #     style_effect_direction[:, next_s] = np.zeros(num_images)\n",
        "    #     discriminator_removed.append(sindex)\n",
        "    #     continue\n",
        "\n",
        "    all_sindices.append(next_s)\n",
        "    images_effect += style_effect_direction[:, next_s]\n",
        "    style_effect_direction[:, next_s] = 0\n",
        "\n",
        "  return [(x // style_change_effect.shape[2],\n",
        "           (x % style_change_effect.shape[2]) + sindex_offset)\n",
        "          for x in all_sindices]\n",
        "\n",
        "\n",
        "def _float_features(values):\n",
        "   \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "   return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
        "\n",
        "\n",
        "def sindex_to_layer_idx_and_index(style_vector_block: list,\n",
        "                                  \n",
        "                                  sindex: int) -> Tuple[int, int]:\n",
        "  \n",
        "  LAYER_SHAPES = []\n",
        "  for item in style_vector_block:\n",
        "    LAYER_SHAPES.append([item.s1_length])\n",
        "    LAYER_SHAPES.append([item.s2_length])\n",
        "  \n",
        "  layer_shapes_cumsum = np.concatenate([[0], np.cumsum(LAYER_SHAPES)])\n",
        "  layer_idx = (layer_shapes_cumsum <= sindex).nonzero()[0][-1]\n",
        "\n",
        "  return layer_idx, sindex - layer_shapes_cumsum[layer_idx]\n",
        "\n",
        "# def get_classifier_results(generator: networks.Generator,\n",
        "#                            classifier: MobileNetV1,\n",
        "#                            expanded_dlatent: tf.Tensor,\n",
        "#                            use_softmax: bool = False):\n",
        "#   image = call_synthesis(generator, expanded_dlatent)\n",
        "#   # image = tf.transpose(image, (0, 2, 3, 1))\n",
        "#   # results = classifier(image, training=False)\n",
        "#   results = classifier(image)\n",
        "#   if use_softmax:\n",
        "#     return nn.Softmax(dim=1)(results).detach().numpy()[0]\n",
        "#   else:\n",
        "#     return results.numpy()[0]\n",
        "\n",
        "def draw_on_image(image: np.ndarray, number: float,\n",
        "                  font_file: str,\n",
        "                  font_fill: Tuple[int, int, int] = (0, 0, 0)) -> np.ndarray:\n",
        "  \"\"\"Draws a number on the top left corner of the image.\"\"\"\n",
        "  fnt = ImageFont.truetype(font_file, 20)\n",
        "  out_image = Image.fromarray((image * 127.5 + 127.5).astype(np.uint8))\n",
        "  draw = ImageDraw.Draw(out_image)\n",
        "  draw.rectangle(tuple([0, 0, 70, 20]),fill=tuple([255, 255, 255]))\n",
        "  draw.multiline_text((10, 10), ('%.3f' % number), font=fnt, fill=font_fill)\n",
        "  return np.array(out_image)\n",
        "\n",
        "\n",
        "def generate_change_image_given_dlatent(\n",
        "    dlatent: np.ndarray,\n",
        "    generator: Generator,\n",
        "    classifier: Optional[Classifier],\n",
        "    class_index: int,\n",
        "    sindex: int,\n",
        "    s_style_min: float,\n",
        "    s_style_max: float,\n",
        "    style_direction_index: int,\n",
        "    shift_size: float,\n",
        "    label_size: int = 2,\n",
        "    num_layers: int = 14,\n",
        "    model = None \n",
        ") -> Tuple[np.ndarray, float, float]:\n",
        "  \"\"\"Modifies an image given the dlatent on a specific S-index.\n",
        "  Args:\n",
        "    dlatent: The image dlatent, with sape [dlatent_size].\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    classifier: The classifier to visualize.\n",
        "    class_index: The index of the class to visualize.\n",
        "    sindex: The specific style index to visualize.\n",
        "    s_style_min: The minimal value of the style index.\n",
        "    s_style_max: The maximal value of the style index.\n",
        "    style_direction_index: If 0 move s to it's min value otherwise to it's max\n",
        "      value.\n",
        "    shift_size: Factor of the shift of the style vector.\n",
        "    label_size: The size of the label.\n",
        "  Returns:\n",
        "    The image after the style index modification, and the output of\n",
        "    the classifier on this image.\n",
        "  \"\"\"\n",
        "  # dlatent = torch.from_numpy(dlatent)\n",
        "  expanded_dlatent_tmp = torch.tile(dlatent.unsqueeze(1),[1, num_layers, 1])\n",
        "  network_inputs = generator.style_vector_calculator(expanded_dlatent_tmp)\n",
        "  # images_out = generator.synthesis.image_given_dlatent(expanded_dlatent_tmp, network_inputs[0])\n",
        "  # images_out = torch.maximum(torch.minimum(images_out, torch.Tensor([1])), torch.Tensor([-1]))\n",
        "\n",
        "  # network_inputs = generator.synthesis.style_vector_calculator(expanded_dlatent_tmp)\n",
        "  noise = model.get_noise(1)\n",
        "  s, s_rgb = generator.style_vector_calculator(expanded_dlatent_tmp.to(model.device))\n",
        "  style_vector = torch.cat(s, dim=1).numpy()\n",
        "  orig_value = style_vector[0, sindex]\n",
        "  target_value = (s_style_min if style_direction_index == 0 else s_style_max)\n",
        "\n",
        "  if target_value == orig_value:\n",
        "    weight_shift = shift_size\n",
        "  else:\n",
        "    weight_shift = shift_size * (target_value - orig_value)\n",
        "\n",
        "  layer_idx, in_idx = sindex_to_layer_idx_and_index(network_inputs[1], sindex)\n",
        "  \n",
        "  layer_one_hot = torch.nn.functional.one_hot(torch.Tensor([in_idx]).to(int), network_inputs[1][layer_idx].shape[1])\n",
        "  \n",
        "  network_inputs[1][layer_idx] += (weight_shift * layer_one_hot)\n",
        "  svbg_new = group_new_style_vec_block(network_inputs[1])\n",
        "  \n",
        "  images_out = generator.synthesis(expanded_dlatent_tmp, noise,svbg_new)\n",
        "  images_out = torch.maximum(torch.minimum(images_out, torch.Tensor([1])), torch.Tensor([-1])) \n",
        "  \n",
        "  change_image = torch.tensor(images_out.numpy())\n",
        "  result = classifier(change_image)\n",
        "  change_prob = nn.Softmax(dim=1)(result).detach().numpy()[0, class_index] \n",
        "  change_image = change_image.permute(0, 2, 3, 1)\n",
        "\n",
        "  return change_image, change_prob\n",
        "\n",
        "\n",
        "def get_discriminator_results_given_dlatent(\n",
        "    dlatent: np.ndarray,\n",
        "    generator: Generator,\n",
        "    discriminator: None, #set to None until we ask Tim\n",
        "    classifier: Classifier,\n",
        "    class_index: int,\n",
        "    sindex: int,\n",
        "    s_style_min: float,\n",
        "    s_style_max: float,\n",
        "    style_direction_index: int,\n",
        "    shift_size: float = 2,\n",
        "    label_size: int = 2,\n",
        ") -> Tuple[float, float]:\n",
        "  \"\"\"Modifies an image given the dlatent on a specific S-index.\n",
        "  Args:\n",
        "    dlatent: The image dlatent, with sape [dlatent_size].\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    class_index: The index of the class to visualize.\n",
        "    sindex: The specific style index to visualize.\n",
        "    s_style_min: The minimal value of the style index.\n",
        "    s_style_max: The maximal value of the style index.\n",
        "    style_direction_index: If 0 move s to it's min value otherwise to it's max\n",
        "      value.\n",
        "    shift_size: Factor of the shift of the style vector.\n",
        "    label_size: The size of the label.\n",
        "  Returns:\n",
        "    The discriminator before and after.\n",
        "  \"\"\"\n",
        "  network_inputs = generator.style_vector_calculator(dlatent)\n",
        "  images_out = generator.g_synthesis(network_inputs, training=False)\n",
        "  images_out = torch.maximum(torch.minimum(images_out, torch.Tensor([1])), torch.Tensor([-1]))\n",
        "  labels = tf.constant(dlatent[:, -label_size:], dtype=tf.float32)\n",
        "  discriminator_before = discriminator([images_out, labels], training=False)\n",
        "  # I am not using the classifier output here, because it is only one.\n",
        "  change_image, _ = (\n",
        "      generate_change_image_given_dlatent(dlatent, generator, classifier,\n",
        "                                          class_index, sindex,\n",
        "                                          s_style_min, s_style_max,\n",
        "                                          style_direction_index, shift_size,\n",
        "                                          label_size))\n",
        "  \n",
        "  results = classifier(change_image)\n",
        "  labels = nn.Softmax(dim=1)(results)\n",
        "  change_image_for_disc = tf.transpose(change_image, (0, 3, 1, 2))\n",
        "  discriminator_after = discriminator([change_image_for_disc, labels], \n",
        "                                      training=False)\n",
        "  return (discriminator_before, discriminator_after)\n",
        "\n",
        "\n",
        "def generate_images_given_dlatent(\n",
        "    dlatent: np.ndarray,\n",
        "    generator: Generator,\n",
        "    classifier: Optional[Classifier],\n",
        "    class_index: int,\n",
        "    sindex: int,\n",
        "    s_style_min: float,\n",
        "    s_style_max: float,\n",
        "    style_direction_index: int,\n",
        "    font_file: Optional[str],\n",
        "    shift_size: float = 2,\n",
        "    label_size: int = 2,\n",
        "    draw_results_on_image: bool = True,\n",
        "    resolution: int = 256,\n",
        "    num_layers: int = 14,\n",
        ") -> Tuple[np.ndarray, float, float, float, float]:\n",
        "  \"\"\"Modifies an image given the dlatent on a specific S-index.\n",
        "  Args:\n",
        "    dlatent: The image dlatent, with sape [dlatent_size].\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    classifier: The classifier to visualize.\n",
        "    class_index: The index of the class to visualize.\n",
        "    sindex: The specific style index to visualize.\n",
        "    s_style_min: The minimal value of the style index.\n",
        "    s_style_max: The maximal value of the style index.\n",
        "    style_direction_index: If 0 move s to it's min value otherwise to it's max\n",
        "      value.\n",
        "    font_file: A path to the font file for writing the probability on the image.\n",
        "    shift_size: Factor of the shift of the style vector.\n",
        "    label_size: The size of the label.\n",
        "    draw_results_on_image: Whether to draw the classifier outputs on the images.\n",
        "  Returns:\n",
        "    The image before and after the style index modification, and the outputs of\n",
        "    the classifier before and after the\n",
        "    modification.\n",
        "  \"\"\"\n",
        "  # dlatent = torch.from_numpy(dlatent)\n",
        "  expanded_dlatent_tmp = torch.tile(dlatent.unsqueeze(1),[1, num_layers, 1])\n",
        "  svbg, _, _ = generator.style_vector_calculator(expanded_dlatent_tmp)\n",
        "  result_image = np.zeros((resolution, 2 * resolution, 3), np.uint8)\n",
        "  images_out = generator.image_given_dlatent(expanded_dlatent_tmp, svbg)\n",
        "  images_out = torch.maximum(torch.minimum(images_out, torch.Tensor([1])), torch.Tensor([-1]))\n",
        "\n",
        "  # no need to permute for pytorch\n",
        "  # base_image = tf.transpose(images_out, [0, 2, 3, 1])\n",
        "\n",
        "  # base_image = torch.tensor(images_out.numpy())\n",
        "\n",
        "  # Removed flag training=False since we are using a pytorch model \n",
        "  # result = classifier(base_image, training=False)\n",
        "  result = classifier(images_out)\n",
        "  base_prob = nn.Softmax(1)(result)\n",
        "  base_prob = base_prob.detach().numpy()[0, class_index]\n",
        "\n",
        "  # permute so that draw_on_image() works\n",
        "  base_image = images_out.permute(0, 2, 3, 1)\n",
        "\n",
        "  if draw_results_on_image:\n",
        "    result_image[:, :resolution, :] = draw_on_image(\n",
        "        base_image[0].numpy(), base_prob, font_file)\n",
        "  else:\n",
        "    result_image[:, :resolution, :] = (base_image[0].numpy() * 127.5 +\n",
        "                                       127.5).astype(np.uint8)\n",
        "\n",
        "  change_image, change_prob = (\n",
        "      generate_change_image_given_dlatent(dlatent, generator, classifier,\n",
        "                                          class_index, sindex,\n",
        "                                          s_style_min, s_style_max,\n",
        "                                          style_direction_index, shift_size,\n",
        "                                          label_size))\n",
        "  if draw_results_on_image:\n",
        "    result_image[:, resolution:, :] = draw_on_image(\n",
        "        change_image[0].numpy(), change_prob, font_file)\n",
        "  else:\n",
        "    result_image[:, resolution:, :] = (\n",
        "        np.maxiumum(np.minimum(change_image[0].numpy(), 1), -1) * 127.5 +\n",
        "                                               127.5).astype(np.uint8)\n",
        "\n",
        "  return (result_image, change_prob, base_prob)\n",
        "\n",
        "\n",
        "\n",
        "def visualize_style(generator: Generator,\n",
        "                    classifier: Classifier,\n",
        "                    all_dlatents: np.ndarray,\n",
        "                    style_change_effect: np.ndarray,\n",
        "                    style_min: np.ndarray,\n",
        "                    style_max: np.ndarray,\n",
        "                    sindex: int,\n",
        "                    style_direction_index: int,\n",
        "                    max_images: int,\n",
        "                    shift_size: float,\n",
        "                    font_file: str,\n",
        "                    label_size: int = 2,\n",
        "                    class_index: int = 0,\n",
        "                    effect_threshold: float = 0.3,\n",
        "                    seed: Optional[int] = None,\n",
        "                    allow_both_directions_change: bool = False,\n",
        "                    draw_results_on_image: bool = True) -> np.ndarray:\n",
        "  \"\"\"Returns an image visualizing the effect of a specific S-index.\n",
        "  Args:\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    classifier: The classifier to visualize.\n",
        "    all_dlatents: An array with shape [num_images, dlatent_size].\n",
        "    style_change_effect: A shape of [num_images, 2, style_size, num_classes].\n",
        "      The effect of each change of style on specific direction on each image.\n",
        "    style_min: The minimal value of each style, with shape [style_size].\n",
        "    style_max: The maximal value of each style, with shape [style_size].\n",
        "    sindex: The specific style index to visualize.\n",
        "    style_direction_index: If 0 move s to its min value otherwise to its max\n",
        "      value.\n",
        "    max_images: Maximal number of images to visualize.\n",
        "    shift_size: Factor of the shift of the style vector.\n",
        "    font_file: A path to the font file for writing the probability on the image.\n",
        "    label_size: The size of the label.\n",
        "    class_index: The index of the class to visualize.\n",
        "    effect_threshold: Choose images whose effect was at least this number.\n",
        "    seed: If given, use this as a seed to the random shuffling of the images.\n",
        "    allow_both_directions_change: Whether to allow both increasing and\n",
        "      decreasing the classifiaction (used for age).\n",
        "    draw_results_on_image: Whether to draw the classifier outputs on the images.\n",
        "  \"\"\"\n",
        "\n",
        "  # Choose the dlatent indices to visualize\n",
        "  if allow_both_directions_change:\n",
        "    images_idx = (np.abs(style_change_effect[:, style_direction_index, sindex,\n",
        "                                             class_index]) >\n",
        "                  effect_threshold).nonzero()[0]\n",
        "  else:\n",
        "    images_idx = ((style_change_effect[:, style_direction_index, sindex,\n",
        "                                       class_index]) >\n",
        "                  effect_threshold).nonzero()[0]\n",
        "  if images_idx.size == 0:\n",
        "    return np.array([])\n",
        "\n",
        "  if seed is not None:\n",
        "    np.random.seed(seed)\n",
        "  np.random.shuffle(images_idx)\n",
        "  images_idx = images_idx[:min(max_images*10, len(images_idx))]\n",
        "  dlatents = all_dlatents[images_idx]\n",
        "\n",
        "  result_images = []\n",
        "  for i in range(len(images_idx)):\n",
        "    cur_dlatent = dlatents[i:i + 1]\n",
        "    (result_image, base_prob, change_prob) = generate_images_given_dlatent(\n",
        "         dlatent=cur_dlatent,\n",
        "         generator=generator,\n",
        "         classifier=classifier,\n",
        "         class_index=class_index,\n",
        "         sindex=sindex,\n",
        "         s_style_min=style_min[sindex],\n",
        "         s_style_max=style_max[sindex],\n",
        "         style_direction_index=style_direction_index,\n",
        "         font_file=font_file,\n",
        "         shift_size=shift_size,\n",
        "         label_size=label_size,\n",
        "         draw_results_on_image=draw_results_on_image)\n",
        "\n",
        "    if np.abs(change_prob - base_prob) < effect_threshold:\n",
        "      continue\n",
        "    result_images.append(result_image)\n",
        "    if len(result_images) == max_images:\n",
        "      break\n",
        "\n",
        "  if len(result_images) < 3:\n",
        "    # No point in returning results with very little images\n",
        "    return np.array([])\n",
        "  return np.concatenate(result_images[:max_images], axis=0)\n",
        "\n",
        "\n",
        "def visualize_style_by_distance_in_s(\n",
        "    generator: Generator,\n",
        "    classifier: Classifier,\n",
        "    all_dlatents: np.ndarray,\n",
        "    all_style_vectors_distances: np.ndarray,\n",
        "    style_min: np.ndarray,\n",
        "    style_max: np.ndarray,\n",
        "    sindex: int,\n",
        "    style_sign_index: int,\n",
        "    max_images: int,\n",
        "    shift_size: float,\n",
        "    font_file: str,\n",
        "    label_size: int = 2,\n",
        "    class_index: int = 0,\n",
        "    draw_results_on_image: bool = True,\n",
        "    effect_threshold: float = 0.1,\n",
        "    images_idx: list = [47, 50, 93, 98, 123, 165, 210, 214]) -> np.ndarray:\n",
        "  \"\"\"Returns an image visualizing the effect of a specific S-index.\n",
        "  Args:\n",
        "    generator: The generator model. Either StyleGAN or GLO.\n",
        "    classifier: The classifier to visualize.\n",
        "    all_dlatents: An array with shape [num_images, dlatent_size].\n",
        "    all_style_vectors_distances: A shape of [num_images, style_size, 2].\n",
        "      The distance each style from the min and max values on each image.\n",
        "    style_min: The minimal value of each style, with shape [style_size].\n",
        "    style_max: The maximal value of each style, with shape [style_size].\n",
        "    sindex: The specific style index to visualize.\n",
        "    style_sign_index: If 0 move s to its min value otherwise to its max\n",
        "      value.\n",
        "    max_images: Maximal number of images to visualize.\n",
        "    shift_size: Factor of the shift of the style vector.\n",
        "    font_file: A path to the font file for writing the probability on the image.\n",
        "    label_size: The size of the label.\n",
        "    class_index: The index of the class to visualize.\n",
        "    draw_results_on_image: Whether to draw the classifier outputs on the images.\n",
        "  \"\"\"\n",
        "\n",
        "  # Choose the dlatent indices to visualize\n",
        "  # images_idx = np.argsort(\n",
        "  #     all_style_vectors_distances[:, sindex, style_sign_index])[::-1]\n",
        "  # if images_idx.size == 0:\n",
        "  #   return np.array([])\n",
        "\n",
        "  # images_idx = images_idx[:min(max_images*10, len(images_idx))]\n",
        "  # dlatents = all_dlatents[images_idx]\n",
        "  dlatents = all_dlatents[images_idx]\n",
        "\n",
        "  result_images = []\n",
        "  for i in range(len(images_idx)):\n",
        "    cur_dlatent = dlatents[i:i + 1]\n",
        "    (result_image, change_prob, base_prob) = generate_images_given_dlatent(\n",
        "         dlatent=cur_dlatent,\n",
        "         generator=generator,\n",
        "         classifier=classifier,\n",
        "         class_index=class_index,\n",
        "         sindex=sindex,\n",
        "         s_style_min=style_min[sindex],\n",
        "         s_style_max=style_max[sindex],\n",
        "         style_direction_index=style_sign_index,\n",
        "         font_file=font_file,\n",
        "         shift_size=shift_size,\n",
        "         label_size=label_size,\n",
        "         draw_results_on_image=draw_results_on_image)\n",
        "    if (change_prob - base_prob) < effect_threshold:\n",
        "      continue\n",
        "    result_images.append(result_image)\n",
        "\n",
        "\n",
        "  # if len(result_images) < 3:\n",
        "  #   # No point in returning results with very little images\n",
        "  #   return np.array([])\n",
        "  return np.concatenate(result_images[:max_images], axis=0)\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "def load_torch_generator(pkl_file_path='./models/generator/generator_kwargs.pkl', pth_file='./models/generator/generator.pth'):\n",
        "    print('Loading generator\\'s necessary kwargs...')\n",
        "    with open(pkl_file_path, 'rb') as f:\n",
        "        kwargs = pickle.load(f)\n",
        "    print('Creating generator model...')\n",
        "    G = Generator(**kwargs).eval().requires_grad_(False)\n",
        "    print('Loading generator\\'s state dict...')\n",
        "    G.load_state_dict(torch.load(pth_file))\n",
        "    print('Done')\n",
        "    return G\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def group_new_style_vec_block(svb):\n",
        "\n",
        "    svbg_new = []\n",
        "    group_index = 0\n",
        "    temp_list = []\n",
        "    for i, stl_vec in enumerate(svb):\n",
        "        temp_list.append(stl_vec)\n",
        "        if i % 2 == 0:\n",
        "            svbg_new.append(temp_list)\n",
        "            temp_list = []\n",
        "            group_index += 1\n",
        "\n",
        "    return svbg_new\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def show_images(images, fmt='png'):\n",
        "  for i in range(images.shape[0]):\n",
        "    image = np.array(images[i])\n",
        "    if image.dtype == np.float32:\n",
        "        image = np.uint8(image * 127.5 + 127.5)\n",
        "    if image.shape[0] == 3:\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "    bytes_io = BytesIO()\n",
        "    Image.fromarray(image).save(bytes_io, fmt)\n",
        "    IPython.display.display(IPython.display.Image(data=bytes_io.getvalue()))\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "def create_images_from_dlatent(G,dlat_path='saved_dlantents.pkl',num_images=1, num_layers=14):\n",
        "    \n",
        "    with open(dlat_path, 'rb') as f:\n",
        "        dlatents_file = pickle.load(f)\n",
        "    dlatents = []\n",
        "    for dlat in dlatents_file:\n",
        "        dlatents.append(dlat[1])\n",
        "    dlatents = torch.Tensor(np.array(dlatents))\n",
        "    expanded_dlatent_tmp = torch.tile(dlatents,[1, num_layers, 1])\n",
        "    \n",
        "    if expanded_dlatent_tmp is not None:\n",
        "        style_vector_block_grouped, _, _ = G.synthesis.style_vector_calculator(expanded_dlatent_tmp[:num_images,:,:])\n",
        "        gen_output = G.image_given_dlatent(expanded_dlatent_tmp[:num_images,:,:] ,style_vector_block_grouped)\n",
        "        img_out = torch.maximum(torch.minimum(gen_output, torch.Tensor([1])), torch.Tensor([-1]))\n",
        "        show_images(img_out)\n",
        "    \n",
        "#----------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "O43ZesfJGUmE"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Utils\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    ## Dataset\n",
        "    This loads the training dataset and resize it to the give image size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str, image_size: int):\n",
        "        \"\"\"\n",
        "        * `path` path to the folder containing the images\n",
        "        * `image_size` size of the image\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Get the paths of all `jpg` files\n",
        "        self.paths = [p for p in Path(path).glob(f'**/*.jpg')]\n",
        "\n",
        "        # Transformation\n",
        "        self.transform = torchvision.transforms.Compose([\n",
        "            # Resize the image\n",
        "            torchvision.transforms.Resize(image_size),\n",
        "            # Convert to PyTorch tensor\n",
        "            torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of images\"\"\"\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get the the `index`-th image\"\"\"\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path)\n",
        "        return self.transform(img)\n",
        "def get_mnist_data(train_dir, image_size, digit=8):\n",
        "\n",
        "  data_transform = transforms.Compose([\n",
        "    #transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((image_size,image_size)),\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "  ])\n",
        "\n",
        "  train_files = os.listdir(train_dir)\n",
        "  dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                        train=True, \n",
        "                        download=True, \n",
        "                        transform=data_transform)\n",
        "  idx = dataset.targets==digit\n",
        "  dataset.targets = dataset.targets[idx]\n",
        "  dataset.data = dataset.data[idx]\n",
        "  return dataset\n",
        "def get_mnist_2_digit_data(train_dir, image_size, digit_0=9, digit_1=8):\n",
        "\n",
        "  data_transform = transforms.Compose([\n",
        "    #transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((image_size,image_size)),\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "  ])\n",
        "  train_files = os.listdir(train_dir)\n",
        "  dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                        train=True, \n",
        "                        download=True, \n",
        "                        transform=data_transform)\n",
        "  idx_0 = dataset.targets==digit_0\n",
        "  idx_1 = dataset.targets==digit_1\n",
        "  \n",
        "  data_0 = dataset.data[idx_0]\n",
        "  targets_0 = torch.zeros(len(data_0))\n",
        "  data_1 = dataset.data[idx_1]\n",
        "  targets_1 = torch.zeros(len(data_1))\n",
        "  dataset.data = torch.concat((data_0,data_1), dim = 0)\n",
        "  dataset.targets = torch.concat((targets_0,targets_1), dim = 0)\n",
        "  return dataset\n",
        "\n",
        "def get_catdog_data(train_dir, image_size):\n",
        "\n",
        "  class CatDogDataset(Dataset):\n",
        "    def __init__(self, file_list, dir, mode='train', transform = None):\n",
        "        self.file_list = file_list\n",
        "        self.dir = dir\n",
        "        self.mode= mode\n",
        "        self.transform = transform\n",
        "        if self.mode == 'train':\n",
        "            if 'dog' in self.file_list[0]:\n",
        "                self.label = 1\n",
        "            else:\n",
        "                self.label = 0\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(os.path.join(self.dir, self.file_list[idx]))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.mode == 'train':\n",
        "            img = img.numpy()\n",
        "            return img.astype('float32'), self.label\n",
        "        else:\n",
        "            img = img.numpy()\n",
        "            return img.astype('float32'), self.file_list[idx]\n",
        "        \n",
        "  data_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((image_size,image_size)),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "  train_files = os.listdir(train_dir)\n",
        "  cat_files = [tf for tf in train_files if 'cat' in tf]\n",
        "  dog_files = [tf for tf in train_files if 'dog' in tf]\n",
        "\n",
        "  cats = CatDogDataset(cat_files, train_dir, transform = data_transform)\n",
        "  dogs = CatDogDataset(dog_files, train_dir, transform = data_transform)\n",
        "\n",
        "  catdogs = ConcatDataset([cats, dogs])\n",
        "  return catdogs"
      ],
      "metadata": {
        "id": "YT9lLXp6zmqI",
        "cellView": "form"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "d7GuJmCKHSVb"
      },
      "outputs": [],
      "source": [
        "#@title StylEx\n",
        "\n",
        "\n",
        "class StylEx(nn.Module):\n",
        "  def __init__(self,\n",
        "               log_resolution = 5, # log image resolution\n",
        "               n_z = 32, # dimensionality of the latent encoding\n",
        "               n_c = 2, # dimensionality of conditional encoding (number of classes)\n",
        "               n_s = 32, # dimensionality of the latent attribute layer\n",
        "               color_channels = 3, # number of input channels\n",
        "               classifier_path = './classifier.pt',\n",
        "               device = None,\n",
        "               style_mixing_prob = .9\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.latent_dim = self.d_latent = n_z + n_c\n",
        "    self.conditional_dim = n_c\n",
        "\n",
        "    self.generator  = Generator(log_resolution, n_z+n_c, n_features=32)\n",
        "    self.encoder = Encoder(log_resolution, d_latent=n_z)\n",
        "    self.discriminator = Discriminator(log_resolution, n_features=32)\n",
        "    self.random_mapping = MappingNetwork(features=n_z+n_c, n_layers=8)\n",
        "\n",
        "    self.classifier = load_classifier(classifier_path)\n",
        "\n",
        "    self.n_gen_blocks = self.generator.n_blocks\n",
        "    self.device = device\n",
        "    self.style_mixing_prob = style_mixing_prob\n",
        "\n",
        "  # Generate noise for random generation\n",
        "  def __noise__(self, batch_size: int, zeros:bool = False):\n",
        "    noise = []\n",
        "    resolution = 4\n",
        "    if zeros:\n",
        "      for i in range(self.n_gen_blocks):\n",
        "        if i == 0:\n",
        "          n1 = None\n",
        "        else:\n",
        "          n1 = torch.zeros(batch_size, 1, resolution, resolution, device=self.device)\n",
        "        n2 = torch.zeros(batch_size, 1, resolution, resolution, device=self.device)\n",
        "        noise.append((n1, n2))\n",
        "        resolution *= 2\n",
        "    else:\n",
        "      for i in range(self.n_gen_blocks):\n",
        "        if i == 0:\n",
        "          n1 = None\n",
        "        else:\n",
        "          n1 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "        n2 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "        noise.append((n1, n2))\n",
        "        resolution *= 2\n",
        "    return noise\n",
        "\n",
        "  # Sample random styles for training\n",
        "  def __get_random__(self, batch_size: int):\n",
        "    # Mix styles\n",
        "    if torch.rand(()).item() < self.style_mixing_prob:\n",
        "      # Random cross-over point\n",
        "      cross_over_point = int(torch.rand(()).item() * self.n_gen_blocks)\n",
        "      # Sample $z_1$ and $z_2$\n",
        "      z2 = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
        "      z1 = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
        "      # Get $w_1$ and $w_2$\n",
        "      w1 = self.random_mapping(z1)\n",
        "      w2 = self.random_mapping(z2)\n",
        "      # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n",
        "      w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n",
        "      w2 = w2[None, :, :].expand(self.n_gen_blocks - cross_over_point, -1, -1)\n",
        "      return torch.cat((w1, w2), dim=0)\n",
        "\n",
        "    # Without mixing\n",
        "    else:\n",
        "      # Sample $z$\n",
        "      z = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
        "      # Use the mapping network trained for random sampling\n",
        "      w = self.random_mapping(z)\n",
        "      # Expand $w$ for the generator blocks\n",
        "      return w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n",
        "\n",
        "  # Find the latent encoding vector appended with the classification vector\n",
        "  def get_latent(self, x):\n",
        "    z = self.encode(x)\n",
        "    if self.conditional_dim > 0:\n",
        "      c = self.classifier(x)\n",
        "      w = torch.concat((z, c), dim=1)\n",
        "    else:\n",
        "      w = z\n",
        "    #print('encoding', torch.abs(w).sum().item())\n",
        "    return w\n",
        "\n",
        "  # Classify the input image\n",
        "  def classify(self, x):\n",
        "    return F.log_softmax(self.classifier(x), dim = 1)\n",
        "\n",
        "  # Return latent encoding of an image\n",
        "  def encode(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  # Transform latent, w, vector into the StyleSpace\n",
        "  def get_attribute(self, w):\n",
        "    print(error)\n",
        "    return self.generator.StyleBlock(w)\n",
        "  \n",
        "  # Returns the reconstructed image and latent vector\n",
        "  def reconstruct(self, x):\n",
        "    w = self.get_latent(x)\n",
        "\n",
        "    # Reconstructed\n",
        "    if self.training:\n",
        "      noise = self.__noise__(x.shape[0], zeros=False)\n",
        "    else:\n",
        "      noise = 0\n",
        "    \n",
        "    w = w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n",
        "    x_ = self.generator(w, noise)\n",
        "    \n",
        "    # Reconstructed latent space\n",
        "    # z_ = self.encode(x_)\n",
        "    # z_ = 0\n",
        "    return x_, w\n",
        "\n",
        "  def get_w(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        ### Sample $w$\n",
        "        This samples $z$ randomly and get $w$ from the mapping network.\n",
        "        We also apply style mixing sometimes where we generate two latent variables\n",
        "        $z_1$ and $z_2$ and get corresponding $w_1$ and $w_2$.\n",
        "        Then we randomly sample a cross-over point and apply $w_1$ to\n",
        "        the generator blocks before the cross-over point and\n",
        "        $w_2$ to the blocks after.\n",
        "        \"\"\"\n",
        "\n",
        "        # Mix styles\n",
        "        if torch.rand(()).item() < self.style_mixing_prob:\n",
        "            # Random cross-over point\n",
        "            cross_over_point = int(torch.rand(()).item() * self.n_gen_blocks)\n",
        "            # Sample $z_1$ and $z_2$\n",
        "            z2 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "            z1 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "            # Get $w_1$ and $w_2$\n",
        "            w1 = self.random_mapping(z1)\n",
        "            w2 = self.random_mapping(z2)\n",
        "            # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n",
        "            w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n",
        "            w2 = w2[None, :, :].expand(self.n_gen_blocks - cross_over_point, -1, -1)\n",
        "            return torch.cat((w1, w2), dim=0)\n",
        "        # Without mixing\n",
        "        else:\n",
        "            # Sample $z$ and $z$\n",
        "            z = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "            # Get $w$ and $w$\n",
        "            w = self.random_mapping(z)\n",
        "            # Expand $w$ for the generator blocks\n",
        "            return w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n",
        "\n",
        "  def get_noise(self, batch_size: int):\n",
        "      \"\"\"\n",
        "      ### Generate noise\n",
        "      This generates noise for each [generator block](index.html#generator_block)\n",
        "      \"\"\"\n",
        "      # List to store noise\n",
        "      noise = []\n",
        "      # Noise resolution starts from $4$\n",
        "      resolution = 4\n",
        "\n",
        "      # Generate noise for each generator block\n",
        "      for i in range(self.n_gen_blocks):\n",
        "          # The first block has only one $3 \\times 3$ convolution\n",
        "          if i == 0:\n",
        "              n1 = None\n",
        "          # Generate noise to add after the first convolution layer\n",
        "          else:\n",
        "              n1 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "          # Generate noise to add after the second convolution layer\n",
        "          n2 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "\n",
        "          # Add noise tensors to the list\n",
        "          noise.append((n1, n2))\n",
        "\n",
        "          # Next block has $2 \\times$ resolution\n",
        "          resolution *= 2\n",
        "\n",
        "      # Return noise tensors\n",
        "      return noise\n",
        "\n",
        "  def generate_images(self, batch_size: int):\n",
        "        # Get latent\n",
        "        w = self.get_w(batch_size)\n",
        "        # Get noise\n",
        "        noise = self.get_noise(batch_size)\n",
        "\n",
        "        images = self.generator(w, noise)\n",
        "\n",
        "        # Return images and w\n",
        "        return images, w\n",
        "        \n",
        "\n",
        "  def generate_style(self, s):\n",
        "    return generator(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9wCuQtrlkIQo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Config\n",
        "\"\"\"\n",
        "---\n",
        "title: StyleGAN 2 Model Training\n",
        "summary: >\n",
        " An annotated PyTorch implementation of StyleGAN2 model training code.\n",
        "---\n",
        "# [StyleGAN 2](index.html) Model Training\n",
        "This is the training code for [StyleGAN 2](index.html) model.\n",
        "![Generated Images](generated_64.png)\n",
        "---*These are $64 \\times 64$ images generated after training for about 80K steps.*---\n",
        "*Our implementation is a minimalistic StyleGAN 2 model training code.\n",
        "Only single GPU training is supported to keep the implementation simple.\n",
        "We managed to shrink it to keep it at less than 500 lines of code, including the training loop.*\n",
        "*Without DDP (distributed data parallel) and multi-gpu training it will not be possible to train the model\n",
        "for large resolutions (128+).\n",
        "If you want training code with fp16 and DDP take a look at\n",
        "[lucidrains/stylegan2-pytorch](https://github.com/lucidrains/stylegan2-pytorch).*\n",
        "We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n",
        "You can find the download instruction in this\n",
        "[discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n",
        "Save the images inside [`data/stylegan` folder](#dataset_path).\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Iterator, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import lpips\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from labml import tracker, lab, monit, experiment\n",
        "from labml.configs import BaseConfigs\n",
        "from labml_helpers.device import DeviceConfigs\n",
        "from labml_helpers.train_valid import ModeState, hook_model_outputs\n",
        "#from labml_nn.gan.stylegan import Discriminator, Generator, MappingNetwork, GradientPenalty, PathLengthPenalty\n",
        "from labml_nn.gan.wasserstein import DiscriminatorLoss, GeneratorLoss\n",
        "from labml_nn.utils import cycle_dataloader\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "class Configs(BaseConfigs):\n",
        "    \"\"\"\n",
        "    ## Configurations\n",
        "    \"\"\"\n",
        "\n",
        "    # Device to train the model on.\n",
        "    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n",
        "    #  picks up an available CUDA device or defaults to CPU.\n",
        "    device: torch.device = DeviceConfigs()\n",
        "\n",
        "    # [StyleGAN2 Discriminator](index.html#discriminator)\n",
        "    discriminator: Discriminator\n",
        "    # [StyleGAN2 Generator](index.html#generator)\n",
        "    generator: Generator\n",
        "    # [StylEx Encoder](index.html#encoder)\n",
        "    encoder: Encoder\n",
        "    # [StylEx Classifier](index.html#classifier)\n",
        "    classifier: Classifier\n",
        "\n",
        "    # [StylEx Model]\n",
        "    model: StylEx\n",
        "    \n",
        "    # [Mapping network](index.html#mapping_network)\n",
        "    mapping_network: MappingNetwork\n",
        "\n",
        "    # Discriminator and generator loss functions.\n",
        "    # We use [Wasserstein loss](../wasserstein/index.html)\n",
        "    discriminator_loss: DiscriminatorLoss\n",
        "    generator_loss: GeneratorLoss\n",
        "\n",
        "    # Optimizers\n",
        "    generator_optimizer: torch.optim.Adam\n",
        "    discriminator_optimizer: torch.optim.Adam\n",
        "    mapping_network_optimizer: torch.optim.Adam\n",
        "    encoder_optimizer: torch.optim.Adam\n",
        "\n",
        "    # [Gradient Penalty Regularization Loss](index.html#gradient_penalty)\n",
        "    gradient_penalty = GradientPenalty()\n",
        "    # Gradient penalty coefficient $\\gamma$\n",
        "    gradient_penalty_coefficient: float = 10.\n",
        "\n",
        "    # [Path length penalty](index.html#path_length_penalty)\n",
        "    path_length_penalty: PathLengthPenalty\n",
        "\n",
        "    # Data loader\n",
        "    loader: Iterator\n",
        "\n",
        "    # Batch size\n",
        "    batch_size: int = 32\n",
        "    # Dimensionality of $z$ and $w$\n",
        "    d_latent: int = 512\n",
        "    # Height/width of the image\n",
        "    image_size: int = 32\n",
        "    # Number of layers in the mapping network\n",
        "    mapping_network_layers: int = 8\n",
        "    # Generator & Discriminator learning rate\n",
        "    learning_rate: float = 1e-3\n",
        "    # Mapping network learning rate ($100 \\times$ lower than the others)\n",
        "    mapping_network_learning_rate: float = 1e-5\n",
        "    # Number of steps to accumulate gradients on. Use this to increase the effective batch size.\n",
        "    gradient_accumulate_steps: int = 1\n",
        "    # $\\beta_1$ and $\\beta_2$ for Adam optimizer\n",
        "    adam_betas: Tuple[float, float] = (0.0, 0.99)\n",
        "    # Probability of mixing styles\n",
        "    style_mixing_prob: float = 0.9\n",
        "    # Number of classes\n",
        "    num_classes: int = 2\n",
        "\n",
        "    # Total number of training steps\n",
        "    training_steps: int = 150_000\n",
        "\n",
        "    # Number of blocks in the generator (calculated based on image resolution)\n",
        "    n_gen_blocks: int\n",
        "\n",
        "    # ### Lazy regularization\n",
        "    # Instead of calculating the regularization losses, the paper proposes lazy regularization\n",
        "    # where the regularization terms are calculated once in a while.\n",
        "    # This improves the training efficiency a lot.\n",
        "\n",
        "    # The interval at which to compute gradient penalty\n",
        "    lazy_gradient_penalty_interval: int = 4\n",
        "    # Path length penalty calculation interval\n",
        "    lazy_path_penalty_interval: int = 32\n",
        "    # Skip calculating path length penalty during the initial phase of training\n",
        "    lazy_path_penalty_after: int = 0\n",
        "\n",
        "    # How often to log generated images\n",
        "    log_generated_interval: int = 500\n",
        "    # How often to save model checkpoints\n",
        "    save_checkpoint_interval: int = 2_000\n",
        "\n",
        "    # Training mode state for logging activations\n",
        "    mode: ModeState\n",
        "    # Whether to log model layer outputs\n",
        "    log_layer_outputs: bool = False\n",
        "    # For Classifier Loss\n",
        "    KL_loss: torch.nn = torch.nn.KLDivLoss(reduction='batchmean',log_target = True)\n",
        "    # For Reconstruction Loss\n",
        "    lpips_loss: torch.nn\n",
        "    L1_loss: torch.nn = torch.nn.L1Loss(reduction='mean')\n",
        "\n",
        "    # [Reconstruction, Adversarial ,Classifier, Path Loss] loss weight\n",
        "    loss_weights: list = [2, 1, 1, 1] \n",
        "\n",
        "    dataset_path: str = str('data/stylegan2/Dogs-Cats/train/')\n",
        "    classifier_path: str = str('./mnist_classifier.pt')\n",
        "    load_model: bool = False\n",
        "    model_path: str = './StylEx_Final.pt'\n",
        "\n",
        "    def init(self):\n",
        "        \"\"\"\n",
        "        ### Initialize\n",
        "        \"\"\"\n",
        "        # Create dataset\n",
        "        # dataset = get_catdog_data(self.dataset_path, self.image_size)\n",
        "        dataset = get_mnist_2_digit_data('./', self.image_size)\n",
        "        # dataset = get_mnist_data('./', self.image_size)\n",
        "        #dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "        #                                download=True, transform=transform)\n",
        "        \n",
        "\n",
        "        # Create data loader\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, num_workers=2,\n",
        "                                                 shuffle=True, drop_last=True, pin_memory=True)\n",
        "        # Continuous [cyclic loader](../../utils.html#cycle_dataloader)\n",
        "        self.loader = cycle_dataloader(dataloader)\n",
        "        \n",
        "        # $\\log_2$ of image resolution\n",
        "        log_resolution = int(math.log2(self.image_size))\n",
        "\n",
        "        # Create discriminator and generator\n",
        "        #self.discriminator = Discriminator(log_resolution).to(self.device)\n",
        "        #self.generator = Generator(log_resolution, self.d_latent).to(self.device)\n",
        "        #self.encoder = Encoder(log_resolution, self.d_latent).to(self.device)\n",
        "        \n",
        "        #self.classifier = Classifier(self.classifier_path).to(self.device)\n",
        "\n",
        "        self.model = StylEx(log_resolution,\n",
        "                            n_z = self.d_latent, \n",
        "                            n_s = self.d_latent,\n",
        "                            n_c = self.num_classes,\n",
        "                            classifier_path = self.classifier_path,\n",
        "                            device = self.device\n",
        "                            ).to(self.device)\n",
        "\n",
        "        if self.load_model:\n",
        "          self.model.load_state_dict(torch.load(self.model_path))\n",
        "        # Get number of generator blocks for creating style and noise inputs\n",
        "        self.n_gen_blocks = self.model.generator.n_blocks\n",
        "        \n",
        "        # Create mapping network\n",
        "        #self.mapping_network = MappingNetwork(self.d_latent, self.mapping_network_layers).to(self.device)\n",
        "        # Create path length penalty loss\n",
        "        self.path_length_penalty = PathLengthPenalty(0.99).to(self.device)\n",
        "\n",
        "        # Add model hooks to monitor layer outputs\n",
        "        if self.log_layer_outputs:\n",
        "            hook_model_outputs(self.mode, self.model.discriminator, 'discriminator')\n",
        "            hook_model_outputs(self.mode, self.model.generator, 'generator')\n",
        "            hook_model_outputs(self.mode, self.model.random_mapping, 'mapping_network')\n",
        "            hook_model_outputs(self.mode, self.model.encoder, 'encoder')\n",
        "\n",
        "        # Discriminator and generator losses\n",
        "        self.discriminator_loss = DiscriminatorLoss().to(self.device)\n",
        "        self.generator_loss = GeneratorLoss().to(self.device)\n",
        "\n",
        "        # Create optimizers\n",
        "        self.discriminator_optimizer = torch.optim.Adam(\n",
        "            self.model.discriminator.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.generator_optimizer = torch.optim.Adam(\n",
        "            self.model.generator.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.mapping_network_optimizer = torch.optim.Adam(\n",
        "            self.model.random_mapping.parameters(),\n",
        "            lr=self.mapping_network_learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.encoder_optimizer = torch.optim.Adam(\n",
        "            self.model.encoder.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "\n",
        "        # Set tracker configurations\n",
        "        tracker.set_image(\"generated\", True)\n",
        "        self.lpips_loss = lpips.LPIPS().to(self.device)\n",
        "\n",
        "    def reconstruct_loss(self, x, x_, w = None):\n",
        "\n",
        "        # Latent reconstruction loss\n",
        "        if w == None:\n",
        "          w = self.model.encode(x)\n",
        "        w_ = self.model.encode(x_)\n",
        "        L_w = self.L1_loss(w, w_)\n",
        "        #print('L_z', L_w.item())\n",
        "\n",
        "        # Feature reconstruction\n",
        "        L_x = self.L1_loss(x, x_)\n",
        "        #print('L_x', L_x.item())\n",
        "\n",
        "        # Feature LPIPS loss\n",
        "        L_LPIPS = self.lpips_loss(x, x_).mean()\n",
        "        #print('L_LPIPS', L_LPIPS.item())\n",
        "\n",
        "        L = .1*L_w + L_x + .1*L_LPIPS\n",
        "\n",
        "        return L\n",
        "\n",
        "    def classifier_loss(self, x, x_):\n",
        "        L = self.KLLoss(self.model.classify(x_), self.model.classify(x))\n",
        "        return L\n",
        "\n",
        "    def generate_images(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        ### Generate images\n",
        "        This generate images using the generator\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        return self.model.generate_images(batch_size)\n",
        "    \n",
        "    def step(self, idx: int):\n",
        "        \"\"\"\n",
        "        ### Training Step\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        # Set to 0 for StylEx training, 1 for StyleGAN Training, and idx%2\n",
        "        # for mixed training\n",
        "        rand_switch = 0\n",
        "\n",
        "        # Train the discriminator\n",
        "        with monit.section('Discriminator'):\n",
        "            # Reset gradients\n",
        "            self.discriminator_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Update `mode`. Set whether to log activation\n",
        "                with self.mode.update(is_log_activations=(idx + 1) % self.log_generated_interval == 0):\n",
        "                    \n",
        "\n",
        "                    # Discriminator classification for generated images\n",
        "\n",
        "                    # Get real images from the data loader\n",
        "                    r_img, lbl = next(self.loader)\n",
        "                    real_images = r_img.to(self.device)\n",
        "\n",
        "                    # Encode and Reconstruct image\n",
        "                    if rand_switch == 1:\n",
        "                      # Sample random images\n",
        "                      generated_images, _ = self.model.generate_images(self.batch_size)\n",
        "                      fake_output = self.model.discriminator(generated_images.detach())\n",
        "                    else:\n",
        "                      generated_images, _ = self.model.reconstruct(real_images)\n",
        "                      fake_output = self.model.discriminator(generated_images.detach())\n",
        "                    \n",
        "                    # We need to calculate gradients w.r.t. real images for gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        real_images.requires_grad_()\n",
        "\n",
        "                    # Discriminator classification for real images\n",
        "                    real_output = self.model.discriminator(real_images)\n",
        "\n",
        "                    # Get discriminator loss\n",
        "                    real_loss, fake_loss = self.discriminator_loss(real_output, fake_output)\n",
        "                    disc_loss = real_loss + fake_loss\n",
        "\n",
        "                    # Add gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        # Calculate and log gradient penalty\n",
        "                        gp = self.gradient_penalty(real_images, real_output)\n",
        "                        tracker.add('loss.gp', gp)\n",
        "                        # Multiply by coefficient and add gradient penalty\n",
        "                        disc_loss = disc_loss + 0.5 * self.gradient_penalty_coefficient * gp * self.lazy_gradient_penalty_interval\n",
        "\n",
        "                    # Compute gradients\n",
        "                    disc_loss.backward()\n",
        "\n",
        "                    # Log discriminator loss\n",
        "                    tracker.add('loss.discriminator_real', real_loss)\n",
        "                    tracker.add('loss.discriminator_fake', fake_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('discriminator', self.model.discriminator)\n",
        "            \n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.discriminator.parameters(), max_norm=1.0)\n",
        "            # Take optimizer step\n",
        "            self.discriminator_optimizer.step()\n",
        "\n",
        "        # Train the generator\n",
        "        with monit.section('Generator'):\n",
        "            # Reset gradients\n",
        "            self.generator_optimizer.zero_grad()\n",
        "            self.mapping_network_optimizer.zero_grad()\n",
        "            self.encoder_optimizer.zero_grad()\n",
        "            # self.affine_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Get generator loss\n",
        "                if rand_switch == 0:\n",
        "                  # Reconstructed images\n",
        "                  generated_images, _ = self.model.generate_images(self.batch_size)\n",
        "                  generated_images = generated_images.detach()\n",
        "\n",
        "                  recon_images, w = self.model.reconstruct(real_images)\n",
        "                  gen_loss_rec = self.reconstruct_loss(real_images, \n",
        "                                                     recon_images)\n",
        "                  recon_output = self.model.discriminator(recon_images)\n",
        "                  gen_loss_adv = self.generator_loss(recon_output)\n",
        "                  gen_loss_class = self.KL_loss(\n",
        "                    self.model.classify(recon_images),\n",
        "                    self.model.classify(real_images)\n",
        "                    )\n",
        "                  # Combine losses using config weight\n",
        "                  gen_loss =  self.loss_weights[0]*gen_loss_rec + \\\n",
        "                              self.loss_weights[1]*gen_loss_adv + \\\n",
        "                              self.loss_weights[2]*gen_loss_class\n",
        "                #Standard StyleGAN training\n",
        "                else:\n",
        "                  # Sample images from generator\n",
        "                  generated_images, w = self.model.generate_images(self.batch_size)\n",
        "                  # Discriminator classification for generated images\n",
        "                  fake_output = self.model.discriminator(generated_images)\n",
        "                  gen_loss = self.generator_loss(fake_output)\n",
        "                                \n",
        "                # Add path length penalty\n",
        "                if idx > self.lazy_path_penalty_after and \\\n",
        "                  (idx + 1) % self.lazy_path_penalty_interval == 0:\n",
        "                    # Calculate path length penalty\n",
        "                    plp = self.path_length_penalty(w, recon_images)\n",
        "                    # Ignore if `nan`\n",
        "                    if not torch.isnan(plp):\n",
        "                        tracker.add('loss.plp', plp)\n",
        "                        gen_loss = gen_loss + self.loss_weights[3] * plp\n",
        "                gen_loss = gen_loss.mean()\n",
        "                # Calculate gradients\n",
        "                gen_loss.backward()\n",
        "\n",
        "                # Log generator loss\n",
        "                if(rand_switch==0):\n",
        "                  tracker.add('loss.classifier', gen_loss_class)\n",
        "                  tracker.add('loss.reconstruction', gen_loss_rec)\n",
        "                  tracker.add('loss.adversarial', gen_loss_adv)\n",
        "                else:\n",
        "                  tracker.add('loss.generator', gen_loss)\n",
        "          \n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('generator', self.model.generator)\n",
        "                tracker.add('mapping_network', self.model.random_mapping)\n",
        "\n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.generator.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.random_mapping.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Take optimizer step\n",
        "            self.generator_optimizer.step()\n",
        "            self.mapping_network_optimizer.step()\n",
        "            self.encoder_optimizer.step()\n",
        "            #print(generated_images.shape)\n",
        "            #print(recon_images.shape)\n",
        "\n",
        "        # Log generated images\n",
        "        if (idx + 1) % self.log_generated_interval == 0:\n",
        "            tracker.add('generated', torch.cat([generated_images[:3],recon_images[:3], real_images[:3]], dim=0))\n",
        "        # Save model checkpoints\n",
        "        if (idx + 1) % self.save_checkpoint_interval == 0:\n",
        "            experiment.save_checkpoint()\n",
        "\n",
        "        # Flush tracker\n",
        "        tracker.save()\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        ## Train model\n",
        "        \"\"\"\n",
        "\n",
        "        # Loop for `training_steps`\n",
        "        for i in monit.loop(self.training_steps):\n",
        "            # Take a training step\n",
        "            self.step(i)\n",
        "            #\n",
        "            if (i + 1) % self.log_generated_interval == 0:\n",
        "                tracker.new_line()\n",
        "            if i % 1000 == 0:\n",
        "              torch.save(self.model.state_dict(), './StylEx.pt')\n",
        "def load_classifier(path):\n",
        "  device = 'cuda'\n",
        "  model = torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "  num_ftrs = model.classifier.in_features\n",
        "  model.classifier = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 500),\n",
        "    nn.Linear(500, 2)\n",
        "    )\n",
        "  model = model.to(device)\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Main\n",
        "def main(load_model=False):\n",
        "     # Create an experiment\n",
        "    experiment.create(name='stylegan2')\n",
        "    # Create configurations object\n",
        "    configs = Configs()\n",
        "\n",
        "    # Set configurations and override some\n",
        "    experiment.configs(configs, {\n",
        "        'device.cuda_device': 0,\n",
        "        'image_size': 32,\n",
        "        'log_generated_interval': 200,\n",
        "        'load_model' : load_model  \n",
        "        })\n",
        "\n",
        "    # Initialize\n",
        "    configs.init()\n",
        "    # Set models for saving and loading\n",
        "    experiment.add_pytorch_models(mapping_network=configs.model.random_mapping,\n",
        "                                  generator=configs.model.generator,\n",
        "                                  discriminator=configs.model.discriminator)\n",
        "    return configs"
      ],
      "metadata": {
        "id": "tGPAkdmsz2XF",
        "cellView": "form"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "RkEOBbv2yFcP",
        "outputId": "d27eb358-7e3c-4d9c-bf6e-20ea66f8186a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device...\n",
              "  Prepare device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t13.59ms</span>\n",
              "Prepare device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t20.04ms</span>\n",
              "</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "config = main(load_model=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Trained_StylEx = config.model\n",
        "dataloader = config.loader\n",
        "Trained_StylEx.eval()\n",
        "\n",
        "generator = Trained_StylEx.generator\n",
        "encoder = Trained_StylEx.encoder\n",
        "classifier = Trained_StylEx.classifier\n",
        "generator.eval()\n",
        "encoder.eval()\n",
        "classifier.eval()"
      ],
      "metadata": {
        "id": "hts90hPW9bDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183a0d10-3d1c-44a4-ec0c-d0e2993df246"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (denseblock1): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition1): _Transition(\n",
              "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock2): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition2): _Transition(\n",
              "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock3): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer13): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer14): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer15): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer16): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer17): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer18): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer19): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer20): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer21): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer22): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer23): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer24): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition3): _Transition(\n",
              "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock4): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer13): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer14): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer15): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer16): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=500, bias=True)\n",
              "    (1): Linear(in_features=500, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "13yckkji59Qm"
      },
      "outputs": [],
      "source": [
        "#@title Global variables\n",
        "\n",
        "num_layers = Trained_StylEx.n_gen_blocks #????\n",
        "label_size = 2\n",
        "resolution = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVx6kE366YaO"
      },
      "source": [
        "## Create image given a dlatent\n",
        "\n",
        "We choose the index of one of the 250 preloaded dlatents from which we generate an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kl5CFx2h6YaP",
        "outputId": "8a63bd6e-12d7-46b9-9c1a-65462a06f171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAADoElEQVR4nO2W3U7qShTHp0M/MLbQCrQijYKiMUZREh/AN/N9fArvJCQmShShUZQQOpYG+oEFSjudfdHEYzx+bve+Oud/19Xp+s30v9bMUMfHx+BvCv7V7P8D/hsA+tUzRVEQQghhIpGII1EUEUIIIVEURVH0IwCEkOf5QqGwublZLpcZhplMJqZpOo7jOM7Dw4Ou6z8CSJK0u7t7cHCQTCY5jqMoimVZnucxxhjjSqXy+PiIEEIIeZ7neZ7v+4SQrwIYhikUCjs7O6IoPgcpiqJpmmEYiqIWFxdTqZSqqtPpNAgC13UHgwFCyDCMyWTyOSCKouFw2Ol0OI5Lp9Nx0DRNhJDv+yzLMgyTyWRkWc5kMgAA3/dVVR0Oh91ut1arvWfPPwCMcZxrPB5ns9lSqcTz/GAwuLq6siyLYRiapnO5XKFQkCSJ53me50VRFEUxlUoZhtHtdsMw/AjwvIh6vc6y7OHh4d7eXuyBZVmO48znc4SQpmmKomSz2Vwup6qqLMuiKO7v7+u6jjH+tyWvy5QQEoYhxrhWq+Xz+VKplE6nFUVBCFmW9fT0BACwLMuyrH6/H4ahJEmJRILn+c89eCmKopaXl9PpNE3TiqIoigIAmM/nvV7v5TCe5yGEYRiORqMwDN+sqLcBhJDRaGTb9tLS0nPHsSxbLBZfzYMQ4rru3d3d5ya/Akwmk/Pzcwjh6urqwsICACCKoul02uv1eJ4XBIGmacdxOp2OpmmmaX4PEKfr9/tBEBBCtra2IIRxTyCEdF0nhFAUFQTBeDx2XRdj/F6edwEAgNlsput6vV63bbtYLObzeZZlNzY2MMbNZtO27Q++/RIAABCG4f39vW3brusmEolcLre2tiYIQhAE7XZ7PB5/ulV8abu2bbvZbF5cXFiWRQiRJKlSqZTL5Q+q86sriEUIcRzn8vISQlitVtPptKqqcfzm5mY2m/0UEMvzvLOzM9u2j46OJElSVTWKIt/3W63WB+fE9040jHG73T49PTVNEwCQz+er1Wq8mj8DAACEYXh7e9toNAzDgBDKsry9vZ1MJimKenP8N35RLEKI7/vX19ccx3EcJwjC+vo6QqjZbL7ZDb9z6Meet1otTdOm06kkSdVqVZZlmn5jur9/qzBNs91ud7vdIAhWVlZUVWUY5k8CwjA0DKPRaGiahjEuFouCIDzvjM/6tgcv5XmepmmDwSCZTKZSKUEQbNt+5cSPALFs2z45OXnv7S80vQDWoTZQoQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstruction\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAEr0lEQVR4nJ2W608bRxDAZ/b8urPBNraxQwx5YBvSkDYKJORTFVWqVPWvav+jVuqnNkqrpv0ADQkPu0YtOEAwNuDX2cZ3e3e70w9uYmNMDNkvqzvNzm8euzOD33/3nSSCay0iAOx94uWSAC5B9FEBACAYYgCN1t0FjBQZcG9QftR510gTED/o6EeNtv2qgE9Qej3AB7M/Rf2VPaAPme6L2DUBCACIgAyIAAgZcymMMSaEcIQgOgcZTP1VAIDg8nrjk7FUai41mx4fH7csXm9UT0/KpaPiwUGx1e4QiffCSFJeEwAYi0YfP1pMZ+YZKIqiaF5fKBi8kbg5c2v23kKbc25z0+ImN4xqtXJ8clpv1IUQgAiXZ6gHICJp244gt8utKApTACQAgqb6vO5YbCLSVWILYZrGWatVq1UOi+/eHeyfVmpSSrokOX0AAL2pv327l0qlxsfGSFD5uNxqNREVZIwhU30+VdVUza8FQ6HxsVgsmkgkksnpg/39/N8507JHhggMw6xUyv/u7Hy+8JnH6wWA8vHJYfGw3e543O6APxCNTETCE6qmqarm96uhYDgYDEUiUcaUrY11S4iPARCAAKrVanbjTTQSnroxlYjHG416qVxq6vWOwV0KK5fGPG43AgTGAhORyPzcfHJ6ZurGFDd5dmsLPg7oMmzHOT45/vmX599+83U0OplJZxRFCQdDzaZuGAbnlsU557zZbtXq9VA4PD0zzZji1/yS8L2RlwMIAAkMzs2j4vMXv3317Fk8kchkMpl0GpAJ2zk+OS4Wj9rttpTS6/Mmk0khCYCq1aojrpCDvqpGe3t7K3+tLj9evjmdZIxJKdGlTCbiscmYoiACIjJA5ghZLpder78WUrJh12hoqUAgAqR8fltVVY/PF49PdhlSSl3XObfcblenY1RrtcPDg739g3qtPlT7JQCibjEgSTs7/4SCYbfbMzERAgDDMHZ2d3Z3CwwV27YN4+ykVpe2dSH0vZZ3SbEjAgAiqevtXG5DSnHv/oNIJKz5A5GJSDabazR00+gQEBEBEZzroedgI6opSVkqn9r2usWtx8tPAwH/rdt3vpRicyu7WyhYFiciABj2iP9njgIAIFGlWstmN91u9+LSkqppd2fTLpfi8Xq289uGaVw4dO6yjgB0LSOCut5cW1t1HGd5edmn+m7fTTlCCCFz2S0h5bCp4GqAnklEeuvs1atVAnr65Inq12ZTaSHl2Vm7UCjQYHvohew8gIaUXQJCwG6P6Zh8ZXXFpShLi4uqX82kM47jdEyjdHh02SBzHjDsLmNvJyC0LOvln38QiaWlJ4GAf35u3rbsF3rjrHU21Hs29O+FRYCA2IWh4zgrr9Zev1lrNpsuRZlLZx49fMQQLxhIFzwYvrDPfUJAADQNY+3NOiLev78QDoUffvHQNIyNzU2rvysQAI72gHr7+1QgIgA2dX0rl8tvb3NuhULhhQcP7t65zdigwpGAPq+7QwUiESEBAFZOK/n89m5h1+Q8OB6anp4JjgV6rw6v8NAGMNi7aISEgFgsFn/9/WWHW5qmdbileDwDPeGqoyN2hybAvlmfSBIAnJZLP/34AyJ22/5A8/8PGwdfz/9ZYFAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import IPython.display\n",
        "import pickle\n",
        "import requests\n",
        "import collections\n",
        "\n",
        "from tfrecord.torch.dataset import TFRecordDataset\n",
        "\n",
        "img_index = 10\n",
        "\n",
        "# Choose dlatent and make it into [1,num_layers,514]\n",
        "imgs, lbl = next(dataloader)\n",
        "imgs = imgs.to(config.device)\n",
        "\n",
        "dlatents = Trained_StylEx.get_latent(imgs).to(config.device)\n",
        "expanded_dlatent_tmp = dlatents[None, :, :].expand(Trained_StylEx.n_gen_blocks, -1, -1)\n",
        "\n",
        "# Feed dlatent into the generator\n",
        "gen_output = generator(expanded_dlatent_tmp.to(config.device), Trained_StylEx.get_noise(1)).detach()\n",
        "\n",
        "img_out = torch.maximum(torch.minimum(gen_output, torch.Tensor([1]).to(config.device)), torch.Tensor([-1]).to(config.device)).detach()\n",
        "print('Real')\n",
        "show_images(imgs[img_index].unsqueeze(0).detach().to('cpu'))\n",
        "print('Reconstruction')\n",
        "show_images(img_out[img_index].unsqueeze(0).detach().to('cpu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRg8BBoD6YaQ"
      },
      "source": [
        "### Generate Style space per index (min max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QX3nY90t6YaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793a1989-b6c1-438b-db5e-7cdc6392dbd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n",
            "928\n"
          ]
        }
      ],
      "source": [
        "values_per_index = collections.defaultdict(list)\n",
        "for dlatent in dlatents:\n",
        "    # Get the style vector:\n",
        "    expanded_dlatent_tmp = dlatent.unsqueeze(0)[None, :, :].expand(Trained_StylEx.n_gen_blocks, -1, -1)\n",
        "    \n",
        "    s_img = torch.cat(generator.style_vector_calculator(\n",
        "        expanded_dlatent_tmp)[1], dim=1).detach().to('cpu').numpy()[0]\n",
        "    for i, s_val in enumerate(s_img):\n",
        "        values_per_index[i].append(s_val)\n",
        "\n",
        "values_per_index = dict(values_per_index)\n",
        "s_indices_num = len(values_per_index.keys())\n",
        "minimums = [min(values_per_index[i]) for i in range(s_indices_num)] \n",
        "maximums = [max(values_per_index[i]) for i in range(s_indices_num)] \n",
        "print(len(minimums))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir ../../../../tmp/mnist\n",
        "%mkdir ../../../../tmp/mnist/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXIyd6S5WEBQ",
        "outputId": "3b31baad-d05b-4c46-a207-257828b319dc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘../../../../tmp/mnist’: File exists\n",
            "mkdir: cannot create directory ‘../../../../tmp/mnist/data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SSpace calculation (this is also very heavy, skip to next cell to load precomputed ones) {form-width: '20%'}\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "def get_classifier_results(model: StylEx, latent_vec):\n",
        "  noise = model.get_noise(latent_vec.shape[1])\n",
        "  return model.classifier(model.generator(latent_vec, noise))\n",
        "\n",
        "s_shift_size = .01 # @param\n",
        "data_path = '../../../../tmp/mnist/data/examples_1.tfrecord' #@param {type: 'string'}\n",
        "b_prob = []\n",
        "c_res = []\n",
        "lat = []\n",
        "s_vec = []\n",
        "\n",
        "with tf.io.TFRecordWriter(data_path) as writer:  \n",
        "  dl = dlatents[0:5]\n",
        "  for dlatent_index, dlatent in enumerate(dl): \n",
        "    expanded_dlatent = dlatent.unsqueeze(0)[None, :, :].expand(\n",
        "        Trained_StylEx.n_gen_blocks, -1, -1)\n",
        "    \n",
        "    base_prob = get_classifier_results(Trained_StylEx, expanded_dlatent).detach().to('cpu')\n",
        "    s_tmp = generator.style_vector_calculator(\n",
        "        expanded_dlatent)\n",
        "    s_rgbs = s_tmp[0]\n",
        "    style_vectors = s_tmp[1]\n",
        "    tot_idx = 0\n",
        "    n_s = len(minimums)\n",
        "\n",
        "    # n_styles x n_classes x n_dir \n",
        "    classifier_results = torch.zeros((n_s, 2, 2))\n",
        "    s_v = torch.zeros(n_s)\n",
        "    for i in range(len(style_vectors)):\n",
        "      style_vector = style_vectors[i].detach()\n",
        "      len_s = style_vector.shape[1]\n",
        "      for sindex in tqdm.tqdm(range(0, len_s)):\n",
        "        s_shift_down = (minimums[tot_idx] - style_vector[0,sindex]) * s_shift_size\n",
        "        s_shift_up = (maximums[tot_idx] - style_vector[0,sindex]) * s_shift_size\n",
        "        noise = Trained_StylEx.get_noise(1)\n",
        "\n",
        "        s_v[tot_idx] = style_vector[0, sindex].item()\n",
        "\n",
        "        # Shift up\n",
        "        s = style_vector[:, sindex] + s_shift_up\n",
        "        style_vectors[i] = s\n",
        "        c_up = classifier(generator.synthesis(expanded_dlatent, style_vectors, noise, s_rgb = s_rgbs)).detach().to('cpu')\n",
        "\n",
        "        # Shift down\n",
        "        s = style_vector[:, sindex] - s_shift_up - s_shift_down\n",
        "        style_vectors[i] = s\n",
        "        c_down = classifier(generator.synthesis(expanded_dlatent, style_vectors, noise, s_rgb = s_rgbs)).detach().to('cpu')\n",
        "        \n",
        "        # Return to base\n",
        "        s = style_vector[:, sindex] + s_shift_down\n",
        "        style_vectors[i] = s\n",
        "\n",
        "        classifier_results[tot_idx,:, 0] = c_down\n",
        "        classifier_results[tot_idx,:, 1] = c_up\n",
        "\n",
        "        tot_idx +=1\n",
        "\n",
        "        # Get the style vector.\n",
        "    s_vec.append(s_v.unsqueeze(0))\n",
        "    feature = {}\n",
        "    b_prob.append(base_prob.unsqueeze(0).to('cpu'))\n",
        "    lat.append(dlatent.unsqueeze(0).to('cpu'))\n",
        "    c_res.append(classifier_results.detach().unsqueeze(0).to('cpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMOblvBMVVJl",
        "outputId": "51828716-1cb9-476f-d5ce-e2fc87994bb5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:21<00:00, 12.01it/s]\n",
            "100%|██████████| 256/256 [00:14<00:00, 18.05it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 17.98it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.14it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 18.11it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 18.05it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:14<00:00, 18.05it/s]\n",
            "100%|██████████| 256/256 [00:15<00:00, 16.64it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.12it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.07it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 18.03it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 17.88it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:14<00:00, 18.02it/s]\n",
            "100%|██████████| 256/256 [00:14<00:00, 18.01it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.20it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.09it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 18.23it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 17.93it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:14<00:00, 18.16it/s]\n",
            "100%|██████████| 256/256 [00:14<00:00, 18.13it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.00it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.10it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 18.21it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 18.31it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n",
            "torch.Size([1, 128])\n",
            "\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 64])\n",
            "\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1, 32])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:14<00:00, 18.20it/s]\n",
            "100%|██████████| 256/256 [00:13<00:00, 18.31it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 18.19it/s]\n",
            "100%|██████████| 128/128 [00:07<00:00, 17.81it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 17.81it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 17.96it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0exK7-3p6YaS",
        "outputId": "d52eae49-143f-4b83-bb7f-4a4a1185ce42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "tensor(3.9458)\n",
            "tensor([[[[  7.2561,  -7.0590],\n",
            "          [  6.6656,  -6.9599],\n",
            "          [  6.7563,  -6.9858],\n",
            "          ...,\n",
            "          [  9.7235, -10.5145],\n",
            "          [  9.6018, -10.4121],\n",
            "          [  6.6499,  -6.2509]],\n",
            "\n",
            "         [[  7.2561,  -7.0590],\n",
            "          [  6.6656,  -6.9599],\n",
            "          [  6.7563,  -6.9858],\n",
            "          ...,\n",
            "          [  9.7235, -10.5145],\n",
            "          [  9.6018, -10.4121],\n",
            "          [  6.6499,  -6.2509]]],\n",
            "\n",
            "\n",
            "        [[[  5.2201,  -5.1566],\n",
            "          [ 12.0826, -10.6534],\n",
            "          [  5.2154,  -5.1593],\n",
            "          ...,\n",
            "          [  8.5804,  -9.4149],\n",
            "          [  8.6849,  -9.5742],\n",
            "          [  9.1543,  -7.9575]],\n",
            "\n",
            "         [[  5.2201,  -5.1566],\n",
            "          [ 12.0827, -10.6534],\n",
            "          [  5.2154,  -5.1593],\n",
            "          ...,\n",
            "          [  8.5804,  -9.4149],\n",
            "          [  8.6849,  -9.5742],\n",
            "          [  9.1543,  -7.9575]]],\n",
            "\n",
            "\n",
            "        [[[  5.6519,  -5.3301],\n",
            "          [  5.5563,  -5.2342],\n",
            "          [  5.4572,  -5.1607],\n",
            "          ...,\n",
            "          [  7.0521,  -7.6658],\n",
            "          [  7.1557,  -7.8165],\n",
            "          [  7.3831,  -7.0991]],\n",
            "\n",
            "         [[  5.6519,  -5.3301],\n",
            "          [  5.5563,  -5.2342],\n",
            "          [  5.4572,  -5.1607],\n",
            "          ...,\n",
            "          [  7.0521,  -7.6658],\n",
            "          [  7.1557,  -7.8165],\n",
            "          [  7.3831,  -7.0991]]],\n",
            "\n",
            "\n",
            "        [[[  9.5468,  -8.7658],\n",
            "          [  9.4212,  -8.7058],\n",
            "          [  9.6035,  -8.8698],\n",
            "          ...,\n",
            "          [  9.4123, -10.0878],\n",
            "          [  9.4988, -10.2194],\n",
            "          [  7.1618,  -7.2306]],\n",
            "\n",
            "         [[  9.5468,  -8.7658],\n",
            "          [  9.4212,  -8.7059],\n",
            "          [  9.6035,  -8.8698],\n",
            "          ...,\n",
            "          [  9.4123, -10.0878],\n",
            "          [  9.4988, -10.2194],\n",
            "          [  7.1618,  -7.2306]]],\n",
            "\n",
            "\n",
            "        [[[  4.9124,  -4.6272],\n",
            "          [  6.7618,  -7.1493],\n",
            "          [  5.0504,  -4.7539],\n",
            "          ...,\n",
            "          [  8.8375,  -9.7143],\n",
            "          [  8.9935,  -9.8981],\n",
            "          [  7.1587,  -6.2232]],\n",
            "\n",
            "         [[  4.9124,  -4.6272],\n",
            "          [  6.7618,  -7.1493],\n",
            "          [  5.0504,  -4.7540],\n",
            "          ...,\n",
            "          [  8.8375,  -9.7143],\n",
            "          [  8.9935,  -9.8981],\n",
            "          [  7.1587,  -6.2232]]]])\n",
            "tensor(19.5743)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-2d84651d90af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mall_style_vectors_distances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_style_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mall_style_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mall_style_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Load effect data from the tfrecord {form-width: '20%'}\n",
        "\n",
        "style_change_effect = []\n",
        "dlatents = []\n",
        "base_probs = []\n",
        "\n",
        "base_probs = torch.cat(b_prob)\n",
        "style_change_effect = torch.cat(c_res)\n",
        "dlatents = torch.cat(lat)\n",
        "\n",
        "expanded_dlatent_tmp = torch.tile(dlatents, [1, num_layers, 1])\n",
        "W_values, style_change_effect, base_probs = dlatents.squeeze(), style_change_effect.squeeze(), base_probs.squeeze()\n",
        "print(len(s_vec))\n",
        "all_style_vectors = torch.cat(s_vec, dim = 0)\n",
        "if len(style_change_effect.shape) == 3:\n",
        "  style_change_effect.unsqueeze_(0)\n",
        "  all_style_vectors.unsqueeze_(0)\n",
        "  base_probs.unsqueeze_(0)\n",
        "  W_values.unsqueeze_(0)\n",
        "style_change_effect = style_change_effect.permute((0, 3, 1, 2))\n",
        "print(torch.min(torch.abs(style_change_effect)))\n",
        "style_change_effect = filter_unstable_images(style_change_effect, effect_threshold=10)\n",
        "print(style_change_effect)\n",
        "print(torch.max(torch.abs(style_change_effect)))\n",
        "\n",
        "style_min = torch.Tensor(minimums)\n",
        "style_max = torch.Tensor(maximums)\n",
        "\n",
        "all_style_vectors_distances = np.zeros((all_style_vectors.shape[0], all_style_vectors.shape[1], 2))\n",
        "all_style_vectors_distances[:,:, 0] = all_style_vectors - torch.tile(style_min, (all_style_vectors.shape[0], 1))\n",
        "all_style_vectors_distances[:,:, 1] = torch.tile(style_max, (all_style_vectors.shape[0], 1)) - all_style_vectors\n",
        "all_style_vectors.shape\n",
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_values.shape"
      ],
      "metadata": {
        "id": "2qzoZ66NJn6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwQuMQ-6YaU"
      },
      "source": [
        "## Run the extraction step of AttFind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OPPLJR6u6YaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b03f6f-8017-41e0-d3cb-b593971e9efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0, 5 images.\n",
            "Class 1, 0 images.\n"
          ]
        }
      ],
      "source": [
        "#@title Split by class\n",
        "all_labels = np.argmax(base_probs, axis=1)\n",
        "style_effect_classes = {}\n",
        "W_classes = {}\n",
        "style_vectors_distances_classes = {}\n",
        "all_style_vectors_classes = {}\n",
        "for img_ind in range(label_size):\n",
        "  img_inx = np.array([i for i in range(all_labels.shape[0]) \n",
        "  if all_labels[i] == img_ind])\n",
        "  curr_style_effect = np.zeros((len(img_inx), style_change_effect.shape[1], \n",
        "                                style_change_effect.shape[2], style_change_effect.shape[3]))\n",
        "  curr_w = np.zeros((len(img_inx), W_values.shape[1]))\n",
        "  curr_style_vector_distances = np.zeros((len(img_inx), style_change_effect.shape[2], 2))\n",
        "  for k, i in enumerate(img_inx):\n",
        "    curr_style_effect[k, :, :] = style_change_effect[i, :, :, :]\n",
        "    curr_w[k, :] = W_values[i, :].detach()\n",
        "    curr_style_vector_distances[k, :, :] = all_style_vectors_distances[i, :, :]\n",
        "  style_effect_classes[img_ind] = curr_style_effect\n",
        "  W_classes[img_ind] = curr_w\n",
        "  style_vectors_distances_classes[img_ind] = curr_style_vector_distances\n",
        "  all_style_vectors_classes[img_ind] = all_style_vectors[img_inx]\n",
        "  print(f'Class {img_ind}, {len(img_inx)} images.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "9raLCfq06YaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3bd8c5-6950-43d0-fde4-2ebf22f5d473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "(5, 2, 928, 2)\n",
            "Directions and style indices for moving from class 1 to class 0 =  [(1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0)]\n",
            "Use the other direction to move for class 0 to 1.\n"
          ]
        }
      ],
      "source": [
        "#@title Significant S values - combined {form-width: '20%'}\n",
        "label_size_clasifier = 2 #@param\n",
        "num_indices =  8 #@param\n",
        "effect_threshold = 0.2 #@param\n",
        "use_discriminator = False #@param {type: 'boolean'}\n",
        "discriminator_model = None\n",
        "s_indices_and_signs_dict = {}\n",
        "print(style_effect_classes[1])\n",
        "for class_index in [0, 1]:\n",
        "  split_ind = 1 - class_index\n",
        "  all_s = style_effect_classes[split_ind]\n",
        "  all_w = W_classes[split_ind]\n",
        "\n",
        "  # Find s indicies\n",
        "  if all_s.shape[0]:\n",
        "    s_indices_and_signs = find_significant_styles(\n",
        "      style_change_effect=all_s,\n",
        "      num_indices=num_indices,\n",
        "      class_index=class_index,\n",
        "      discriminator=discriminator_model,\n",
        "      generator=generator,\n",
        "      classifier=classifier,\n",
        "      all_dlatents=all_w,\n",
        "      style_min=style_min,\n",
        "      style_max=style_max,\n",
        "      max_image_effect=effect_threshold*5,\n",
        "      label_size=label_size_clasifier,\n",
        "      discriminator_threshold=0.2,\n",
        "      sindex_offset=0)\n",
        "\n",
        "    s_indices_and_signs_dict[class_index] = s_indices_and_signs\n",
        "  else:\n",
        "    s_indices_and_signs_dict[class_index] = []\n",
        "\n",
        "# Combine the style indicies for the two classes.\n",
        "\n",
        "sindex_class_0 = [sindex for _, sindex in s_indices_and_signs_dict[0]]\n",
        "\n",
        "all_sindex_joined_class_0 = [(1 - direction, sindex) for direction, sindex in \n",
        "                            s_indices_and_signs_dict[1] if sindex not in sindex_class_0]\n",
        "all_sindex_joined_class_0 += s_indices_and_signs_dict[0]\n",
        "\n",
        "scores = []\n",
        "for direction, sindex in all_sindex_joined_class_0:\n",
        "  other_direction = 1 if direction == 0 else 0\n",
        "  curr_score = torch.mean(style_change_effect[:, direction, sindex, 0]) + torch.mean(style_change_effect[:, other_direction, sindex, 1])\n",
        "  scores.append(curr_score)\n",
        "\n",
        "s_indices_and_signs = [all_sindex_joined_class_0[i] for i in np.argsort(scores)[::-1]]\n",
        "\n",
        "print('Directions and style indices for moving from class 1 to class 0 = ', s_indices_and_signs[:num_indices])\n",
        "print('Use the other direction to move for class 0 to 1.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HRrx8J8X6YaX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "86e4b708-92fd-4685-824e-2908faf10273"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-028f9896a582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/ipwn/arialuni.ttf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mselect_images_by_s_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   yy = visualize_style(generator, \n",
            "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
          ]
        }
      ],
      "source": [
        "max_images = 8 #@param\n",
        "sindex = 800 #@param\n",
        "class_index = 0 #@param {type: \"integer\"} \n",
        "shift_sign = \"1\" #@param [0, 1]\n",
        "wsign_index = int(shift_sign)\n",
        "\n",
        "shift_size =  1 #@param\n",
        "effect_threshold = 0 #@param\n",
        "split_by_class = False #@param {type:\"boolean\"}\n",
        "select_images_by_s_distance = True #@param {type:\"boolean\"}\n",
        "draw_probabilities_on_image = True #@param {type:\"boolean\"}\n",
        "\n",
        "if split_by_class:\n",
        "  split_ind = 1 if class_index == 0 else 0\n",
        "  all_s = style_effect_classes[split_ind]\n",
        "  all_w = W_classes[split_ind]\n",
        "  all_s_distances = style_vectors_distances_classes[split_ind]\n",
        "else:\n",
        "  all_s = style_change_effect\n",
        "  all_w = W_values\n",
        "  all_s_distances = all_style_vectors_distances\n",
        "\n",
        "font_file = './arialuni.ttf'\n",
        "if not os.path.exists(font_file):\n",
        "  r = requests.get('https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/ipwn/arialuni.ttf')\n",
        "  open(font_file, 'wb').write(r.content)\n",
        "stop\n",
        "if not select_images_by_s_distance:\n",
        "  yy = visualize_style(generator, \n",
        "                       classifier,\n",
        "                       all_w,\n",
        "                       all_s,\n",
        "                       style_min,\n",
        "                       style_max,\n",
        "                       sindex,\n",
        "                       wsign_index,\n",
        "                       max_images=max_images,\n",
        "                       shift_size=shift_size,\n",
        "                       font_file=font_file,\n",
        "                       label_size=label_size,\n",
        "                       class_index=class_index,\n",
        "                       effect_threshold=effect_threshold,\n",
        "                       draw_results_on_image=draw_probabilities_on_image)\n",
        "    \n",
        "else:\n",
        "  yy = visualize_style_by_distance_in_s(\n",
        "    generator,\n",
        "    classifier,\n",
        "    all_w,\n",
        "    all_s_distances,\n",
        "    style_min,\n",
        "    style_max,\n",
        "    sindex,\n",
        "    wsign_index,\n",
        "    max_images=max_images,\n",
        "    shift_size=shift_size,\n",
        "    font_file=font_file,\n",
        "    label_size=label_size,\n",
        "    class_index=class_index,\n",
        "    effect_threshold=effect_threshold,\n",
        "    draw_results_on_image=draw_probabilities_on_image)\n",
        "\n",
        "if yy.size > 0:\n",
        "  show_image(yy)\n",
        "else:\n",
        "  print('no images found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI-t6EiE6YaY"
      },
      "outputs": [],
      "source": [
        "#@title Show animation {form-width: '20%'}\n",
        "\n",
        "import matplotlib.animation \n",
        "from IPython.display import HTML\n",
        "\n",
        "ani = make_animation(yy, resolution)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOg167Mr6YaZ"
      },
      "outputs": [],
      "source": [
        "#Show the 4 top attributes\n",
        "\n",
        "index_to_naming = {1: \"Skin Pigminatation\", 2: \"Eyebrow Thickness\", 3: \"Add/Remove Glasses\", 4: \"Dark/White Hair\"}\n",
        "images_list = [[5, 3], [3, 0], [0, 6], [4, 1]]\n",
        "shift_sizes = shift_sizes = [(2, 1.5),(1, 1),(1, 1),(1.5, 2)]\n",
        "effect_threshold = 0\n",
        "\n",
        "print('Original images are on the first row, the probabilities displayed are for the other class - left column for being old, right column for being young)')\n",
        "\n",
        "for i, (direction, sindex) in enumerate(s_indices_and_signs[:4]):\n",
        "  images_s = np.zeros((resolution * 2, resolution * 2, 3)).astype(np.uint8)\n",
        "  for d in [direction, 1 - direction]:\n",
        "    class_index = 0 if d == direction else 1\n",
        "    split_ind = 1 if d == direction else 0\n",
        "    all_s = style_effect_classes[split_ind]\n",
        "    all_s_distances = style_vectors_distances_classes[split_ind]\n",
        "    \n",
        "    # Generate images\n",
        "    yy = visualize_style_by_distance_in_s(\n",
        "      generator,\n",
        "      classifier,\n",
        "      all_w,\n",
        "      all_s_distances,\n",
        "      style_min,\n",
        "      style_max,\n",
        "      sindex,\n",
        "      d,\n",
        "      max_images=max_images,\n",
        "      shift_size=shift_sizes[i][class_index],\n",
        "      font_file=font_file,\n",
        "      label_size=label_size,\n",
        "      class_index=class_index,\n",
        "      effect_threshold=effect_threshold,\n",
        "      draw_results_on_image=draw_probabilities_on_image)\n",
        "    \n",
        "    for n in range(2):\n",
        "      images_s[n * resolution: (n + 1) * resolution, class_index * resolution: (class_index + 1) * resolution, :] = yy[(images_list[i][class_index]) * resolution: (images_list[i][class_index] + 1) * resolution, n * resolution: (n + 1) * resolution, :]\n",
        "  \n",
        "  print(f'Attribute {i + 1}: {index_to_naming[i+1]}')\n",
        "  show_image(images_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1MhcRsN6Yaa"
      },
      "source": [
        "## Image-specific explanations\n",
        "\n",
        "In this section, we implement the **Independent** selection strategy, to find the top 5 attributes for explaining a perceived-age classifier for a **specific image**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "tvCK2ucG6Yaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa1de4b-fc84-4912-a5ee-15e46cbd3eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "#Choose latent index to find image's top 5 attributes\n",
        "latent_index = 0\n",
        "print(base_probs.shape)\n",
        "\n",
        "if len(base_probs.shape) == 1:\n",
        "  base_probs.unsqueeze_(0)\n",
        "  style_change_effect.unsqueeze(0)\n",
        "  W_values[latent_index].unsqueeze(0)\n",
        "  all_style_vectors.unsqueeze(0)\n",
        "\n",
        "one_latent = torch.unsqueeze(W_values[latent_index], 0)\n",
        "one_base_prob = torch.unsqueeze(base_probs[latent_index], 0)\n",
        "one_style_change_effect = torch.unsqueeze(style_change_effect[latent_index], 0)\n",
        "\n",
        "expanded_dlatent_tmp = torch.tile(one_latent, [1, num_layers, 1])\n",
        "\n",
        "style_min,_ = torch.min(all_style_vectors, dim=0)\n",
        "style_max,_ = torch.max(all_style_vectors, dim=0)\n",
        "all_style_vectors_distances = np.zeros((all_style_vectors.shape[0], all_style_vectors.shape[1], 2))\n",
        "all_style_vectors_distances[:,:, 0] = all_style_vectors - torch.tile(style_min, (all_style_vectors.shape[0], 1))\n",
        "all_style_vectors_distances[:,:, 1] = torch.tile(style_max, (all_style_vectors.shape[0], 1)) - all_style_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "PNRvTpXZ6Yab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450b905d-8a86-420e-9889-9236fc2b797c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2, 928, 2)\n",
            "Directions and style indices for moving from class 1 (yound) to class 0 (old) =  [(1, 860), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
          ]
        }
      ],
      "source": [
        "# Find s indicies\n",
        "class_index = 0# @param {type: \"integer\"} \n",
        "label_size_clasifier = 2 #@param\n",
        "num_indices = 5 #@param\n",
        "effect_threshold = 0.5 #@param\n",
        "use_discriminator = False #@param {type: 'boolean'}\n",
        "discriminator_model = None\n",
        "\n",
        "s_indices_and_signs = find_significant_styles(\n",
        "    style_change_effect=one_style_change_effect.numpy(),\n",
        "    num_indices=num_indices,\n",
        "    class_index=class_index,\n",
        "    discriminator=discriminator_model,\n",
        "    generator=generator,\n",
        "    classifier=classifier,\n",
        "    all_dlatents=one_latent,\n",
        "    style_min=style_min,\n",
        "    style_max=style_max,\n",
        "    max_image_effect=effect_threshold*5,\n",
        "    label_size=label_size_clasifier,\n",
        "    discriminator_threshold=0.2,\n",
        "    sindex_offset=0)\n",
        "\n",
        "print(f'Directions and style indices for moving from class 1 (yound) to class {class_index} (old) = ', s_indices_and_signs[:num_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "IaMiVjE_6Yab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b498902-b3da-4039-ffa9-6950fee80592"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-e65fd2dda706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0meffect_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffect_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mdraw_results_on_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdraw_probabilities_on_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       images_idx=[0])\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Attribute {i + 1}: {index_to_naming[i + 1]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-8e3bf553ecd1>\u001b[0m in \u001b[0;36mvisualize_style_by_distance_in_s\u001b[0;34m(generator, classifier, all_dlatents, all_style_vectors_distances, style_min, style_max, sindex, style_sign_index, max_images, shift_size, font_file, label_size, class_index, draw_results_on_image, effect_threshold, images_idx)\u001b[0m\n\u001b[1;32m    712\u001b[0m          \u001b[0mshift_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshift_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m          \u001b[0mlabel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m          draw_results_on_image=draw_results_on_image)\n\u001b[0m\u001b[1;32m    715\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchange_prob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbase_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0meffect_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-8e3bf553ecd1>\u001b[0m in \u001b[0;36mgenerate_images_given_dlatent\u001b[0;34m(dlatent, generator, classifier, class_index, sindex, s_style_min, s_style_max, style_direction_index, font_file, shift_size, label_size, draw_results_on_image, resolution, num_layers)\u001b[0m\n\u001b[1;32m    518\u001b[0m   \u001b[0;31m# dlatent = torch.from_numpy(dlatent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m   \u001b[0mexpanded_dlatent_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m   \u001b[0msvbg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_vector_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_dlatent_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m   \u001b[0mresult_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m   \u001b[0mimages_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_given_dlatent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_dlatent_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvbg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-5d9d66802f76>\u001b[0m in \u001b[0;36mstyle_vector_calculator\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# The first style block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_styles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mstyle_vector_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-5d9d66802f76>\u001b[0m in \u001b[0;36mget_styles\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# Get style vector $s$\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mToRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-5d9d66802f76>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
          ]
        }
      ],
      "source": [
        "shift_sign = \"0\" #@param [0, 1]\n",
        "wsign_index = int(shift_sign)\n",
        "\n",
        "shift_size = 2#@param\n",
        "effect_threshold = 0#@param\n",
        "\n",
        "shift_sizes = [1, -1.5, -1, 1.5, 2.5]\n",
        "index_to_naming = {1: \"Add Glasses\", 2: \"White Hair\", 3: \"Neck Coverage\", 4: \"Receding Hairline\", 5: \"Glowy White Hair\"}\n",
        "\n",
        " \n",
        "for i, (direction, sindex) in enumerate(s_indices_and_signs[:5]): \n",
        "  yy = visualize_style_by_distance_in_s(\n",
        "      generator,\n",
        "      classifier,\n",
        "      one_latent,\n",
        "      all_style_vectors_distances,\n",
        "      style_min,\n",
        "      style_max,\n",
        "      sindex,\n",
        "      direction,\n",
        "      max_images=max_images,\n",
        "      shift_size=shift_sizes[i],\n",
        "      font_file=font_file,\n",
        "      label_size=label_size,\n",
        "      class_index=class_index,\n",
        "      effect_threshold=effect_threshold,\n",
        "      draw_results_on_image=draw_probabilities_on_image,\n",
        "      images_idx=[0])\n",
        "\n",
        "  print(f'Attribute {i + 1}: {index_to_naming[i + 1]}')\n",
        "\n",
        "  if yy.size > 0:\n",
        "    show_image(yy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o3AN-j8J5eg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Trained StylEx Test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}