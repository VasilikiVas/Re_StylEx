{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24204/3819716165.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcPickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfind_significant_styles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./classifier.pth'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\irene\\Documents\\GitHub\\FACT_UvA_2022\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[0mLAYER_SHAPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdense\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle_vector_calculator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle_dense_blocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m   \u001b[0mLAYER_SHAPES\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_bias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "from mobilenet_pytorch import MobileNetV1\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, List\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import six.moves.cPickle as cPickle\n",
    "\n",
    "from utils import find_significant_styles\n",
    "\n",
    "PATH = './classifier.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models:\n",
    "We load the pretrained pytorch models after transferring their tensorflow models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "input = cv2.imread('imgs/5.png')\n",
    "input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)[None, ...]/255\n",
    "\n",
    "# torch needs input in form [B, C, H, W] but input is [B, H, W, C]\n",
    "input = torch.tensor(input.transpose(0,3,1,2), dtype=torch.float32)\n",
    "\n",
    "classifier = MobileNetV1()\n",
    "classifier.load_state_dict(torch.load(PATH))\n",
    "classifier.eval()\n",
    "\n",
    "generator = tf.keras.models.load_model('./generator.savedmodel')\n",
    "discriminator = tf.keras.models.load_model('./discriminator.savedmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latents extraction: \n",
    "We don't provide the dataset_provider, so we will load the dlatents from a precomputed np.array.\n",
    "Here we present how to run the encoder on one image, to calculate one dlatent (we pre-calculate for 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the precomputed dlatents (already concatenated to the labels)\n",
    "dlatents_url = bucket_path + 'dlatents.pkl'\n",
    "r = requests.get(dlatents_url)\n",
    "dlatents = cPickle.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load effect data from the tfrecord {form-width: '20%'}\n",
    "examples_url = bucket_path + 'examples_1.tfrecord'\n",
    "r = requests.get(examples_url)\n",
    "data_path = path + 'examples_1.tfrecord'\n",
    "open(data_path, 'wb').write(r.content)\n",
    "num_classes = 2\n",
    "print(f'Loaded dataset: {data_path}')\n",
    "table = tf.data.TFRecordDataset([data_path])\n",
    "# Read sspace tfrecord unwrapped:\n",
    "style_change_effect = []\n",
    "dlatents = []\n",
    "base_probs = []\n",
    "for raw_record in table:\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(raw_record.numpy())\n",
    "  dlatents.append(\n",
    "      np.array(example.features.feature['dlatent'].float_list.value))\n",
    "  seffect = np.array(\n",
    "      example.features.feature['result'].float_list.value).reshape(\n",
    "          (-1, 2, num_classes))\n",
    "  style_change_effect.append(seffect.transpose([1, 0, 2]))\n",
    "  base_probs.append(\n",
    "      np.array(example.features.feature['base_prob'].float_list.value))\n",
    "\n",
    "style_change_effect = np.array(style_change_effect)\n",
    "dlatents = np.array(dlatents)\n",
    "W_values, style_change_effect, base_probs = dlatents, style_change_effect, np.array(base_probs)\n",
    "\n",
    "\n",
    "style_change_effect = filter_unstable_images(style_change_effect, effect_threshold=2)\n",
    "\n",
    "all_style_vectors = tf.concat(generator.style_vector_calculator(W_values, training=False)[0], axis=1).numpy()\n",
    "style_min = np.min(all_style_vectors, axis=0)\n",
    "style_max = np.max(all_style_vectors, axis=0)\n",
    "\n",
    "all_style_vectors_distances = np.zeros((all_style_vectors.shape[0], all_style_vectors.shape[1], 2))\n",
    "all_style_vectors_distances[:,:, 0] = all_style_vectors - np.tile(style_min, (all_style_vectors.shape[0], 1))\n",
    "all_style_vectors_distances[:,:, 1] = np.tile(style_max, (all_style_vectors.shape[0], 1)) - all_style_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the extraction step of AttFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Split by class\n",
    "all_labels = np.argmax(base_probs, axis=1)\n",
    "style_effect_classes = {}\n",
    "W_classes = {}\n",
    "style_vectors_distances_classes = {}\n",
    "all_style_vectors_classes = {}\n",
    "for img_ind in range(label_size):\n",
    "  img_inx = np.array([i for i in range(all_labels.shape[0]) \n",
    "  if all_labels[i] == img_ind])\n",
    "  curr_style_effect = np.zeros((len(img_inx), style_change_effect.shape[1], \n",
    "                                style_change_effect.shape[2], style_change_effect.shape[3]))\n",
    "  curr_w = np.zeros((len(img_inx), W_values.shape[1]))\n",
    "  curr_style_vector_distances = np.zeros((len(img_inx), style_change_effect.shape[2], 2))\n",
    "  for k, i in enumerate(img_inx):\n",
    "    curr_style_effect[k, :, :] = style_change_effect[i, :, :, :]\n",
    "    curr_w[k, :] = W_values[i, :]\n",
    "    curr_style_vector_distances[k, :, :] = all_style_vectors_distances[i, :, :]\n",
    "  style_effect_classes[img_ind] = curr_style_effect\n",
    "  W_classes[img_ind] = curr_w\n",
    "  style_vectors_distances_classes[img_ind] = curr_style_vector_distances\n",
    "  all_style_vectors_classes[img_ind] = all_style_vectors[img_inx]\n",
    "  print(f'Class {img_ind}, {len(img_inx)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Significant S values - combined {form-width: '20%'}\n",
    "label_size_clasifier = 2 #@param\n",
    "num_indices =  8 #@param\n",
    "effect_threshold = 0.2 #@param\n",
    "use_discriminator = False #@param {type: 'boolean'}\n",
    "discriminator_model = discriminator if use_discriminator else None\n",
    "s_indices_and_signs_dict = {}\n",
    "\n",
    "for class_index in [0, 1]:\n",
    "  split_ind = 1 - class_index\n",
    "  all_s = style_effect_classes[split_ind]\n",
    "  all_w = W_classes[split_ind]\n",
    "\n",
    "  # Find s indicies\n",
    "  s_indices_and_signs = find_significant_styles(\n",
    "    style_change_effect=all_s,\n",
    "    num_indices=num_indices,\n",
    "    class_index=class_index,\n",
    "    discriminator=discriminator_model,\n",
    "    generator=generator,\n",
    "    classifier=classifier,\n",
    "    all_dlatents=all_w,\n",
    "    style_min=style_min,\n",
    "    style_max=style_max,\n",
    "    max_image_effect=effect_threshold*5,\n",
    "    label_size=label_size_clasifier,\n",
    "    discriminator_threshold=0.2,\n",
    "    sindex_offset=0)\n",
    "\n",
    "  s_indices_and_signs_dict[class_index] = s_indices_and_signs\n",
    "\n",
    "# Combine the style indicies for the two classes.\n",
    "sindex_class_0 = [sindex for _, sindex in s_indices_and_signs_dict[0]]\n",
    "\n",
    "all_sindex_joined_class_0 = [(1 - direction, sindex) for direction, sindex in \n",
    "                             s_indices_and_signs_dict[1] if sindex not in sindex_class_0]\n",
    "all_sindex_joined_class_0 += s_indices_and_signs_dict[0]\n",
    "\n",
    "scores = []\n",
    "for direction, sindex in all_sindex_joined_class_0:\n",
    "  other_direction = 1 if direction == 0 else 0\n",
    "  curr_score = np.mean(style_change_effect[:, direction, sindex, 0]) + np.mean(style_change_effect[:, other_direction, sindex, 1])\n",
    "  scores.append(curr_score)\n",
    "\n",
    "s_indices_and_signs = [all_sindex_joined_class_0[i] for i in np.argsort(scores)[::-1]]\n",
    "\n",
    "print('Directions and style indices for moving from class 1 to class 0 = ', s_indices_and_signs[:num_indices])\n",
    "print('Use the other direction to move for class 0 to 1.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = 2\n",
    "resolution = 256\n",
    "#@title Show the 4 top attributes - as displayed in Fig.4 (b)\n",
    "\n",
    "draw_probabilities_on_image = True #@param {type: \"boolean\"}\n",
    "index_to_naming = {0: \"Skin Pigminatation\", 1: \"Eyebrow Thickness\", 2: \"Add/Remove Glasses\", 3: \"Dark/White Hair\"}\n",
    "images_list = [[0, 4], [16, 17], [18, 18], [3, 14]]\n",
    "max_images = 20\n",
    "shift_sizes = [(2, 1.5),(1, 1),(1, 1),(1.5, 2)]\n",
    "effect_threshold = 0.05\n",
    "font_file = '/tmp/arialuni.ttf'\n",
    "if not os.path.exists(font_file):\n",
    "  gfile.Copy('/google_src/head/depot/google3/googledata/third_party/fonts/ascender/arialuni.ttf', font_file)\n",
    "\n",
    "for i, (direction, sindex) in enumerate(s_indices_and_signs[:4]):\n",
    "  images_s = np.zeros((resolution * 2, resolution * 2, 3)).astype(np.uint8)\n",
    "  for d in [direction, 1 - direction]:\n",
    "    # Take only images from the offsite class\n",
    "    class_index = 0 if d == direction else 1\n",
    "    split_ind = 1 if d == direction else 0\n",
    "    all_s = style_effect_classes[split_ind]\n",
    "    all_w = W_classes[split_ind]\n",
    "    all_s_distances = style_vectors_distances_classes[split_ind]\n",
    "    # Generate images\n",
    "    yy = visualize_style_by_distance_in_s(\n",
    "      generator,\n",
    "      classifier,\n",
    "      all_w,\n",
    "      all_s_distances,\n",
    "      style_min,\n",
    "      style_max,\n",
    "      sindex,\n",
    "      d,\n",
    "      max_images=max_images,\n",
    "      shift_size=shift_sizes[i][class_index],\n",
    "      font_file=font_file,\n",
    "      label_size=label_size,\n",
    "      class_index=class_index,\n",
    "      effect_threshold=effect_threshold,\n",
    "      draw_results_on_image=draw_probabilities_on_image)\n",
    "    for n in range(2):\n",
    "      images_s[n * resolution: (n + 1) * resolution, class_index * resolution: (class_index + 1) * resolution, :] = yy[(images_list[i][class_index]) * resolution: (images_list[i][class_index] + 1) * resolution, n * resolution: (n + 1) * resolution, :]\n",
    "  print(f'Attribute {i} {index_to_naming[i]}: \\n(Original images are on the first row, the probabilities displayed are for the other class - left column for being old, write column for being young)')\n",
    "  show_image(images_s)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37f41d3b80ebd45966a952b1638f691fcfb2896753936854d927f7450e307de4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl2021': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
