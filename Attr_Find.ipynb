{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenet_pytorch import MobileNetV1\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, List\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import pickle\n",
    "\n",
    "from utils import find_significant_styles, filter_unstable_images\n",
    "\n",
    "CLASSIFIER_PATH = './models/classifier.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models:\n",
    "We load the pretrained pytorch models after transferring their tensorflow models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "classifier = MobileNetV1()\n",
    "classifier.load_state_dict(torch.load(CLASSIFIER_PATH))\n",
    "classifier.eval()\n",
    "\n",
    "generator = tf.keras.models.load_model('./generator.savedmodel')\n",
    "discriminator = tf.keras.models.load_model('./discriminator.savedmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "\n",
    "num_layers = 14\n",
    "label_size = 2\n",
    "resolution = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latents extraction: \n",
    "We don't provide the dataset_provider, so we will load the dlatents from a precomputed np.array.\n",
    "Here we present how to run the encoder on one image, to calculate one dlatent (we pre-calculate for 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the precomputed dlatents (already concatenated to the labels)\n",
    "latents_file = open(\"./saved_dlantents.pkl\",'rb')\n",
    "dlatents = pickle.load(latents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: ./data/examples_1.tfrecord\n"
     ]
    }
   ],
   "source": [
    "#@title Load effect data from the tfrecord {form-width: '20%'}\n",
    "data_path = './data/examples_1.tfrecord'\n",
    "num_classes = 2\n",
    "print(f'Loaded dataset: {data_path}')\n",
    "table = tf.data.TFRecordDataset([data_path])\n",
    "# Read sspace tfrecord unwrapped:\n",
    "style_change_effect = []\n",
    "dlatents = []\n",
    "base_probs = []\n",
    "for raw_record in table:\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(raw_record.numpy())\n",
    "  dlatents.append(\n",
    "      np.array(example.features.feature['dlatent'].float_list.value))\n",
    "  seffect = np.array(\n",
    "      example.features.feature['result'].float_list.value).reshape(\n",
    "          (-1, 2, num_classes))\n",
    "  style_change_effect.append(seffect.transpose([1, 0, 2]))\n",
    "  base_probs.append(\n",
    "      np.array(example.features.feature['base_prob'].float_list.value))\n",
    "\n",
    "style_change_effect = np.array(style_change_effect)\n",
    "dlatents = np.array(dlatents)\n",
    "W_values, style_change_effect, base_probs = dlatents, style_change_effect, np.array(base_probs)\n",
    "\n",
    "style_change_effect = filter_unstable_images(style_change_effect, effect_threshold=2)\n",
    "\n",
    "all_style_vectors = tf.concat(generator.style_vector_calculator(W_values, training=False)[0], axis=1).numpy()\n",
    "style_min = np.min(all_style_vectors, axis=0)\n",
    "style_max = np.max(all_style_vectors, axis=0)\n",
    "\n",
    "all_style_vectors_distances = np.zeros((all_style_vectors.shape[0], all_style_vectors.shape[1], 2))\n",
    "all_style_vectors_distances[:,:, 0] = all_style_vectors - np.tile(style_min, (all_style_vectors.shape[0], 1))\n",
    "all_style_vectors_distances[:,:, 1] = np.tile(style_max, (all_style_vectors.shape[0], 1)) - all_style_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the extraction step of AttFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0, 90 images.\n",
      "Class 1, 160 images.\n"
     ]
    }
   ],
   "source": [
    "#@title Split by class\n",
    "all_labels = np.argmax(base_probs, axis=1)\n",
    "style_effect_classes = {}\n",
    "W_classes = {}\n",
    "style_vectors_distances_classes = {}\n",
    "all_style_vectors_classes = {}\n",
    "for img_ind in range(label_size):\n",
    "  img_inx = np.array([i for i in range(all_labels.shape[0]) \n",
    "  if all_labels[i] == img_ind])\n",
    "  curr_style_effect = np.zeros((len(img_inx), style_change_effect.shape[1], \n",
    "                                style_change_effect.shape[2], style_change_effect.shape[3]))\n",
    "  curr_w = np.zeros((len(img_inx), W_values.shape[1]))\n",
    "  curr_style_vector_distances = np.zeros((len(img_inx), style_change_effect.shape[2], 2))\n",
    "  for k, i in enumerate(img_inx):\n",
    "    curr_style_effect[k, :, :] = style_change_effect[i, :, :, :]\n",
    "    curr_w[k, :] = W_values[i, :]\n",
    "    curr_style_vector_distances[k, :, :] = all_style_vectors_distances[i, :, :]\n",
    "  style_effect_classes[img_ind] = curr_style_effect\n",
    "  W_classes[img_ind] = curr_w\n",
    "  style_vectors_distances_classes[img_ind] = curr_style_vector_distances\n",
    "  all_style_vectors_classes[img_ind] = all_style_vectors[img_inx]\n",
    "  print(f'Class {img_ind}, {len(img_inx)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directions and style indices for moving from class 1 to class 0 =  [(1, 5300), (0, 5147), (1, 3301), (1, 3199), (1, 3921), (1, 1208), (0, 5246), (0, 4172)]\n",
      "Use the other direction to move for class 0 to 1.\n"
     ]
    }
   ],
   "source": [
    "#@title Significant S values - combined {form-width: '20%'}\n",
    "label_size_clasifier = 2 #@param\n",
    "num_indices =  8 #@param\n",
    "effect_threshold = 0.2 #@param\n",
    "use_discriminator = False #@param {type: 'boolean'}\n",
    "discriminator_model = discriminator if use_discriminator else None\n",
    "s_indices_and_signs_dict = {}\n",
    "\n",
    "for class_index in [0, 1]:\n",
    "  split_ind = 1 - class_index\n",
    "  all_s = style_effect_classes[split_ind]\n",
    "  all_w = W_classes[split_ind]\n",
    "\n",
    "  # Find s indicies\n",
    "  s_indices_and_signs = find_significant_styles(\n",
    "    style_change_effect=all_s,\n",
    "    num_indices=num_indices,\n",
    "    class_index=class_index,\n",
    "    discriminator=discriminator_model,\n",
    "    generator=generator,\n",
    "    classifier=classifier,\n",
    "    all_dlatents=all_w,\n",
    "    style_min=style_min,\n",
    "    style_max=style_max,\n",
    "    max_image_effect=effect_threshold*5,\n",
    "    label_size=label_size_clasifier,\n",
    "    discriminator_threshold=0.2,\n",
    "    sindex_offset=0)\n",
    "\n",
    "  s_indices_and_signs_dict[class_index] = s_indices_and_signs\n",
    "\n",
    "# Combine the style indicies for the two classes.\n",
    "sindex_class_0 = [sindex for _, sindex in s_indices_and_signs_dict[0]]\n",
    "\n",
    "all_sindex_joined_class_0 = [(1 - direction, sindex) for direction, sindex in \n",
    "                             s_indices_and_signs_dict[1] if sindex not in sindex_class_0]\n",
    "all_sindex_joined_class_0 += s_indices_and_signs_dict[0]\n",
    "\n",
    "scores = []\n",
    "for direction, sindex in all_sindex_joined_class_0:\n",
    "  other_direction = 1 if direction == 0 else 0\n",
    "  curr_score = np.mean(style_change_effect[:, direction, sindex, 0]) + np.mean(style_change_effect[:, other_direction, sindex, 1])\n",
    "  scores.append(curr_score)\n",
    "\n",
    "s_indices_and_signs = [all_sindex_joined_class_0[i] for i in np.argsort(scores)[::-1]]\n",
    "\n",
    "print('Directions and style indices for moving from class 1 to class 0 = ', s_indices_and_signs[:num_indices])\n",
    "print('Use the other direction to move for class 0 to 1.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show the 4 top attributes - as displayed in Fig.4 (b)\n",
    "\n",
    "draw_probabilities_on_image = True #@param {type: \"boolean\"}\n",
    "index_to_naming = {0: \"Skin Pigminatation\", 1: \"Eyebrow Thickness\", 2: \"Add/Remove Glasses\", 3: \"Dark/White Hair\"}\n",
    "images_list = [[0, 4], [16, 17], [18, 18], [3, 14]]\n",
    "max_images = 20\n",
    "shift_sizes = [(2, 1.5),(1, 1),(1, 1),(1.5, 2)]\n",
    "effect_threshold = 0.05\n",
    "font_file = '/tmp/arialuni.ttf'\n",
    "if not os.path.exists(font_file):\n",
    "  gfile.Copy('/google_src/head/depot/google3/googledata/third_party/fonts/ascender/arialuni.ttf', font_file)\n",
    "\n",
    "for i, (direction, sindex) in enumerate(s_indices_and_signs[:4]):\n",
    "  images_s = np.zeros((resolution * 2, resolution * 2, 3)).astype(np.uint8)\n",
    "  for d in [direction, 1 - direction]:\n",
    "    # Take only images from the offsite class\n",
    "    class_index = 0 if d == direction else 1\n",
    "    split_ind = 1 if d == direction else 0\n",
    "    all_s = style_effect_classes[split_ind]\n",
    "    all_w = W_classes[split_ind]\n",
    "    all_s_distances = style_vectors_distances_classes[split_ind]\n",
    "    # Generate images\n",
    "    yy = visualize_style_by_distance_in_s(\n",
    "      generator,\n",
    "      classifier,\n",
    "      all_w,\n",
    "      all_s_distances,\n",
    "      style_min,\n",
    "      style_max,\n",
    "      sindex,\n",
    "      d,\n",
    "      max_images=max_images,\n",
    "      shift_size=shift_sizes[i][class_index],\n",
    "      font_file=font_file,\n",
    "      label_size=label_size,\n",
    "      class_index=class_index,\n",
    "      effect_threshold=effect_threshold,\n",
    "      draw_results_on_image=draw_probabilities_on_image)\n",
    "    for n in range(2):\n",
    "      images_s[n * resolution: (n + 1) * resolution, class_index * resolution: (class_index + 1) * resolution, :] = yy[(images_list[i][class_index]) * resolution: (images_list[i][class_index] + 1) * resolution, n * resolution: (n + 1) * resolution, :]\n",
    "  print(f'Attribute {i} {index_to_naming[i]}: \\n(Original images are on the first row, the probabilities displayed are for the other class - left column for being old, write column for being young)')\n",
    "  show_image(images_s)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37f41d3b80ebd45966a952b1638f691fcfb2896753936854d927f7450e307de4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl2021': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
